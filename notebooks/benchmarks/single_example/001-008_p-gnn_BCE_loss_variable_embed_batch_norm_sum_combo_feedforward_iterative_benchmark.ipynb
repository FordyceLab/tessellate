{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXUYGCpNbSla"
   },
   "source": [
    "# Benchmark position-aware graph neural network/2D CNN architecture\n",
    "\n",
    "This notebook contains all of the code to overfit a P-GNN/2D CNN to four contact channels of a single structure (6E6O).\n",
    "\n",
    "## Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRzY2E5pbYm6"
   },
   "source": [
    "### Dataloader code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jf5S3ZSabSlc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import periodictable as pt\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_files(acc, model, graph_dir, contacts_dir):\n",
    "    \"\"\"\n",
    "    Read graph and contacts files.\n",
    "    \n",
    "    Args:\n",
    "    - acc (str) - String of the PDB ID (lowercese).\n",
    "    - model (int) - Model number of the desired bioassembly.\n",
    "    - graph_dir (str) - Directory containing the nodes, edges,\n",
    "        and mask files.\n",
    "    - contacts_dir (str) - Directory containing the .contacts\n",
    "        files from get_contacts.py.\n",
    "        \n",
    "    Returns:\n",
    "    - Dictionary of DataFrames and lists corresponding to\n",
    "        graph nodes, edges, mask, and contacts. \n",
    "    \"\"\"\n",
    "\n",
    "    # Get the file names for the graph files\n",
    "    node_file = os.path.join(graph_dir, '{}-{}_nodes.csv'.format(acc, model))\n",
    "    edge_file = os.path.join(graph_dir, '{}-{}_edges.csv'.format(acc, model))\n",
    "    mask_file = os.path.join(graph_dir, '{}-{}_mask.csv'.format(acc, model))\n",
    "\n",
    "    # Get the contacts file\n",
    "    contacts_file = os.path.join(contacts_dir, '{}-{}.contacts'.format(acc, model))\n",
    "\n",
    "    # Read the nodes and edges\n",
    "    nodes = pd.read_csv(node_file)\n",
    "    edges = pd.read_csv(edge_file)\n",
    "\n",
    "    # Check if the mask is empty\n",
    "    if os.path.getsize(mask_file) > 0:\n",
    "        with open(mask_file) as f:\n",
    "            mask = f.read().split('\\n')\n",
    "    else:\n",
    "        mask = []\n",
    "\n",
    "    # Read the contacts\n",
    "    contacts = pd.read_table(contacts_file, sep='\\t',\n",
    "                             header=None, names=['type', 'start', 'end'])\n",
    "\n",
    "    # Return the data\n",
    "    data = {\n",
    "        'nodes': nodes,\n",
    "        'edges': edges,\n",
    "        'mask': mask,\n",
    "        'contacts': contacts\n",
    "    }\n",
    "\n",
    "    return data\n",
    "    \n",
    "    \n",
    "def process_res_data(data):\n",
    "    \"\"\"\n",
    "    Process residue-level data from atom-level data.\n",
    "    \n",
    "    Args:\n",
    "    - data (dict) - Dictionary of graph data output from `read_files`.\n",
    "    \n",
    "    Returns:\n",
    "    - Dictionary of atom and residue graph and contact data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract data form dict\n",
    "    nodes = data['nodes']\n",
    "    edges = data['edges']\n",
    "    mask = data['mask']\n",
    "    contacts = data['contacts']\n",
    "\n",
    "    # Get residue nodes\n",
    "    res_nodes = pd.DataFrame()\n",
    "    res_nodes['res'] = [':'.join(atom.split(':')[:3]) for atom in nodes['atom']]\n",
    "    res_nodes = res_nodes.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Get residue edges\n",
    "    res_edges = edges.copy()\n",
    "    res_edges['start'] = [':'.join(atom.split(':')[:3]) for atom in res_edges['start']]\n",
    "    res_edges['end'] = [':'.join(atom.split(':')[:3]) for atom in res_edges['end']]\n",
    "    res_edges = res_edges[res_edges['start'] != res_edges['end']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Get residue contacts\n",
    "    res_contacts = contacts.copy()\n",
    "    res_contacts['start'] = [':'.join(atom.split(':')[:3]) for atom in res_contacts['start']]\n",
    "    res_contacts['end'] = [':'.join(atom.split(':')[:3]) for atom in res_contacts['end']]\n",
    "    res_contacts = res_contacts[res_contacts['start'] != res_contacts['end']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Get residue mask\n",
    "    res_mask = list(set([':'.join(atom.split(':')[:3]) for atom in mask]))\n",
    "\n",
    "    # Return data dict\n",
    "    data = {\n",
    "        'atom_nodes': nodes,\n",
    "        'atom_edges': edges,\n",
    "        'atom_contact': contacts,\n",
    "        'atom_mask': mask,\n",
    "        'res_nodes': res_nodes,\n",
    "        'res_edges': res_edges,\n",
    "        'res_contact': res_contacts,\n",
    "        'res_mask': res_mask\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_map_dicts(entity_list):\n",
    "    \"\"\"\n",
    "    Map identifiers to indices and vice versa.\n",
    "    \n",
    "    Args:\n",
    "    - entity_list (list) - List of entities (atoms, residues, etc.)\n",
    "        to index.\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of the entity to index and index to entity dicts, respectively.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the entity:index dictionary\n",
    "    ent2idx_dict = {entity: idx for idx, entity in enumerate(entity_list)}\n",
    "\n",
    "    # Create the index:entity dictionary\n",
    "    idx2ent_dict = {idx: entity for entity, idx in ent2idx_dict.items()}\n",
    "    \n",
    "    return (ent2idx_dict, idx2ent_dict)\n",
    "\n",
    "\n",
    "def create_adj_mat(data, dict_map, mat_type):\n",
    "    \"\"\"\n",
    "    Creates an adjacency matrix.\n",
    "    \n",
    "    Args:\n",
    "    - data (DataFrame) - Dataframe with 'start' and 'end' column\n",
    "        for each interaction. For atom-level adjacency, 'order' \n",
    "        column is also required. For atom or residue conatcts,\n",
    "        'type' column is also required.\n",
    "    \n",
    "    Returns:\n",
    "    - Coordinate format matrix (numpy). For atom adjacency, third column\n",
    "        corresponds to bond order. For contacts, third column\n",
    "        corresponds to channel.\n",
    "    \n",
    "    Channel mappings (shorthand from get_contacts.py source):\n",
    "\n",
    "        0:\n",
    "            hp             hydrophobic interactions\n",
    "        1:\n",
    "            hb             hydrogen bonds\n",
    "            lhb            ligand hydrogen bonds\n",
    "            hbbb           backbone-backbone hydrogen bonds\n",
    "            hbsb           backbone-sidechain hydrogen bonds\n",
    "            hbss           sidechain-sidechain hydrogen bonds\n",
    "            hbls           ligand-sidechain residue hydrogen bonds\n",
    "            hblb           ligand-backbone residue hydrogen bonds\n",
    "        2:\n",
    "            vdw            van der Waals\n",
    "        3:\n",
    "            wb             water bridges\n",
    "            wb2            extended water bridges\n",
    "            lwb            ligand water bridges\n",
    "            lwb2           extended ligand water bridges\n",
    "        4:\n",
    "            sb             salt bridges\n",
    "        5:\n",
    "            ps             pi-stacking\n",
    "        6:\n",
    "            pc             pi-cation\n",
    "        7:\n",
    "            ts             t-stacking\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the coordinate list\n",
    "    coord_mat = []\n",
    "\n",
    "    # Map channel names to numeric channels\n",
    "    channel = {\n",
    "        # Hydrophobic interactions in first channel\n",
    "        'hp': 0,\n",
    "\n",
    "        # Hydrogen bonds in second channel\n",
    "        'hb': 1,\n",
    "        'lhb': 1, \n",
    "        'hbbb': 1,\n",
    "        'hbsb': 1,\n",
    "        'hbss': 1,\n",
    "        'hbls': 1,\n",
    "        'hblb': 1,\n",
    "\n",
    "        # VdW in third channel\n",
    "        'vdw': 2,\n",
    "\n",
    "        # Water bridges\n",
    "        'wb': 3, \n",
    "        'wb2': 3,\n",
    "        'lwb': 3,\n",
    "        'lwb2': 3,\n",
    "\n",
    "        # Salt bridges\n",
    "        'sb': 4,\n",
    "\n",
    "        # Other interactions\n",
    "        'ps': 5,\n",
    "        'pc': 6,\n",
    "        'ts': 7,\n",
    "    }\n",
    "\n",
    "    # Assemble the contacts\n",
    "    for idx, row in data.iterrows():\n",
    "\n",
    "        entry = [dict_map[row['start']], dict_map[row['end']]]\n",
    "\n",
    "        # Add order or type if necessary\n",
    "        if mat_type == 'atom_graph':\n",
    "            entry.append(row['order'])\n",
    "        elif mat_type == 'atom_contact':\n",
    "            entry.append(channel[row['type']])\n",
    "        elif mat_type == 'res_contact':\n",
    "            entry.append(channel[row['type']])\n",
    "\n",
    "        coord_mat.append(entry)\n",
    "\n",
    "    return(np.array(coord_mat))\n",
    "\n",
    "\n",
    "def create_mem_mat(atom_dict, res_dict):\n",
    "    \"\"\"\n",
    "    Create a membership matrix mapping atoms to residues.\n",
    "    \n",
    "    Args:\n",
    "    - atom_dict (dict) - Dictionary mapping atoms to indices.\n",
    "    - res_dict (dict) - Dictionary mapping residues to indices.\n",
    "    \n",
    "    Returns:\n",
    "    - Coordinate format membership matrix (numpy) with first\n",
    "        row being residue number and the second column being\n",
    "        atom number.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the coordinate list\n",
    "    mem_coord = []\n",
    "    \n",
    "    # Map atoms to residues\n",
    "    for atom, atom_idx in atom_dict.items():\n",
    "        res_idx = res_dict[':'.join(atom.split(':')[:3])]\n",
    "        \n",
    "        mem_coord.append([res_idx, atom_idx])\n",
    "        \n",
    "    mem_coord = np.array(mem_coord)\n",
    "    \n",
    "    return mem_coord\n",
    "\n",
    "\n",
    "def create_idx_list(id_list, dict_map):\n",
    "    \"\"\"\n",
    "    Create list of indices.\n",
    "    \n",
    "    Args:\n",
    "    - id_list (list) - List of masked atom or residue identifiers.\n",
    "    - dict_map (dict) - Dictionary mapping entities to indices.\n",
    "    \n",
    "    Returns:\n",
    "    - A numpy array of the masked indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate the numpy index array\n",
    "    idx_array = np.array([dict_map[iden] for iden in id_list])\n",
    "    \n",
    "    return idx_array\n",
    "\n",
    "\n",
    "class TesselateDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for structural data.\n",
    "    \n",
    "    Args:\n",
    "    - accession_list (str) - File path from which to read PDB IDs for dataset.\n",
    "    - graph_dir (str) - Directory containing the nodes, edges, and mask files.\n",
    "    - contacts_dir (str) - Directory containing the .contacts files from\n",
    "        get_contacts.py.\n",
    "    - return_data (list) - List of datasets to return. Value must be 'all' or\n",
    "        a subset of the following list:\n",
    "            - pdb_id\n",
    "            - model\n",
    "            - atom_nodes\n",
    "            - atom_adj\n",
    "            - atom_contact\n",
    "            - atom_mask\n",
    "            - res_adj\n",
    "            - res_contact\n",
    "            - res_mask\n",
    "            - mem_mat\n",
    "            - idx2atom_dict\n",
    "            - idx2res_dict    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, accession_list, graph_dir, contacts_dir, add_covalent=False, return_data='all', in_memory=False):\n",
    "        \n",
    "        if return_data == 'all':\n",
    "            self.return_data = [\n",
    "                'pdb_id',\n",
    "                'model',\n",
    "                'atom_nodes',\n",
    "                'atom_adj',\n",
    "                'atom_contact',\n",
    "                'atom_mask',\n",
    "                'res_adj',\n",
    "                'res_contact',\n",
    "                'res_mask',\n",
    "                'mem_mat',\n",
    "                'idx2atom_dict',\n",
    "                'idx2res_dict'\n",
    "            ]\n",
    "        \n",
    "        # Store reference to accession list file\n",
    "        self.accession_list = accession_list\n",
    "        \n",
    "        # Store references to the necessary directories\n",
    "        self.graph_dir = graph_dir\n",
    "        self.contacts_dir = contacts_dir\n",
    "        \n",
    "        # Whether to add covalent bonds to prediction task and\n",
    "        # remove sequence non-deterministic covalent bonds from the adjacency matrix\n",
    "        self.add_covalent=add_covalent\n",
    "        \n",
    "        # Read in and store a list of accession IDs\n",
    "        with open(accession_list, 'r') as handle:\n",
    "            self.accessions = np.array([acc.strip().lower().split() for acc in handle.readlines()])\n",
    "            \n",
    "        self.data = {}\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of the dataset.\n",
    "        \n",
    "        Returns:\n",
    "        - Integer count of number of examples.\n",
    "        \"\"\"\n",
    "        return len(self.accessions)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an item with a particular index value.\n",
    "        \n",
    "        Args:\n",
    "        - idx (int) - Index of desired sample.\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary of dataset example. All tensors are sparse when possible.\n",
    "        \"\"\"\n",
    "        if idx in self.data:\n",
    "            return self.data[idx]\n",
    "        \n",
    "        # initialize the return dictionary\n",
    "        return_dict = {}\n",
    "        \n",
    "        acc_entry = self.accessions[idx]\n",
    "        \n",
    "        # Get the PDB ID\n",
    "        acc = acc_entry[0]\n",
    "\n",
    "        # Get the model number if one exists\n",
    "        if len(acc_entry) == 1:\n",
    "            model = 1\n",
    "        else:\n",
    "            model = acc_entry[1]\n",
    "            \n",
    "        # Read and process the files\n",
    "        data = read_files(acc, model, self.graph_dir, self.contacts_dir)\n",
    "        data = process_res_data(data)\n",
    "        \n",
    "        # Generate the mapping dictionaries\n",
    "        atom2idx_dict, idx2atom_dict = get_map_dicts(data['atom_nodes']['atom'].unique())\n",
    "        res2idx_dict, idx2res_dict = get_map_dicts(data['res_nodes']['res'].unique())\n",
    "        \n",
    "        # Get numbers of atoms and residues per sample\n",
    "        n_atoms = len(atom2idx_dict)\n",
    "        n_res = len(res2idx_dict)\n",
    "        \n",
    "        # Handle all of the possible returned datasets\n",
    "        if 'pdb_id' in self.return_data:\n",
    "            return_dict['pdb_id'] = acc\n",
    "            \n",
    "        if 'model' in self.return_data:\n",
    "            return_dict['model'] = model\n",
    "            \n",
    "        if 'atom_nodes' in self.return_data:\n",
    "            ele_nums = [pt.elements.symbol(element).number for element in data['atom_nodes']['element']]\n",
    "            return_dict['atom_nodes'] = torch.LongTensor(ele_nums)\n",
    "        \n",
    "        if 'atom_adj' in self.return_data:\n",
    "            adj = create_adj_mat(data['atom_edges'], atom2idx_dict, mat_type='atom_graph').T\n",
    "            \n",
    "            x = torch.LongTensor(adj[0, :]).squeeze()\n",
    "            y = torch.LongTensor(adj[1, :]).squeeze()\n",
    "            val = torch.FloatTensor(adj[2, :]).squeeze()\n",
    "            \n",
    "            atom_adj = torch.zeros([n_atoms, n_atoms]).index_put_((x, y), val, accumulate=False)\n",
    "            \n",
    "            atom_adj = atom_adj.index_put_((y, x), val, accumulate=False)\n",
    "            \n",
    "            atom_adj[range(n_atoms), range(n_atoms)] = 1\n",
    "            \n",
    "            atom_adj = (atom_adj > 0).float()\n",
    "            \n",
    "            return_dict['atom_adj'] = atom_adj\n",
    "            \n",
    "        if 'atom_contact' in self.return_data:\n",
    "            atom_contact = create_adj_mat(data['atom_contact'], atom2idx_dict, mat_type='atom_contact').T\n",
    "            \n",
    "            x = torch.LongTensor(atom_contact[0, :]).squeeze()\n",
    "            y = torch.LongTensor(atom_contact[1, :]).squeeze()\n",
    "            z = torch.LongTensor(atom_contact[2, :]).squeeze()\n",
    "\n",
    "            atom_contact = torch.zeros([n_atoms, n_atoms, 8]).index_put_((x, y, z),\n",
    "                                                                    torch.ones(len(x)))\n",
    "            atom_contact = atom_contact.index_put_((y, x, z), \n",
    "                                                   torch.ones(len(x)))\n",
    "            \n",
    "            return_dict['atom_contact'] = atom_contact\n",
    "            \n",
    "        if 'atom_mask' in self.return_data:\n",
    "            atom_mask = create_idx_list(data['atom_mask'], atom2idx_dict)\n",
    "            \n",
    "            masked_pos = torch.from_numpy(atom_mask)\n",
    "            \n",
    "            if self.add_covalent:\n",
    "                channels = 9\n",
    "            else:\n",
    "                channels = 8\n",
    "            \n",
    "            mask = torch.ones([n_atoms, n_atoms, channels])\n",
    "            mask[masked_pos, :, :] = 0\n",
    "            mask[:, masked_pos, :] = 0\n",
    "            \n",
    "            return_dict['atom_mask'] = mask\n",
    "            \n",
    "        if 'res_adj' in self.return_data:            \n",
    "            adj = create_adj_mat(data['res_edges'], res2idx_dict, mat_type='res_graph').T\n",
    "            \n",
    "            x = torch.LongTensor(adj[0, :]).squeeze()\n",
    "            y = torch.LongTensor(adj[1, :]).squeeze()\n",
    "            \n",
    "            res_adj = torch.zeros([n_res, n_res]).index_put_((x, y), torch.ones(len(x)))\n",
    "            \n",
    "            res_adj = res_adj.index_put_((y, x), torch.ones(len(x)))\n",
    "            \n",
    "            res_adj[range(n_res), range(n_res)] = 1\n",
    "            \n",
    "            return_dict['res_adj'] = res_adj\n",
    "            \n",
    "        if 'res_contact' in self.return_data:\n",
    "            res_contact = create_adj_mat(data['res_contact'], res2idx_dict, mat_type='res_contact').T\n",
    "            \n",
    "            x = torch.LongTensor(res_contact[0, :]).squeeze()\n",
    "            y = torch.LongTensor(res_contact[1, :]).squeeze()\n",
    "            z = torch.LongTensor(res_contact[2, :]).squeeze()\n",
    "\n",
    "            res_contact = torch.zeros([n_res, n_res, 8]).index_put_((x, y, z),\n",
    "                                                                    torch.ones(len(x)))\n",
    "            \n",
    "            res_contact = res_contact.index_put_((y, x, z),\n",
    "                                                 torch.ones(len(x)))\n",
    "            \n",
    "            return_dict['res_contact'] = res_contact\n",
    "            \n",
    "        if 'res_mask' in self.return_data:\n",
    "            res_mask = create_idx_list(data['res_mask'], res2idx_dict)\n",
    "            \n",
    "            masked_pos = torch.from_numpy(res_mask)\n",
    "            \n",
    "            if self.add_covalent:\n",
    "                channels = 9\n",
    "            else:\n",
    "                channels = 8\n",
    "            \n",
    "            mask = torch.ones([n_res, n_res, channels])\n",
    "            mask[masked_pos, :, :] = 0\n",
    "            mask[:, masked_pos, :] = 0\n",
    "            \n",
    "            return_dict['res_mask'] = mask\n",
    "            \n",
    "        if 'mem_mat' in self.return_data:\n",
    "            mem_mat = create_mem_mat(atom2idx_dict, res2idx_dict).T\n",
    "            \n",
    "            x = torch.LongTensor(mem_mat[0, :]).squeeze()\n",
    "            y = torch.LongTensor(mem_mat[1, :]).squeeze()\n",
    "            \n",
    "            mem_mat = torch.zeros([n_res, n_atoms]).index_put_((x, y),\n",
    "                                                               torch.ones(len(x)))\n",
    "            \n",
    "            return_dict['mem_mat'] = mem_mat\n",
    "            \n",
    "        if 'idx2atom_dict' in self.return_data:\n",
    "            return_dict['idx2atom_dict'] = idx2atom_dict\n",
    "            \n",
    "        if 'idx2res_dict' in self.return_data:\n",
    "            return_dict['idx2res_dict'] = idx2res_dict\n",
    "            \n",
    "        self.data[idx] = return_dict\n",
    "            \n",
    "        # Return the processed data\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yn1eZ_eMbSlj"
   },
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6k6mfHhlbSlk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "####################\n",
    "# Embedding layers #\n",
    "####################\n",
    "\n",
    "class AtomEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Embed the atoms to fixed-length input vectors.\n",
    "    \n",
    "    Args:\n",
    "    - num_features (int) - Size of the returned embedding vectors.\n",
    "    - scale_grad_by_freq (bool) - Scale gradients by the inverse of\n",
    "        frequency (default=True).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_features, scale_grad_by_freq=True):\n",
    "        super(AtomEmbed, self).__init__()\n",
    "        self.embedding = nn.Embedding(118,\n",
    "                                      n_features,\n",
    "                                      scale_grad_by_freq=scale_grad_by_freq)\n",
    "        \n",
    "    def forward(self, atomic_numbers):\n",
    "        \"\"\"\n",
    "        Return the embeddings for each atom in the graph.\n",
    "        \n",
    "        Args:\n",
    "        - atoms (torch.LongTensor) - Tensor (n_atoms) containing atomic numbers.\n",
    "        \n",
    "        Returns:\n",
    "        - torch.FloatTensor of dimension (n_atoms, n_features) containing\n",
    "            the embedding vectors.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get and return the embeddings for each atom\n",
    "        embedded_atoms = self.embedding(atomic_numbers)\n",
    "        return embedded_atoms\n",
    "\n",
    "\n",
    "#########################\n",
    "# Position-aware layers #\n",
    "#########################\n",
    "\n",
    "# # PGNN layer, only pick closest node for message passing\n",
    "class PGNN_layer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim,dist_trainable=False):\n",
    "        super(PGNN_layer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.dist_trainable = dist_trainable\n",
    "\n",
    "        if self.dist_trainable:\n",
    "            self.dist_compute = Nonlinear(1, output_dim, 1)\n",
    "\n",
    "        self.linear_hidden = nn.Linear(input_dim*2, output_dim)\n",
    "        self.linear_out_position = nn.Linear(output_dim,1)\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    def forward(self, feature, dists_max, dists_argmax):\n",
    "        if self.dist_trainable:\n",
    "            dists_max = self.dist_compute(dists_max.unsqueeze(-1)).squeeze()\n",
    "\n",
    "        subset_features = feature[dists_argmax.flatten(), :]\n",
    "        subset_features = subset_features.reshape((dists_argmax.shape[0], dists_argmax.shape[1],\n",
    "                                                   feature.shape[1]))\n",
    "\n",
    "        messages = subset_features * dists_max.unsqueeze(-1)\n",
    "\n",
    "        self_feature = feature.unsqueeze(1).repeat(1, dists_max.shape[1], 1)\n",
    "        messages = torch.cat((messages, self_feature), dim=-1)\n",
    "\n",
    "        messages = self.linear_hidden(messages).squeeze()\n",
    "        messages = self.act(messages) # n*m*d\n",
    "\n",
    "        out_position = self.linear_out_position(messages).squeeze(-1)  # n*m_out\n",
    "        out_structure = torch.mean(messages, dim=1)  # n*d\n",
    "\n",
    "        return out_position, out_structure\n",
    "\n",
    "\n",
    "### Non linearity\n",
    "class Nonlinear(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Nonlinear, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = nn.init.xavier_uniform_(m.weight.data, gain=nn.init.calculate_gain('relu'))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data = nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HyQx2QqYbSlo"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NvIeIhAcbSlp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "########################################\n",
    "# Pairwise matrix generation functions #\n",
    "########################################\n",
    "\n",
    "def pairwise_mat(nodes, method='mean'):\n",
    "    \"\"\"\n",
    "    Generate matrix for pairwise determination of interactions.\n",
    "    \n",
    "    Args:\n",
    "    - nodes (torch.FloatTensor) - Tensor of node (n_nodes, n_features) features.\n",
    "    - method (str) - One of 'sum' or 'mean' for combination startegy for\n",
    "        pairwise combination matrix (default = 'mean').\n",
    "        \n",
    "    Returns:\n",
    "    - torch.FloatTensor of shape (n_pairwise, n_nodes) than can be used used to\n",
    "        combine feature vectors. Values are 1 if method == \"sum\" and 0.5 if\n",
    "        method == \"mean\".\n",
    "    \"\"\" \n",
    "\n",
    "    # Get the upper triangle indices\n",
    "    triu = np.vstack(np.triu_indices(nodes.shape[0]))\n",
    "    \n",
    "    # Loop through all indices and add to list with \n",
    "    idxs = torch.from_numpy(triu).T\n",
    "    \n",
    "    # Convert to tensor\n",
    "    combos = torch.zeros([idxs.shape[0], nodes.shape[0]]).scatter(1, idxs, 1)\n",
    "    \n",
    "    # Set to 0.5 if method is 'mean'\n",
    "    if method == 'mean':\n",
    "        combos *= 0.5\n",
    "        \n",
    "    return combos\n",
    "\n",
    "\n",
    "def pairwise_3d(nodes):\n",
    "    # Get the upper triangle indices\n",
    "    repeated_nodes = nodes.unsqueeze(0).expand(nodes.shape[0], -1, -1)\n",
    "    repeated_nodes2 = repeated_nodes.permute(1, 0, 2)\n",
    "    \n",
    "    return torch.cat((repeated_nodes, repeated_nodes2), dim=-1)\n",
    "\n",
    "\n",
    "############################\n",
    "# Upper triangle functions #\n",
    "############################\n",
    "\n",
    "def triu_condense(input_tensor):\n",
    "    \"\"\"\n",
    "    Condense the upper triangle of a tensor into a 2d dense representation.\n",
    "    \n",
    "    Args:\n",
    "    - input_tensor (torch.Tensor) - Tensor of shape (n, n, m).\n",
    "    \n",
    "    Returns:\n",
    "    - Tensor of shape (n(n+1)/2, m) where elements along the third dimension in\n",
    "        the original tensor are packed row-wise according to the upper\n",
    "        triangular indices.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get upper triangle index info\n",
    "    row_idx, col_idx = np.triu_indices(input_tensor.shape[0])\n",
    "    row_idx = torch.LongTensor(row_idx)\n",
    "    col_idx = torch.LongTensor(col_idx)\n",
    "    \n",
    "    # Return the packed matrix\n",
    "    output = input_tensor[row_idx, col_idx, :]\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def triu_expand(input_matrix):\n",
    "    \"\"\"\n",
    "    Expand a dense representation of the upper triangle of a tensor into \n",
    "    a 3D squareform representation.\n",
    "    \n",
    "    Args:\n",
    "    - input_matrix (torch.Tensor) - Tensor of shape (n(n+1)/2, m).\n",
    "    \n",
    "    Returns:\n",
    "    - Tensor of shape (n, n, m) where elements along the third dimension in the\n",
    "        original tensor are packed row-wise according to the upper triangular\n",
    "        indices.\n",
    "    \"\"\"\n",
    "    # Get the edge size n of the tensor\n",
    "    n_elements = input_matrix.shape[0]\n",
    "    n_chan = input_matrix.shape[1]\n",
    "    n_res = int((-1 + np.sqrt(1 + 4 * 2 * (n_elements))) / 2)\n",
    "    \n",
    "    # Get upper triangle index info\n",
    "    row_idx, col_idx = np.triu_indices(n_res)\n",
    "    row_idx = torch.LongTensor(row_idx)\n",
    "    col_idx = torch.LongTensor(col_idx)\n",
    "    \n",
    "    # Generate the output tensor\n",
    "    output = torch.zeros((n_res, n_res, n_chan))\n",
    "    \n",
    "    # Input the triu values\n",
    "    for i in range(n_chan):\n",
    "        i_tens = torch.full((len(row_idx),), i, dtype=torch.long)\n",
    "        output.index_put_((row_idx, col_idx, i_tens), input_matrix[:, i])\n",
    "    \n",
    "    # Input the tril values\n",
    "    for i in range(n_chan):\n",
    "        i_tens = torch.full((len(row_idx),), i, dtype=torch.long)\n",
    "        output.index_put_((col_idx, row_idx, i_tens), input_matrix[:, i])\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "###################\n",
    "# P-GNN functions #\n",
    "###################\n",
    "\n",
    "def generate_dists(adj_mat):\n",
    "    adj_mask = adj_mat == 0\n",
    "    \n",
    "    dist = adj_mat * (-1 * torch.eye(adj_mat.shape[0], device=adj_mat.device) + 1)\n",
    "    dist = 1 / (dist + 1)\n",
    "    dist[adj_mask] = 0\n",
    "    \n",
    "    return dist.squeeze()\n",
    "\n",
    "\n",
    "def get_dist_max(anchorset_id, dist):\n",
    "    dist_max = torch.zeros((dist.shape[0],len(anchorset_id)),\n",
    "                           device=dist.device)\n",
    "    dist_argmax = torch.zeros((dist.shape[0],len(anchorset_id)),\n",
    "                              device=dist.device).long()\n",
    "    for i in range(len(anchorset_id)):\n",
    "        temp_id = anchorset_id[i]\n",
    "        dist_temp = dist[:, temp_id]\n",
    "        dist_max_temp, dist_argmax_temp = torch.max(dist_temp, dim=-1)\n",
    "        dist_max[:,i] = dist_max_temp\n",
    "        dist_argmax[:,i] = dist_argmax_temp\n",
    "    return dist_max, dist_argmax\n",
    "\n",
    "\n",
    "def get_random_anchorset(n,c=0.5):\n",
    "    m = int(np.log2(n))\n",
    "    copy = int(c*m)\n",
    "    anchorset_id = []\n",
    "    for i in range(m):\n",
    "        anchor_size = int(n/np.exp2(i + 1))\n",
    "        for j in range(copy):\n",
    "            anchorset_id.append(np.random.choice(n,size=anchor_size,replace=False))\n",
    "    return anchorset_id\n",
    "\n",
    "\n",
    "def preselect_anchor(n_nodes, dists):\n",
    "    anchorset_id = get_random_anchorset(n_nodes, c=1)\n",
    "    return get_dist_max(anchorset_id, dists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.logging import LightningLoggerBase, rank_zero_only\n",
    "import wandb\n",
    "\n",
    "class WandBLogger(LightningLoggerBase):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(WandBLogger, self).__init__()\n",
    "        wandb.init(**kwargs)\n",
    "        self.project_name = 'Test'\n",
    "        \n",
    "    @property\n",
    "    def experiment(self):\n",
    "        return self\n",
    "        \n",
    "    @rank_zero_only\n",
    "    def watch(self, model):\n",
    "        wandb.watch(model)\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_hyperparams(self, params):\n",
    "        # params is an argparse.Namespace\n",
    "        # your code to record hyperparameters goes here\n",
    "        wandb.config.update(params)\n",
    "\n",
    "    @rank_zero_only\n",
    "    def log_metrics(self, metrics, step_num=None, commit=False):\n",
    "        # metrics is a dictionary of metric names and values\n",
    "        # your code to record metrics goes here\n",
    "        wandb.log(metrics, commit=commit)\n",
    "\n",
    "    def save(self):\n",
    "        # Optional. Any code necessary to save logger data goes here\n",
    "        pass\n",
    "\n",
    "    @rank_zero_only\n",
    "    def finalize(self, status):\n",
    "        # Optional. Any code that needs to be run after training\n",
    "        # finishes goes here\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.experiment.project_name\n",
    "    \n",
    "    @property\n",
    "    def version(self):\n",
    "        return wandb.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJZARlB3bSlt"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZTJ299UbSlv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class PGNN2D(pl.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim,\n",
    "                 anchorset_n, n_contact_channels, \n",
    "                 layer_num=2, train_data=None,\n",
    "                 val_data=None, test_data=None):\n",
    "        super(PGNN2D, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.n_contact_channels = n_contact_channels\n",
    "        self.layer_num = layer_num\n",
    "        \n",
    "        # Datasets\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "        \n",
    "        # Embedding\n",
    "        self.embed = AtomEmbed(input_dim)\n",
    "        \n",
    "        # First P-GNN layer\n",
    "        self.conv_atom_first = PGNN_layer(input_dim, hidden_dim)\n",
    "        \n",
    "        self.batch_norm_first = nn.BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # All other P-GNN layers\n",
    "        if layer_num>1:\n",
    "            self.conv_atom_hidden = nn.ModuleList([PGNN_layer(hidden_dim,\n",
    "                                                              hidden_dim)\n",
    "                                                  for i in range(layer_num - 1)])\n",
    "            \n",
    "            self.batch_norm_hidden = nn.ModuleList([nn.BatchNorm1d(hidden_dim)\n",
    "                                                   for i in range(layer_num - 1)])\n",
    "            \n",
    "\n",
    "        # All other res P-GNN layers\n",
    "        self.conv_res_hidden = nn.ModuleList([PGNN_layer(hidden_dim,\n",
    "                                                          hidden_dim)\n",
    "                                              for i in range(layer_num)])\n",
    "\n",
    "        self.batch_norm_res_hidden = nn.ModuleList([nn.BatchNorm1d(hidden_dim)\n",
    "                                               for i in range(layer_num)])\n",
    "        \n",
    "        self.batch_norm_res_dist_hidden = nn.BatchNorm1d(1)\n",
    "            \n",
    "            \n",
    "        \n",
    "        # RNN to condense position-aware embeddings\n",
    "        self.embed_condense = torch.nn.GRU(input_size=1, hidden_size=25, batch_first=True)\n",
    "        self.rnn_out_batch_norm = nn.BatchNorm1d(25)\n",
    "        \n",
    "        self.batch_norm_pairwise = nn.BatchNorm1d(25)\n",
    "        \n",
    "        # Linear distance\n",
    "        self.linear_dist = nn.Linear(25, 1)\n",
    "        \n",
    "        # Feed forward layers\n",
    "        self.linear1 = nn.Linear(25, 25)\n",
    "        self.linear1_batch_norm = nn.BatchNorm1d(25)\n",
    "        \n",
    "        self.linear2 = nn.Linear(25, 25)\n",
    "        self.linear2_batch_norm = nn.BatchNorm1d(25)\n",
    "        \n",
    "        self.linear3 = nn.Linear(25, 8)\n",
    "        \n",
    "        # Focal loss\n",
    "        self.loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        self.activations = {}\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        data['atom_dist'] = generate_dists(data['atom_adj'])\n",
    "\n",
    "        data['atom_dist_max'], data['atom_dist_argmax'] = preselect_anchor(data['atom_adj'].squeeze().shape[0], data['atom_dist'])\n",
    "\n",
    "        x = self.embed(data['atom_nodes'].squeeze())\n",
    "        \n",
    "        atom_embed = x.detach().cpu().numpy()\n",
    "        \n",
    "        x_position, x = self.conv_atom_first(x, data['atom_dist_max'], data['atom_dist_argmax'])\n",
    "        x = F.relu(x)\n",
    "        x = self.batch_norm_first(x)\n",
    "            \n",
    "        for i in range(self.layer_num-1):\n",
    "            \n",
    "            data['atom_dist'] = generate_dists(data['atom_adj'])\n",
    "\n",
    "            data['atom_dist_max'], data['atom_dist_argmax'] = preselect_anchor(data['atom_adj'].squeeze().shape[0], data['atom_dist'])\n",
    "            \n",
    "            x_position, x = self.conv_atom_hidden[i](x, data['atom_dist_max'], data['atom_dist_argmax'])\n",
    "            x = self.batch_norm_hidden[i](x)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "        atom_embed_update = x_position.detach().cpu().numpy()\n",
    "            \n",
    "        x = data['mem_mat'].squeeze().matmul(x)\n",
    "        \n",
    "        res_embed = x.detach().cpu().numpy()\n",
    "        \n",
    "        pairwise_matrix = pairwise_mat(x, method='sum')\n",
    "        \n",
    "        dists = []\n",
    "        \n",
    "        for i in range(self.layer_num-1):\n",
    "            \n",
    "            data['res_dist'] = generate_dists(data['res_adj'])\n",
    "\n",
    "            data['res_dist_max'], data['res_dist_argmax'] = preselect_anchor(data['res_adj'].squeeze().shape[0], data['res_dist'])\n",
    "            \n",
    "            x_position, x = self.conv_res_hidden[i](x, data['res_dist_max'], data['res_dist_argmax'])\n",
    "            x = self.batch_norm_res_hidden[i](x)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "            _, condensed = self.embed_condense(x_position.unsqueeze(-1))\n",
    "            \n",
    "            pairwise = pairwise_matrix.matmul(condensed.squeeze())\n",
    "            \n",
    "            data['res_adj'] = triu_expand(1 / torch.sigmoid(self.linear_dist(pairwise))).squeeze()\n",
    "            \n",
    "            dists.append(data['res_adj'].detach().cpu().numpy())\n",
    "            \n",
    "        \n",
    "        pairwise = self.batch_norm_pairwise(pairwise)\n",
    "        \n",
    "        x_new = self.linear1(pairwise)\n",
    "        x_new = self.linear1_batch_norm(x_new)\n",
    "        \n",
    "        x_new = self.linear2(x_new)\n",
    "        x_new = self.linear2_batch_norm(x_new)\n",
    "        \n",
    "        preds = self.linear3(x_new)\n",
    "        \n",
    "        self.activations = {\n",
    "            'dists': dists,\n",
    "            'atom_embed': atom_embed,\n",
    "            'atom_embed_update': atom_embed_update,\n",
    "            'res_embed': res_embed,\n",
    "            'combined': pairwise.detach().cpu().numpy(),\n",
    "            'preds': preds.detach().cpu().numpy()\n",
    "        }\n",
    "\n",
    "        return preds\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        y_hat = self.forward(batch)\n",
    "        y = triu_condense(batch['res_contact'].squeeze())\n",
    "        loss = self.loss(y_hat, y)\n",
    "        self.metrics_dict = {'loss': loss}\n",
    "        \n",
    "        if self.logger is not None:\n",
    "            self.logger.experiment.log_metrics(self.metrics_dict, self.global_step, commit=True)\n",
    "    \n",
    "        return self.metrics_dict\n",
    "    \n",
    "#     def on_post_performance_check(self):\n",
    "#         self.logger.experiment.log_metrics(self.metrics_dict)\n",
    "#         self.metrics_dict = {}\n",
    "\n",
    "    def configure_optimizers(self):        \n",
    "        parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "        return torch.optim.SGD(parameters, lr=.1, momentum=0.9)\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, shuffle=True, num_workers=10, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_channels(values):\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))\n",
    "    \n",
    "    ax = ax.flatten()\n",
    "    \n",
    "    channel_names = [\n",
    "        'Hydrophobic',\n",
    "        'Hydrogen bond',\n",
    "        'Van der Waals',\n",
    "        'Water bridges',\n",
    "        'Salt bridges',\n",
    "        'Pi-stacking',\n",
    "        'Pi-cation',\n",
    "        'T-stacking'\n",
    "    ]\n",
    "    \n",
    "    for channel in range(preds.shape[-1]):\n",
    "        ax[channel].imshow(values[:, :, channel].squeeze(), vmin=0, vmax=1)\n",
    "        ax[channel].set(title=channel_names[channel], xlabel='Residue #', ylabel='Residue #')\n",
    "                        \n",
    "    fig.show()\n",
    "    \n",
    "\n",
    "######################\n",
    "# ROC and PRC curves #\n",
    "######################\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "\n",
    "def calc_metric_curve(preds, target, curve_type, squareform=False):\n",
    "    \"\"\"\n",
    "    Calculate ROC or PRC curves and area for the predicted contact channels.\n",
    "    \n",
    "    Args:\n",
    "    - preds (np.ndarray) - Numpy array of model predictions either of form\n",
    "        (n_res, n_res, n_chan) or (n_res * [n_res - 1] / 2, n_chan).\n",
    "    - target (np.ndarray) - Numpy array of target values either of form\n",
    "        (n_res, n_res, n_chan) or (n_res * [n_res - 1] / 2, n_chan),\n",
    "        must match form of preds.\n",
    "    - curve_type (str) - One of 'ROC' or 'PRC' to denote type of curve.\n",
    "    - squareform (bool) - True if tensors are of shape (n_res, n_res, n_chan),\n",
    "        False if they are of shape (n_res * [n_res - 1] / 2, n_chan)\n",
    "        (default = True).\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of x, y, and AUC values to be used for plotting the curves\n",
    "        using plot_curve metric.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get correct curve function\n",
    "    if curve_type.upper() == 'ROC':\n",
    "        curve_func = roc_curve\n",
    "    elif curve_type.upper() == 'PRC':\n",
    "        curve_func = precision_recall_curve\n",
    "        \n",
    "    # Generate dicts to hold outputs from curve generation functions\n",
    "    x = dict()\n",
    "    y = dict()\n",
    "    auc_ = dict()\n",
    "    \n",
    "    # Handle case of squareform matrix (only get non-redundant triu indices)\n",
    "    if squareform:\n",
    "        indices = np.triu_indices(target.shape[0])\n",
    "\n",
    "    # For each channel\n",
    "    for i in range(target.shape[-1]):\n",
    "        \n",
    "        # Handle case of squareform\n",
    "        if squareform:\n",
    "            var1, var2, _ = curve_func(target[:, :, i][indices],\n",
    "                                       preds[:, :, i][indices])\n",
    "            \n",
    "        # Handle case of pairwise\n",
    "        else:\n",
    "            var1, var2, _ = curve_func(target[:, i], preds[:, i])\n",
    "        \n",
    "        # Assign outputs to correct dict for plotting\n",
    "        if curve_type.upper() == 'ROC':\n",
    "            x[i] = var1\n",
    "            y[i] = var2\n",
    "        elif curve_type.upper() == 'PRC':\n",
    "            x[i] = var2\n",
    "            y[i] = var1\n",
    "        \n",
    "        # Calc AUC\n",
    "        auc_[i] = auc(x[i], y[i])\n",
    "        \n",
    "    return (x, y, auc_)\n",
    "\n",
    "\n",
    "def plot_curve_metric(x, y, auc, curve_type, title=None, labels=None):\n",
    "    \"\"\"\n",
    "    Plot ROC or PRC curves per output channel.\n",
    "    \n",
    "    Args:\n",
    "    - x (dict) - Dict of numpy arrays for values to plot on x axis.\n",
    "    - y (dict) - Dict of numpy arrays for values to plot on x axis.\n",
    "    - auc (dict) - Dict of numpy arrays for areas under each curve.\n",
    "    - curve_type (str) - One of 'ROC' or 'PRC' to denote type of curve.\n",
    "    - title\n",
    "    - labels\n",
    "    \n",
    "    Returns:\n",
    "    - pyplot object of curves. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate figure\n",
    "    plt.figure()\n",
    "    \n",
    "    # Linetype spec\n",
    "    lw = 2\n",
    "    curve_type = curve_type.upper()\n",
    "    \n",
    "    # Get the number of channels being plotted\n",
    "    n_chan = len(x)\n",
    "    \n",
    "    # Make labels numeric if not provided\n",
    "    if labels is None:\n",
    "        labels = list(range(n_chan))\n",
    "    \n",
    "    # Check to make sure the labels are the right length\n",
    "    if len(labels) != n_chan:\n",
    "        raise ValueError('Number of labels ({}) does not match number of prediction channels ({}).'.format(len(labels), n_chan))\n",
    "    \n",
    "    # Get a lit of colors for all the channels\n",
    "    color_list = plt.cm.Set1(np.linspace(0, 1, n_chan))\n",
    "    \n",
    "    # Plot each line\n",
    "    for i, color in enumerate(color_list):\n",
    "        plt.plot(x[i], y[i], color=color,\n",
    "                 lw=lw, label='{} (area = {:0.2f})'.format(labels[i], auc[i]))\n",
    "        \n",
    "    # Add labels and diagonal line for ROC\n",
    "    if curve_type == 'ROC':\n",
    "        xlab = 'FPR'\n",
    "        ylab = 'TPR'\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # Add labels for PRC \n",
    "    elif curve_type == 'PRC':\n",
    "        xlab = 'Recall'\n",
    "        ylab = 'Precision'\n",
    "        plt.legend(loc=\"lower left\")\n",
    "    \n",
    "    # Extend limits, add labels and title\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "    \n",
    "    if title is not None:\n",
    "        plt.title('{} for {}'.format(curve_type, title))\n",
    "    else:\n",
    "        plt.title('{}'.format(curve_type))\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_curve(preds, target, curve_type, title=None, labels=None,\n",
    "               squareform=False):\n",
    "    \"\"\"\n",
    "    Wrapper to directly plot curves from model output and target.\n",
    "    \n",
    "    Args:\n",
    "    - preds (np array-like) - Array or tensor of predicted values output by\n",
    "        model.\n",
    "    - target (np array-like) - Array or tensor of target values.\n",
    "    - curve_type (str) - One of 'ROC' or 'PRC'.\n",
    "    - title (str) - Title of plot (default = None).\n",
    "    - labels (list) - List of labels for each channel on the plot\n",
    "        (default = None).\n",
    "    - squareform (bool) - Whether the predictions and targets are in square form\n",
    "        (default = False).\n",
    "    \"\"\"\n",
    "    x, y, auc_ = calc_metric_curve(preds, target, curve_type, squareform)\n",
    "    return plot_curve_metric(x, y, auc_, curve_type, title, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3QZxiiHbSl1"
   },
   "source": [
    "## Training\n",
    "\n",
    "### Instantiate dataloader and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6gcL3lpbbSl3"
   },
   "outputs": [],
   "source": [
    "data_base = '/home/tshimko/tesselate/'\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "train_data = TesselateDataset(data_base + 'test2.txt', graph_dir=data_base + 'data/graphs',\n",
    "                             contacts_dir=data_base + 'data/contacts',\n",
    "                              return_data='all', in_memory=True)\n",
    "\n",
    "model = PGNN2D(input_dim=10, hidden_dim=10, output_dim=10,\n",
    "             anchorset_n=int(np.log2(303))**2, n_contact_channels=8,\n",
    "             layer_num=4,\n",
    "             train_data=train_data,\n",
    "             val_data=None, test_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11611,
     "status": "ok",
     "timestamp": 1574002255203,
     "user": {
      "displayName": "Tyler Carter Shimko",
      "photoUrl": "",
      "userId": "18332645348589660395"
     },
     "user_tz": -60
    },
    "id": "od4NiBExbSl7",
    "outputId": "5fc65a3e-de61-40e7-afdf-eb5de7e1ef4b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/TShimko126/tesselate\" target=\"_blank\">https://app.wandb.ai/TShimko126/tesselate</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/TShimko126/tesselate/runs/ktlnosvd\" target=\"_blank\">https://app.wandb.ai/TShimko126/tesselate/runs/ktlnosvd</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to query for notebook name, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable\n",
      "INFO:root:                                      Name               Type Params\n",
      "0                                    embed          AtomEmbed    1 K\n",
      "1                          embed.embedding          Embedding    1 K\n",
      "2                          conv_atom_first         PGNN_layer  221  \n",
      "3            conv_atom_first.linear_hidden             Linear  210  \n",
      "4      conv_atom_first.linear_out_position             Linear   11  \n",
      "5                      conv_atom_first.act               ReLU    0  \n",
      "6                         batch_norm_first        BatchNorm1d   20  \n",
      "7                         conv_atom_hidden         ModuleList  663  \n",
      "8                       conv_atom_hidden.0         PGNN_layer  221  \n",
      "9         conv_atom_hidden.0.linear_hidden             Linear  210  \n",
      "10  conv_atom_hidden.0.linear_out_position             Linear   11  \n",
      "11                  conv_atom_hidden.0.act               ReLU    0  \n",
      "12                      conv_atom_hidden.1         PGNN_layer  221  \n",
      "13        conv_atom_hidden.1.linear_hidden             Linear  210  \n",
      "14  conv_atom_hidden.1.linear_out_position             Linear   11  \n",
      "15                  conv_atom_hidden.1.act               ReLU    0  \n",
      "16                      conv_atom_hidden.2         PGNN_layer  221  \n",
      "17        conv_atom_hidden.2.linear_hidden             Linear  210  \n",
      "18  conv_atom_hidden.2.linear_out_position             Linear   11  \n",
      "19                  conv_atom_hidden.2.act               ReLU    0  \n",
      "20                       batch_norm_hidden         ModuleList   60  \n",
      "21                     batch_norm_hidden.0        BatchNorm1d   20  \n",
      "22                     batch_norm_hidden.1        BatchNorm1d   20  \n",
      "23                     batch_norm_hidden.2        BatchNorm1d   20  \n",
      "24                         conv_res_hidden         ModuleList  884  \n",
      "25                       conv_res_hidden.0         PGNN_layer  221  \n",
      "26         conv_res_hidden.0.linear_hidden             Linear  210  \n",
      "27   conv_res_hidden.0.linear_out_position             Linear   11  \n",
      "28                   conv_res_hidden.0.act               ReLU    0  \n",
      "29                       conv_res_hidden.1         PGNN_layer  221  \n",
      "30         conv_res_hidden.1.linear_hidden             Linear  210  \n",
      "31   conv_res_hidden.1.linear_out_position             Linear   11  \n",
      "32                   conv_res_hidden.1.act               ReLU    0  \n",
      "33                       conv_res_hidden.2         PGNN_layer  221  \n",
      "34         conv_res_hidden.2.linear_hidden             Linear  210  \n",
      "35   conv_res_hidden.2.linear_out_position             Linear   11  \n",
      "36                   conv_res_hidden.2.act               ReLU    0  \n",
      "37                       conv_res_hidden.3         PGNN_layer  221  \n",
      "38         conv_res_hidden.3.linear_hidden             Linear  210  \n",
      "39   conv_res_hidden.3.linear_out_position             Linear   11  \n",
      "40                   conv_res_hidden.3.act               ReLU    0  \n",
      "41                   batch_norm_res_hidden         ModuleList   80  \n",
      "42                 batch_norm_res_hidden.0        BatchNorm1d   20  \n",
      "43                 batch_norm_res_hidden.1        BatchNorm1d   20  \n",
      "44                 batch_norm_res_hidden.2        BatchNorm1d   20  \n",
      "45                 batch_norm_res_hidden.3        BatchNorm1d   20  \n",
      "46              batch_norm_res_dist_hidden        BatchNorm1d    2  \n",
      "47                          embed_condense                GRU    2 K\n",
      "48                      rnn_out_batch_norm        BatchNorm1d   50  \n",
      "49                     batch_norm_pairwise        BatchNorm1d   50  \n",
      "50                             linear_dist             Linear   26  \n",
      "51                                 linear1             Linear  650  \n",
      "52                      linear1_batch_norm        BatchNorm1d   50  \n",
      "53                                 linear2             Linear  650  \n",
      "54                      linear2_batch_norm        BatchNorm1d   50  \n",
      "55                                 linear3             Linear  208  \n",
      "56                                    loss  BCEWithLogitsLoss    0  \n",
      "Epoch 1:   0%|          | 0/1 [00:00<?, ?batch/s]INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/config.yaml\n",
      "INFO:wandb.run_manager:file/dir created: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-history.jsonl\n",
      "INFO:wandb.run_manager:file/dir created: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-summary.json\n",
      "INFO:wandb.run_manager:file/dir created: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/diff.patch\n",
      "INFO:wandb.run_manager:file/dir created: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-metadata.json\n",
      "INFO:wandb.run_manager:file/dir created: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-events.jsonl\n",
      "INFO:wandb.run_manager:file/dir created: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/requirements.txt\n",
      "Epoch 1: 100%|██████████| 1/1 [00:00<00:00,  1.40batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.7438, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)} 0 True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/1 [00:00<00:00,  1.40batch/s, batch_nb=0, loss=0.744, v_nb=0.8.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0} 0 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-history.jsonl\n",
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-summary.json\n",
      "Epoch 3:   0%|          | 0/1 [00:00<00:00,  1.47batch/s, batch_nb=0, loss=0.736, v_nb=0.8.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.7280, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)} 1 True\n",
      "{'epoch': 1} 1 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4:   0%|          | 0/1 [00:00<00:00,  1.52batch/s, batch_nb=0, loss=0.727, v_nb=0.8.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.7090, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)} 2 True\n",
      "{'epoch': 2} 2 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-history.jsonl\n",
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-summary.json\n",
      "Epoch 5:   0%|          | 0/1 [00:00<00:00,  1.51batch/s, batch_nb=0, loss=0.718, v_nb=0.8.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.6900, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)} 3 True\n",
      "{'epoch': 3} 3 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-history.jsonl\n",
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-summary.json\n",
      "Epoch 6:   0%|          | 0/1 [00:00<00:00,  1.53batch/s, batch_nb=0, loss=0.708, v_nb=0.8.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.6699, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)} 4 True\n",
      "{'epoch': 4} 4 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7:   0%|          | 0/1 [00:00<00:00,  1.55batch/s, batch_nb=0, loss=0.699, v_nb=0.8.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.6522, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)} 5 True\n",
      "{'epoch': 5} 5 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-history.jsonl\n",
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-summary.json\n",
      "Epoch 8:   0%|          | 0/1 [00:00<00:00,  1.50batch/s, batch_nb=0, loss=0.690, v_nb=0.8.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.6350, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)} 6 True\n",
      "{'epoch': 6} 6 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-history.jsonl\n",
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-summary.json\n",
      "Epoch 9:   0%|          | 0/1 [00:00<00:00,  1.49batch/s, batch_nb=0, loss=0.681, v_nb=0.8.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.6165, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)} 7 True\n",
      "{'epoch': 7} 7 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 1/1 [00:00<00:00,  1.51batch/s, batch_nb=0, loss=0.671, v_nb=0.8.16]INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-history.jsonl\n",
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-summary.json\n",
      "Epoch 10:   0%|          | 0/1 [00:00<00:00,  1.51batch/s, batch_nb=0, loss=0.671, v_nb=0.8.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.5973, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)} 8 True\n",
      "{'epoch': 8} 8 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11:   0%|          | 0/1 [00:00<00:00,  1.54batch/s, batch_nb=0, loss=0.662, v_nb=0.8.16]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.5754, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)} 9 True\n",
      "{'epoch': 9} 9 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-history.jsonl\n",
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-summary.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': tensor(0.5529, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)} 10 True\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5e320aa90004>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                   \u001b[0mearly_stop_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                   checkpoint_callback=False)\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_schedulers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;31m# return 1 when finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# update LR schedulers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0;31m# RUN TRAIN STEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# ---------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_nb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mbatch_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_step_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[0;34m(self, batch, batch_nb)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                 \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;31m# nan grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/pytorch_lightning/trainer/train_loop_mixin.py\u001b[0m in \u001b[0;36moptimizer_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m                     \u001b[0;31m# backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mmodel_ref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                     \u001b[0mmodel_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_amp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                     \u001b[0;31m# track metrics for callbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/pytorch_lightning/root_module/hooks.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, use_amp, loss, optimizer)\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:wandb.run_manager:shutting down system stats and metadata service\n",
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-history.jsonl\n",
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-summary.json\n",
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-events.jsonl\n",
      "INFO:wandb.run_manager:stopping streaming files and file change observer\n",
      "INFO:wandb.run_manager:file/dir modified: /home/tshimko/tesselate/notebooks/benchmarks/wandb/run-20191129_134616-ktlnosvd/wandb-metadata.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <function _init_jupyter.<locals>.cleanup at 0x7fc423884488> (for post_run_cell):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/backcall/backcall.py\u001b[0m in \u001b[0;36madapted\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m#            print(args, kwargs, unmatched_pos, cut_positional, unmatched_kw)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0madapted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m()\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;31m# shutdown async logger because _user_process_finished isn't called in jupyter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mshutdown_async_log_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m     \u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'post_run_cell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36m_stop_jupyter_agent\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_stop_jupyter_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munmirror_stdout_stderr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cloud\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stopping streaming files and file change observer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_end_file_syncing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36m_end_file_syncing\u001b[0;34m(self, exitcode)\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;31m# TODO: there was a case where _file_event_handlers was getting modified in the loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_event_handlers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m             \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file_pusher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tailer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tailer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tailer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/wandb/run_manager.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "logger = WandBLogger(entity='TShimko126', project='tesselate')\n",
    "logger.watch(model)\n",
    "\n",
    "trainer = Trainer(max_nb_epochs=15000,\n",
    "                  logger=logger,\n",
    "#                   logger=False,\n",
    "                  early_stop_callback=False,\n",
    "                  checkpoint_callback=False)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ycs4WmYKrqoE"
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acts = model.activations\n",
    "x = train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions\n",
    "preds = triu_expand(torch.from_numpy(acts['preds'])).sigmoid()\n",
    "plot_channels(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the true values\n",
    "plot_channels(x['res_contact'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curve(torch.from_numpy(acts['preds']).sigmoid(), triu_condense(x['res_contact'].squeeze()), 'ROC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curve(torch.from_numpy(acts['preds']).sigmoid(), triu_condense(x['res_contact'].squeeze()), 'PRC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pred 1: {}, {}\\nPred 2: {}, {}\\nPred 2: {}, {}'.format(acts['dists'][0].min(),\n",
    "                                                              acts['dists'][0].max(),\n",
    "                                                              acts['dists'][1].min(),\n",
    "                                                              acts['dists'][1].max(),\n",
    "                                                              acts['dists'][2].min(),\n",
    "                                                              acts['dists'][2].max()))\n",
    "\n",
    "fig, ax = plt.subplots(1, 3)\n",
    "ax[0].imshow(acts['dists'][0], )\n",
    "ax[1].imshow(acts['dists'][1])\n",
    "ax[2].imshow(acts['dists'][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "p-gnn2D_four_channel_benchmark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
