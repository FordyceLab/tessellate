{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXUYGCpNbSla"
   },
   "source": [
    "# Benchmark position-aware graph neural network/2D CNN architecture\n",
    "\n",
    "This notebook contains all of the code to overfit a GAT to four contact channels of a single structure (6E6O).\n",
    "\n",
    "## Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRzY2E5pbYm6"
   },
   "source": [
    "### Dataloader code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jf5S3ZSabSlc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import periodictable as pt\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def read_files(acc, model, graph_dir, contacts_dir):\n",
    "    \"\"\"\n",
    "    Read graph and contacts files.\n",
    "\n",
    "    Args:\n",
    "    - acc (str) - String of the PDB ID (lowercese).\n",
    "    - model (int) - Model number of the desired bioassembly.\n",
    "    - graph_dir (str) - Directory containing the nodes, edges,\n",
    "        and mask files.\n",
    "    - contacts_dir (str) - Directory containing the .contacts\n",
    "        files from get_contacts.py.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary of DataFrames and lists corresponding to\n",
    "        graph nodes, edges, mask, and contacts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the file names for the graph files\n",
    "    node_file = os.path.join(graph_dir, '{}-{}_nodes.csv'.format(acc, model))\n",
    "    edge_file = os.path.join(graph_dir, '{}-{}_edges.csv'.format(acc, model))\n",
    "    mask_file = os.path.join(graph_dir, '{}-{}_mask.csv'.format(acc, model))\n",
    "\n",
    "    # Get the contacts file\n",
    "    contacts_file = os.path.join(contacts_dir, '{}-{}.contacts'.format(acc, model))\n",
    "\n",
    "    # Read the nodes and edges\n",
    "    nodes = pd.read_csv(node_file)\n",
    "    edges = pd.read_csv(edge_file)\n",
    "\n",
    "    # Check if the mask is empty\n",
    "    if os.path.getsize(mask_file) > 0:\n",
    "        with open(mask_file) as f:\n",
    "            mask = f.read().split('\\n')\n",
    "    else:\n",
    "        mask = []\n",
    "\n",
    "    # Read the contacts\n",
    "    contacts = pd.read_table(contacts_file, sep='\\t',\n",
    "                             header=None, names=['type', 'start', 'end'])\n",
    "\n",
    "    # Return the data\n",
    "    data = {\n",
    "        'nodes': nodes,\n",
    "        'edges': edges,\n",
    "        'mask': mask,\n",
    "        'contacts': contacts\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_res_data(data):\n",
    "    \"\"\"\n",
    "    Process residue-level data from atom-level data.\n",
    "\n",
    "    Args:\n",
    "    - data (dict) - Dictionary of graph data output from `read_files`.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary of atom and residue graph and contact data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract data form dict\n",
    "    nodes = data['nodes']\n",
    "    edges = data['edges']\n",
    "    mask = data['mask']\n",
    "    contacts = data['contacts']\n",
    "\n",
    "    # Get residue nodes\n",
    "    res_nodes = pd.DataFrame()\n",
    "    res_nodes['res'] = [':'.join(atom.split(':')[:3]) for atom in nodes['atom']]\n",
    "    res_nodes = res_nodes.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Get residue edges\n",
    "    res_edges = edges.copy()\n",
    "    res_edges['start'] = [':'.join(atom.split(':')[:3]) for atom in res_edges['start']]\n",
    "    res_edges['end'] = [':'.join(atom.split(':')[:3]) for atom in res_edges['end']]\n",
    "    res_edges = res_edges[res_edges['start'] != res_edges['end']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Get residue contacts\n",
    "    res_contacts = contacts.copy()\n",
    "    res_contacts['start'] = [':'.join(atom.split(':')[:3]) for atom in res_contacts['start']]\n",
    "    res_contacts['end'] = [':'.join(atom.split(':')[:3]) for atom in res_contacts['end']]\n",
    "    res_contacts = res_contacts[res_contacts['start'] != res_contacts['end']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Get residue mask\n",
    "    res_mask = list(set([':'.join(atom.split(':')[:3]) for atom in mask]))\n",
    "\n",
    "    # Return data dict\n",
    "    data = {\n",
    "        'atom_nodes': nodes,\n",
    "        'atom_edges': edges,\n",
    "        'atom_contact': contacts,\n",
    "        'atom_mask': mask,\n",
    "        'res_nodes': res_nodes,\n",
    "        'res_edges': res_edges,\n",
    "        'res_contact': res_contacts,\n",
    "        'res_mask': res_mask\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_map_dicts(entity_list):\n",
    "    \"\"\"\n",
    "    Map identifiers to indices and vice versa.\n",
    "\n",
    "    Args:\n",
    "    - entity_list (list) - List of entities (atoms, residues, etc.)\n",
    "        to index.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of the entity to index and index to entity dicts, respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the entity:index dictionary\n",
    "    ent2idx_dict = {entity: idx for idx, entity in enumerate(entity_list)}\n",
    "\n",
    "    # Create the index:entity dictionary\n",
    "    idx2ent_dict = {idx: entity for entity, idx in ent2idx_dict.items()}\n",
    "\n",
    "    return (ent2idx_dict, idx2ent_dict)\n",
    "\n",
    "\n",
    "def create_adj_mat(data, dict_map, mat_type):\n",
    "    \"\"\"\n",
    "    Creates an adjacency matrix.\n",
    "\n",
    "    Args:\n",
    "    - data (DataFrame) - Dataframe with 'start' and 'end' column\n",
    "        for each interaction. For atom-level adjacency, 'order'\n",
    "        column is also required. For atom or residue conatcts,\n",
    "        'type' column is also required.\n",
    "\n",
    "    Returns:\n",
    "    - Coordinate format matrix (numpy). For atom adjacency, third column\n",
    "        corresponds to bond order. For contacts, third column\n",
    "        corresponds to channel.\n",
    "\n",
    "    Channel mappings (shorthand from get_contacts.py source):\n",
    "\n",
    "        0:\n",
    "            hp             hydrophobic interactions\n",
    "        1:\n",
    "            hb             hydrogen bonds\n",
    "            lhb            ligand hydrogen bonds\n",
    "            hbbb           backbone-backbone hydrogen bonds\n",
    "            hbsb           backbone-sidechain hydrogen bonds\n",
    "            hbss           sidechain-sidechain hydrogen bonds\n",
    "            hbls           ligand-sidechain residue hydrogen bonds\n",
    "            hblb           ligand-backbone residue hydrogen bonds\n",
    "        2:\n",
    "            vdw            van der Waals\n",
    "        3:\n",
    "            wb             water bridges\n",
    "            wb2            extended water bridges\n",
    "            lwb            ligand water bridges\n",
    "            lwb2           extended ligand water bridges\n",
    "        4:\n",
    "            sb             salt bridges\n",
    "        5:\n",
    "            ps             pi-stacking\n",
    "        6:\n",
    "            pc             pi-cation\n",
    "        7:\n",
    "            ts             t-stacking\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the coordinate list\n",
    "    coord_mat = []\n",
    "\n",
    "    # Map channel names to numeric channels\n",
    "    channel = {\n",
    "        # Hydrophobic interactions in first channel\n",
    "        'hp': 0,\n",
    "        'hplp': 0,\n",
    "        'hpll': 0,\n",
    "\n",
    "        # Hydrogen bonds in second channel\n",
    "        'hb': 1,\n",
    "        'lhb': 1,\n",
    "        'hbbb': 1,\n",
    "        'hbsb': 1,\n",
    "        'hbss': 1,\n",
    "        'hbls': 1,\n",
    "        'hblb': 1,\n",
    "\n",
    "        # VdW in third channel\n",
    "        'vdw': 2,\n",
    "\n",
    "        # Water bridges\n",
    "        'wb': 3,\n",
    "        'wb2': 3,\n",
    "        'lwb': 3,\n",
    "        'lwb2': 3,\n",
    "\n",
    "        # Salt bridges\n",
    "        'sb': 4,\n",
    "        'sbpl': 4,\n",
    "\n",
    "        # Other interactions\n",
    "        'ps': 5,\n",
    "        'pc': 6,\n",
    "        'ts': 7,\n",
    "    }\n",
    "\n",
    "    # Assemble the contacts\n",
    "    for idx, row in data.iterrows():\n",
    "\n",
    "        if row['start'] in dict_map and row['end'] in dict_map:\n",
    "\n",
    "            entry = [dict_map[row['start']], dict_map[row['end']]]\n",
    "\n",
    "            # Add order or type if necessary\n",
    "            if mat_type == 'atom_graph':\n",
    "                entry.append(row['order'])\n",
    "            elif mat_type == 'atom_contact':\n",
    "                entry.append(channel[row['type']])\n",
    "            elif mat_type == 'res_contact':\n",
    "                entry.append(channel[row['type']])\n",
    "\n",
    "            coord_mat.append(entry)\n",
    "\n",
    "    return np.array(coord_mat)\n",
    "\n",
    "\n",
    "def create_conn_adj_mat(adj):\n",
    "    \"\"\"\n",
    "    Create connection adjacency matrix\n",
    "    \"\"\"\n",
    "\n",
    "    conn_map = {(a, b): idx for idx, (a, b) in enumerate(zip(*np.triu_indices_from(adj)))}\n",
    "\n",
    "    one_hop_neighbors = {idx: np.argwhere(row > 0).squeeze().tolist() for idx, row in enumerate(adj)}\n",
    "\n",
    "    conns = []\n",
    "\n",
    "    for i, j in it.combinations_with_replacement(one_hop_neighbors, 2):\n",
    "        row = conn_map[(i, j)]\n",
    "        adj_conn_coords = set([(m, n) if m <= n else (n, m)\n",
    "                               for m, n in it.product(one_hop_neighbors[i],\n",
    "                                                      one_hop_neighbors[j])])\n",
    "        adj_conns = [conn_map[x] for x in adj_conn_coords]\n",
    "\n",
    "        conns.append(np.array(list(it.product([row], adj_conns))))\n",
    "\n",
    "    conn_adj = np.concatenate(conns, axis=0).T\n",
    "\n",
    "    return conn_adj\n",
    "\n",
    "\n",
    "def create_mem_mat(atom_dict, res_dict):\n",
    "    \"\"\"\n",
    "    Create a membership matrix mapping atoms to residues.\n",
    "\n",
    "    Args:\n",
    "    - atom_dict (dict) - Dictionary mapping atoms to indices.\n",
    "    - res_dict (dict) - Dictionary mapping residues to indices.\n",
    "\n",
    "    Returns:\n",
    "    - Coordinate format membership matrix (numpy) with first\n",
    "        row being residue number and the second column being\n",
    "        atom number.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the coordinate list\n",
    "    mem_coord = []\n",
    "\n",
    "    # Map atoms to residues\n",
    "    for atom, atom_idx in atom_dict.items():\n",
    "        res_idx = res_dict[':'.join(atom.split(':')[:3])]\n",
    "\n",
    "        mem_coord.append([res_idx, atom_idx])\n",
    "\n",
    "    mem_coord = np.array(mem_coord)\n",
    "\n",
    "    return mem_coord\n",
    "\n",
    "\n",
    "def create_idx_list(id_list, dict_map):\n",
    "    \"\"\"\n",
    "    Create list of indices.\n",
    "\n",
    "    Args:\n",
    "    - id_list (list) - List of masked atom or residue identifiers.\n",
    "    - dict_map (dict) - Dictionary mapping entities to indices.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the masked indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the numpy index array\n",
    "    idx_array = np.array([dict_map[iden] for iden in id_list])\n",
    "\n",
    "    return idx_array\n",
    "\n",
    "\n",
    "class TesselateDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for structural data.\n",
    "\n",
    "    Args:\n",
    "    - accession_list (str) - File path from which to read PDB IDs for dataset.\n",
    "    - graph_dir (str) - Directory containing the nodes, edges, and mask files.\n",
    "    - contacts_dir (str) - Directory containing the .contacts files from\n",
    "        get_contacts.py.\n",
    "    - return_data (list) - List of datasets to return. Value must be 'all' or\n",
    "        a subset of the following list:\n",
    "            - pdb_id\n",
    "            - model\n",
    "            - atom_nodes\n",
    "            - atom_adj\n",
    "            - atom_contact\n",
    "            - atom_mask\n",
    "            - res_adj\n",
    "            - res_dist\n",
    "            - res_contact\n",
    "            - res_mask\n",
    "            - mem_mat\n",
    "            - idx2atom_dict\n",
    "            - idx2res_dict\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, accession_list, graph_dir, contacts_dir, add_covalent=False, return_data='all', in_memory=False):\n",
    "\n",
    "        if return_data == 'all':\n",
    "            self.return_data = [\n",
    "                'pdb_id',\n",
    "                'model',\n",
    "                'atom_nodes',\n",
    "                'atom_adj',\n",
    "                'atom_contact',\n",
    "                'atom_mask',\n",
    "                'res_adj',\n",
    "                'res_dist',\n",
    "                'conn_adj',\n",
    "                'res_contact',\n",
    "                'res_mask',\n",
    "                'mem_mat',\n",
    "                'idx2atom_dict',\n",
    "                'idx2res_dict'\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            self.return_data = return_data\n",
    "\n",
    "        # Store reference to accession list file\n",
    "        self.accession_list = accession_list\n",
    "\n",
    "        # Store references to the necessary directories\n",
    "        self.graph_dir = graph_dir\n",
    "        self.contacts_dir = contacts_dir\n",
    "\n",
    "        # Whether to add covalent bonds to prediction task and\n",
    "        # remove sequence non-deterministic covalent bonds from the adjacency matrix\n",
    "        self.add_covalent=add_covalent\n",
    "\n",
    "        # Read in and store a list of accession IDs\n",
    "        with open(accession_list, 'r') as handle:\n",
    "            self.accessions = np.array([acc.strip().lower().split() for acc in handle.readlines()])\n",
    "\n",
    "        self.data = {}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - Integer count of number of examples.\n",
    "        \"\"\"\n",
    "        return len(self.accessions)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an item with a particular index value.\n",
    "\n",
    "        Args:\n",
    "        - idx (int) - Index of desired sample.\n",
    "\n",
    "        Returns:\n",
    "        - Dictionary of dataset example. All tensors are sparse when possible.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            if idx in self.data:\n",
    "                return self.data[idx]\n",
    "\n",
    "            # initialize the return dictionary\n",
    "            return_dict = {}\n",
    "\n",
    "            acc_entry = self.accessions[idx]\n",
    "\n",
    "            # Get the PDB ID\n",
    "            acc = acc_entry[0]\n",
    "\n",
    "            # Get the model number if one exists\n",
    "            if len(acc_entry) == 1:\n",
    "                model = 1\n",
    "            else:\n",
    "                model = acc_entry[1]\n",
    "\n",
    "            # Read and process the files\n",
    "            data = read_files(acc, model, self.graph_dir, self.contacts_dir)\n",
    "            data = process_res_data(data)\n",
    "\n",
    "            # Generate the mapping dictionaries\n",
    "            atom2idx_dict, idx2atom_dict = get_map_dicts(data['atom_nodes']['atom'].unique())\n",
    "            res2idx_dict, idx2res_dict = get_map_dicts(data['res_nodes']['res'].unique())\n",
    "\n",
    "            # Get numbers of atoms and residues per sample\n",
    "            n_atoms = len(atom2idx_dict)\n",
    "            n_res = len(res2idx_dict)\n",
    "\n",
    "            # Handle all of the possible returned datasets\n",
    "            if 'pdb_id' in self.return_data:\n",
    "                return_dict['pdb_id'] = acc\n",
    "\n",
    "            if 'model' in self.return_data:\n",
    "                return_dict['model'] = model\n",
    "\n",
    "            if 'atom_nodes' in self.return_data:\n",
    "                ele_nums = [pt.elements.symbol(element).number for element in data['atom_nodes']['element']]\n",
    "                return_dict['atom_nodes'] = torch.LongTensor(ele_nums)\n",
    "                assert not torch.isnan(return_dict['atom_nodes']).any()\n",
    "\n",
    "            if 'atom_adj' in self.return_data:\n",
    "\n",
    "                adj = create_adj_mat(data['atom_edges'], atom2idx_dict, mat_type='atom_graph').T\n",
    "\n",
    "                x = torch.LongTensor(adj[0, :]).squeeze()\n",
    "                y = torch.LongTensor(adj[1, :]).squeeze()\n",
    "                val = torch.FloatTensor(adj[2, :]).squeeze()\n",
    "\n",
    "                atom_adj = torch.zeros([n_atoms, n_atoms]).index_put_((x, y), val, accumulate=False)\n",
    "\n",
    "                atom_adj = atom_adj.index_put_((y, x), val, accumulate=False)\n",
    "\n",
    "                atom_adj[range(n_atoms), range(n_atoms)] = 1\n",
    "\n",
    "                atom_adj = (atom_adj > 0).float()\n",
    "\n",
    "                return_dict['atom_adj'] = atom_adj\n",
    "\n",
    "                assert not torch.isnan(return_dict['atom_adj']).any()\n",
    "\n",
    "            if 'atom_contact' in self.return_data:\n",
    "                atom_contact = create_adj_mat(data['atom_contact'], atom2idx_dict, mat_type='atom_contact').T\n",
    "\n",
    "                x = torch.LongTensor(atom_contact[0, :]).squeeze()\n",
    "                y = torch.LongTensor(atom_contact[1, :]).squeeze()\n",
    "                z = torch.LongTensor(atom_contact[2, :]).squeeze()\n",
    "\n",
    "                atom_contact = torch.zeros([n_atoms, n_atoms, 8]).index_put_((x, y, z),\n",
    "                                                                        torch.ones(len(x)))\n",
    "                atom_contact = atom_contact.index_put_((y, x, z),\n",
    "                                                       torch.ones(len(x)))\n",
    "\n",
    "                return_dict['atom_contact'] = atom_contact\n",
    "\n",
    "                assert not torch.isnan(return_dict['atom_contact']).any()\n",
    "\n",
    "            if 'atom_mask' in self.return_data:\n",
    "                atom_mask = create_idx_list(data['atom_mask'], atom2idx_dict)\n",
    "\n",
    "                masked_pos = torch.from_numpy(atom_mask)\n",
    "\n",
    "                if self.add_covalent:\n",
    "                    channels = 9\n",
    "                else:\n",
    "                    channels = 8\n",
    "\n",
    "                mask = torch.ones([n_atoms, n_atoms, channels])\n",
    "\n",
    "                if len(masked_pos) > 0:\n",
    "                    mask[masked_pos, :, :] = 0\n",
    "                    mask[:, masked_pos, :] = 0\n",
    "\n",
    "                return_dict['atom_mask'] = mask\n",
    "\n",
    "                assert not torch.isnan(return_dict['atom_mask']).any()\n",
    "\n",
    "            if 'res_adj' in self.return_data:\n",
    "                adj = create_adj_mat(data['res_edges'], res2idx_dict, mat_type='res_graph').T\n",
    "\n",
    "                x = torch.LongTensor(adj[0, :]).squeeze()\n",
    "                y = torch.LongTensor(adj[1, :]).squeeze()\n",
    "\n",
    "                res_adj = torch.zeros([n_res, n_res]).index_put_((x, y), torch.ones(len(x)))\n",
    "\n",
    "                res_adj = res_adj.index_put_((y, x), torch.ones(len(x)))\n",
    "\n",
    "                res_adj[range(n_res), range(n_res)] = 1\n",
    "\n",
    "                return_dict['res_adj'] = res_adj\n",
    "\n",
    "                assert not torch.isnan(return_dict['res_adj']).any()\n",
    "\n",
    "                if 'res_dist' in self.return_data:\n",
    "                    G = nx.from_numpy_matrix(return_dict['res_adj'].numpy())\n",
    "                    res_dist = torch.from_numpy(nx.floyd_warshall_numpy(G)).float()\n",
    "\n",
    "                    res_dist[torch.isinf(res_dist)] = -1\n",
    "\n",
    "                    return_dict['res_dist'] = res_dist\n",
    "\n",
    "                    chain_mem = torch.zeros(res_dist.shape)\n",
    "                    chain_mem[~torch.isinf(return_dict['res_dist'])] = 1\n",
    "\n",
    "                    return_dict['chain_mem'] = chain_mem\n",
    "\n",
    "                    assert not torch.isnan(return_dict['res_dist']).any()\n",
    "                    assert not torch.isinf(return_dict['res_dist']).any()\n",
    "                    assert not torch.isnan(return_dict['chain_mem']).any()\n",
    "                    assert not torch.isinf(return_dict['chain_mem']).any()\n",
    "\n",
    "                if 'conn_adj' in self.return_data:\n",
    "\n",
    "                    conn_adj = create_conn_adj_mat(return_dict['res_adj'].numpy())\n",
    "\n",
    "                    return_dict['conn_adj'] = torch.from_numpy(conn_adj)\n",
    "\n",
    "            if 'res_contact' in self.return_data:\n",
    "                res_contact = create_adj_mat(data['res_contact'], res2idx_dict, mat_type='res_contact').T\n",
    "\n",
    "                x = torch.LongTensor(res_contact[0, :]).squeeze()\n",
    "                y = torch.LongTensor(res_contact[1, :]).squeeze()\n",
    "                z = torch.LongTensor(res_contact[2, :]).squeeze()\n",
    "\n",
    "                res_contact = torch.zeros([n_res, n_res, 8]).index_put_((x, y, z),\n",
    "                                                                        torch.ones(len(x)))\n",
    "\n",
    "                res_contact = res_contact.index_put_((y, x, z),\n",
    "                                                     torch.ones(len(x)))\n",
    "\n",
    "                return_dict['res_contact'] = res_contact\n",
    "\n",
    "                assert not torch.isnan(return_dict['res_contact']).any()\n",
    "\n",
    "            if 'res_mask' in self.return_data:\n",
    "                res_mask = create_idx_list(data['res_mask'], res2idx_dict)\n",
    "\n",
    "                masked_pos = torch.from_numpy(res_mask)\n",
    "\n",
    "                if self.add_covalent:\n",
    "                    channels = 9\n",
    "                else:\n",
    "                    channels = 8\n",
    "\n",
    "                mask = torch.ones([n_res, n_res, channels])\n",
    "\n",
    "                if len(masked_pos) > 0:\n",
    "                    mask[masked_pos, :, :] = 0\n",
    "                    mask[:, masked_pos, :] = 0\n",
    "\n",
    "                return_dict['res_mask'] = mask\n",
    "\n",
    "                assert not torch.isnan(return_dict['res_mask']).any()\n",
    "\n",
    "            if 'mem_mat' in self.return_data:\n",
    "                mem_mat = create_mem_mat(atom2idx_dict, res2idx_dict).T\n",
    "\n",
    "                x = torch.LongTensor(mem_mat[0, :]).squeeze()\n",
    "                y = torch.LongTensor(mem_mat[1, :]).squeeze()\n",
    "\n",
    "                mem_mat = torch.zeros([n_res, n_atoms]).index_put_((x, y),\n",
    "                                                                   torch.ones(len(x)))\n",
    "\n",
    "                return_dict['mem_mat'] = mem_mat\n",
    "\n",
    "                assert not torch.isnan(return_dict['mem_mat']).any()\n",
    "\n",
    "            if 'idx2atom_dict' in self.return_data:\n",
    "                return_dict['idx2atom_dict'] = idx2atom_dict\n",
    "\n",
    "            if 'idx2res_dict' in self.return_data:\n",
    "                return_dict['idx2res_dict'] = idx2res_dict\n",
    "\n",
    "            self.data[idx] = return_dict\n",
    "\n",
    "            # Return the processed data\n",
    "            return return_dict\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", acc, str(e))\n",
    "            return np.array([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yn1eZ_eMbSlj"
   },
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6k6mfHhlbSlk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from torch_scatter import scatter_softmax\n",
    "\n",
    "\n",
    "####################\n",
    "# Embedding layers #\n",
    "####################\n",
    "\n",
    "class AtomEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Embed the atoms to fixed-length input vectors.\n",
    "\n",
    "    Args:\n",
    "    - num_features (int) - Size of the returned embedding vectors.\n",
    "    - scale_grad_by_freq (bool) - Scale gradients by the inverse of\n",
    "        frequency (default=True).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features, scale_grad_by_freq=True):\n",
    "        super(AtomEmbed, self).__init__()\n",
    "        self.embedding = nn.Embedding(118,\n",
    "                                      n_features,\n",
    "                                      scale_grad_by_freq=scale_grad_by_freq)\n",
    "\n",
    "    def forward(self, atomic_numbers):\n",
    "        \"\"\"\n",
    "        Return the embeddings for each atom in the graph.\n",
    "\n",
    "        Args:\n",
    "        - atoms (torch.LongTensor) - Tensor (n_atoms) containing atomic numbers.\n",
    "\n",
    "        Returns:\n",
    "        - torch.FloatTensor of dimension (n_atoms, n_features) containing\n",
    "            the embedding vectors.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get and return the embeddings for each atom\n",
    "        embedded_atoms = self.embedding(atomic_numbers)\n",
    "        return embedded_atoms\n",
    "\n",
    "\n",
    "####################\n",
    "# Attention layers #\n",
    "####################\n",
    "\n",
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n",
    "\n",
    "\n",
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)\n",
    "\n",
    "\n",
    "class GraphAttnLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttnLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "\n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
    "\n",
    "        N = input.size()[0]\n",
    "\n",
    "        if adj.shape[0] == adj.shape[1]:\n",
    "            edge = adj.nonzero().t()\n",
    "        else:\n",
    "            edge = adj\n",
    "\n",
    "#         mask = adj.byte()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        values = self.leakyrelu(self.a.mm(edge_h).squeeze())\n",
    "\n",
    "        edge_e = scatter_softmax(values, edge[0])\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GraphAttn(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttn, self).__init__()\n",
    "        self.attn = GraphAttnLayer(in_features, out_features, dropout, alpha, concat)\n",
    "        self.batch_norm = nn.BatchNorm1d(out_features)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "\n",
    "        out = self.attn(input, adj)\n",
    "        out_norm = self.batch_norm(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadGraphAttn(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head graph attention layer.\n",
    "\n",
    "    Args:\n",
    "    - n_head (int) - Number of heads for the attention layer.\n",
    "    - in_features (int) - Number of total input features.\n",
    "    - out_features (int) - Number of output features per head.\n",
    "    - dropout (bool) - P(keep) for dropout.\n",
    "    - alpha (float) - Alpha value for leaky ReLU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, in_features, out_features, dropout, alpha):\n",
    "        super(MultiHeadGraphAttn, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Operations\n",
    "        heads = [GraphAttn(in_features, out_features, dropout, alpha)\n",
    "                 for i in range(n_heads)]\n",
    "        self.attn_heads = nn.ModuleList(heads)\n",
    "\n",
    "    def forward(self, nodes, adj, cat_dim=1):\n",
    "        \"\"\"\n",
    "        Perform forward pass through multi-head graph attention layer.\n",
    "\n",
    "        Args:\n",
    "        - nodes (torch.FloatTensor) - Node feature matrix\n",
    "            (n_nodes, in_features).\n",
    "        - adj (torch.FloatTensor) - Adjacency matrix (n_nodes, n_nodes).\n",
    "\n",
    "        Returns:\n",
    "        - torch.FloatTensor of dimension (n_nodes, n_nodes) of attentional\n",
    "            coefficients where a_ij is the attention value of for node j with\n",
    "            respect to node i.\n",
    "        \"\"\"\n",
    "\n",
    "        if cat_dim == 1:\n",
    "            vals = torch.cat([head(nodes, adj) for head in self.attn_heads],\n",
    "                         dim = 1)\n",
    "\n",
    "        elif cat_dim == -1:\n",
    "            vals = torch.stack([head(nodes, adj) for head in self.attn_heads])\n",
    "\n",
    "        return vals\n",
    "\n",
    "\n",
    "class FCContactPred(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully connected layer to perform contact prediction.\n",
    "\n",
    "    Args:\n",
    "    - node_features (int) - Number of input features.\n",
    "    - out_features (int) - Number of output prediction values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, node_features, out_preds, layers=3):\n",
    "        super(FCContactPred, self).__init__()\n",
    "\n",
    "        self.linear_first = nn.Linear(node_features, 25, bias=True)\n",
    "\n",
    "        self.int_layers = nn.ModuleList([nn.Linear(25, 25, bias=True)\n",
    "                                         for i in range(layers - 2)])\n",
    "\n",
    "        self.linear_final = nn.Linear(25, out_preds, bias=True)\n",
    "\n",
    "    def forward(self, combined_nodes):\n",
    "        \"\"\"\n",
    "        Predict pointwise multichannel contacts from summarized pairwise\n",
    "        residue features.\n",
    "\n",
    "        Args:\n",
    "        - nodes (torch.FloatTensor) - Tensor of (convolved) node features\n",
    "            (n_pairwise, n_features).\n",
    "\n",
    "        Returns:\n",
    "        - torch.FloatTensor (n_contacts, n_channels) containing the prediction\n",
    "            for every potential contact point and every contact channel.\n",
    "        \"\"\"\n",
    "        # Get the logits from the linear layer\n",
    "        prelogits = self.linear_first(combined_nodes)\n",
    "        prelogits = F.relu(prelogits)\n",
    "\n",
    "        for layer in self.int_layers:\n",
    "            prelogits = layer(prelogits)\n",
    "            prelogits = F.relu(prelogits)\n",
    "\n",
    "        logits = self.linear_final(prelogits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class AttnLinkPredict(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head graph attention for link prediction.\n",
    "\n",
    "    Args:\n",
    "    - n_layers (int) - Number of layers.\n",
    "    - n_head (int) - Number of heads for the attention layer.\n",
    "    - in_features (int) - Number of total input features.\n",
    "    - out_features (int) - Number of output features per head.\n",
    "    - dropout (bool) - P(keep) for dropout.\n",
    "    - alpha (float) - Alpha value for leaky ReLU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, n_heads, in_features, out_features, dropout, alpha):\n",
    "        super(AttnLinkPredict, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "        layers = [MultiHeadGraphAttn(n_heads, in_features,\n",
    "                                     out_features, dropout,\n",
    "                                     alpha)]\n",
    "\n",
    "        layers.extend([MultiHeadGraphAttn(n_heads, n_heads * out_features,\n",
    "                                     out_features, dropout,\n",
    "                                     alpha)\n",
    "                      for i in range(n_layers - 1)])\n",
    "\n",
    "        self.attn_layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.attn_batch_norm = nn.BatchNorm1d(n_heads * out_features)\n",
    "\n",
    "        self.output_linear = nn.Linear(n_heads * out_features, 8)\n",
    "\n",
    "    def forward(self, nodes, adjacency):\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for target1, target2 in it.combinations_with_replacement(range(len(nodes)), 2):\n",
    "            targets = adjacency[[target1, target2], :]\n",
    "\n",
    "            where = [target1, target2]\n",
    "            where.extend([ind for ind in np.where(targets.cpu() > 0)[1] if ind not in where])\n",
    "\n",
    "            out_adj_size = len(where) + 1\n",
    "            adj = torch.zeros(out_adj_size, out_adj_size)\n",
    "            adj[[0, 0, 1, 2], [1, 2, 0, 0]] = 1\n",
    "\n",
    "            neighbors = adjacency[where][:, where]\n",
    "            adj[1:, 1:] = neighbors\n",
    "\n",
    "            node_feats = nodes[where]\n",
    "\n",
    "            node_feats = torch.cat((torch.sum(node_feats[:2], dim=0, keepdim=True), node_feats), dim=0)\n",
    "\n",
    "            for layer in self.attn_layers:\n",
    "                node_feats = layer(node_feats, adj)\n",
    "\n",
    "            outputs.append(node_feats[0, :].unsqueeze(0))\n",
    "\n",
    "        linear_in = self.attn_batch_norm(torch.cat(outputs, dim=0))\n",
    "\n",
    "        return self.output_linear(linear_in)\n",
    "\n",
    "\n",
    "class LinkPredict(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head graph attention for link prediction.\n",
    "\n",
    "    Args:\n",
    "    - n_layers (int) - Number of layers.\n",
    "    - n_head (int) - Number of heads for the attention layer.\n",
    "    - in_features (int) - Number of total input features.\n",
    "    - out_features (int) - Number of output features per head.\n",
    "    - dropout (bool) - P(keep) for dropout.\n",
    "    - alpha (float) - Alpha value for leaky ReLU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, n_heads, in_features, out_features, dropout, alpha):\n",
    "        super(AttnLinkPredict, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "        layers = [MultiHeadGraphAttn(n_heads, in_features,\n",
    "                                     out_features, dropout,\n",
    "                                     alpha)]\n",
    "\n",
    "        layers.extend([MultiHeadGraphAttn(n_heads, n_heads * out_features,\n",
    "                                     out_features, dropout,\n",
    "                                     alpha)\n",
    "                      for i in range(n_layers - 1)])\n",
    "\n",
    "        self.attn_layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.attn_batch_norm = nn.BatchNorm1d(n_heads * out_features)\n",
    "\n",
    "        self.output_linear = nn.Linear(n_heads * out_features, 8)\n",
    "\n",
    "    def forward(self, nodes, adjacency):\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for target1, target2 in it.combinations_with_replacement(range(len(nodes)), 2):\n",
    "            targets = adjacency[[target1, target2], :]\n",
    "\n",
    "            where = [target1, target2]\n",
    "            where.extend([ind for ind in np.where(targets.cpu() > 0)[1] if ind not in where])\n",
    "\n",
    "            out_adj_size = len(where) + 1\n",
    "            adj = torch.zeros(out_adj_size, out_adj_size)\n",
    "            adj[[0, 0, 1, 2], [1, 2, 0, 0]] = 1\n",
    "\n",
    "            neighbors = adjacency[where][:, where]\n",
    "            adj[1:, 1:] = neighbors\n",
    "\n",
    "            node_feats = nodes[where]\n",
    "\n",
    "            node_feats = torch.cat((torch.sum(node_feats[:2], dim=0, keepdim=True), node_feats), dim=0)\n",
    "\n",
    "            for layer in self.attn_layers:\n",
    "                node_feats = layer(node_feats, adj)\n",
    "\n",
    "            outputs.append(node_feats[0, :].unsqueeze(0))\n",
    "\n",
    "        linear_in = self.attn_batch_norm(torch.cat(outputs, dim=0))\n",
    "\n",
    "        return self.output_linear(linear_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HyQx2QqYbSlo"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NvIeIhAcbSlp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "########################################\n",
    "# Pairwise matrix generation functions #\n",
    "########################################\n",
    "\n",
    "def pairwise_mat(nodes, method='mean'):\n",
    "    \"\"\"\n",
    "    Generate matrix for pairwise determination of interactions.\n",
    "\n",
    "    Args:\n",
    "    - nodes (torch.FloatTensor) - Tensor of node (n_nodes, n_features) features.\n",
    "    - method (str) - One of 'sum' or 'mean' for combination startegy for\n",
    "        pairwise combination matrix (default = 'mean').\n",
    "\n",
    "    Returns:\n",
    "    - torch.FloatTensor of shape (n_pairwise, n_nodes) than can be used used to\n",
    "        combine feature vectors. Values are 1 if method == \"sum\" and 0.5 if\n",
    "        method == \"mean\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the upper triangle indices\n",
    "    triu = np.vstack(np.triu_indices(nodes.shape[0]))\n",
    "\n",
    "    # Loop through all indices and add to list with\n",
    "    idxs = torch.from_numpy(triu).T\n",
    "\n",
    "    # Convert to tensor\n",
    "    combos = torch.zeros([idxs.shape[0], nodes.shape[0]]).scatter(1, idxs, 1)\n",
    "\n",
    "    # Set to 0.5 if method is 'mean'\n",
    "    if method == 'mean':\n",
    "        combos *= 0.5\n",
    "\n",
    "    return combos\n",
    "\n",
    "\n",
    "####################################\n",
    "# Pairwise concatenation functions #\n",
    "####################################\n",
    "\n",
    "def cat_pairwise(embeddings):\n",
    "\n",
    "    triu = np.vstack(np.triu_indices(embeddings.shape[0])).T\n",
    "\n",
    "    node1 = []\n",
    "    node2 = []\n",
    "\n",
    "    for i, j in triu:\n",
    "        node1.append(embeddings[i])\n",
    "        node2.append(embeddings[j])\n",
    "\n",
    "    node1 = torch.stack(node1, dim=0)\n",
    "    node2 = torch.flip(torch.stack(node2, dim=0), dims=(1,))\n",
    "\n",
    "    return torch.cat((node1, node2), dim=1)\n",
    "\n",
    "\n",
    "############################\n",
    "# Upper triangle functions #\n",
    "############################\n",
    "\n",
    "def triu_condense(input_tensor):\n",
    "    \"\"\"\n",
    "    Condense the upper triangle of a tensor into a 2d dense representation.\n",
    "\n",
    "    Args:\n",
    "    - input_tensor (torch.Tensor) - Tensor of shape (n, n, m).\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of shape (n(n+1)/2, m) where elements along the third dimension in\n",
    "        the original tensor are packed row-wise according to the upper\n",
    "        triangular indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get upper triangle index info\n",
    "    row_idx, col_idx = np.triu_indices(input_tensor.shape[0])\n",
    "    row_idx = torch.LongTensor(row_idx)\n",
    "    col_idx = torch.LongTensor(col_idx)\n",
    "\n",
    "    # Return the packed matrix\n",
    "    output = input_tensor[row_idx, col_idx, :]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def triu_expand(input_matrix):\n",
    "    \"\"\"\n",
    "    Expand a dense representation of the upper triangle of a tensor into\n",
    "    a 3D squareform representation.\n",
    "\n",
    "    Args:\n",
    "    - input_matrix (torch.Tensor) - Tensor of shape (n(n+1)/2, m).\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of shape (n, n, m) where elements along the third dimension in the\n",
    "        original tensor are packed row-wise according to the upper triangular\n",
    "        indices.\n",
    "    \"\"\"\n",
    "    # Get the edge size n of the tensor\n",
    "    n_elements = input_matrix.shape[0]\n",
    "    n_chan = input_matrix.shape[1]\n",
    "    n_res = int((-1 + np.sqrt(1 + 4 * 2 * (n_elements))) / 2)\n",
    "\n",
    "    # Get upper triangle index info\n",
    "    row_idx, col_idx = np.triu_indices(n_res)\n",
    "    row_idx = torch.tensor(row_idx, dtype=torch.long, device=input_matrix.device)\n",
    "    col_idx = torch.tensor(col_idx, dtype=torch.long, device=input_matrix.device)\n",
    "\n",
    "    # Generate the output tensor\n",
    "    output = torch.zeros((n_res, n_res, n_chan), device=input_matrix.device)\n",
    "\n",
    "    # Input the triu values\n",
    "    for i in range(n_chan):\n",
    "        i_tens = torch.full((len(row_idx),), i, dtype=torch.long, device=input_matrix.device)\n",
    "        output.index_put_((row_idx, col_idx, i_tens), input_matrix[:, i])\n",
    "\n",
    "    # Input the tril values\n",
    "    for i in range(n_chan):\n",
    "        i_tens = torch.full((len(row_idx),), i, dtype=torch.long, device=input_matrix.device)\n",
    "        output.index_put_((col_idx, row_idx, i_tens), input_matrix[:, i])\n",
    "\n",
    "    return output\n",
    "\n",
    "####################\n",
    "# F1 Loss function #\n",
    "####################\n",
    "\n",
    "def f1_loss(y_true:torch.Tensor, y_pred:torch.Tensor, is_training=False) -> torch.Tensor:\n",
    "    '''Calculate F1 score. Can work with gpu tensors\n",
    "\n",
    "    The original implmentation is written by Michal Haltuf on Kaggle.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        `ndim` == 1. 0 <= val <= 1\n",
    "\n",
    "    Reference\n",
    "    ---------\n",
    "    - https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
    "    - https://discuss.pytorch.org/t/calculating-precision-recall-and-f1-score-in-case-of-multi-label-classification/28265/6\n",
    "\n",
    "    '''\n",
    "    assert y_true.ndim == 1\n",
    "    assert y_pred.ndim == 1 or y_pred.ndim == 2\n",
    "\n",
    "    if y_pred.ndim == 2:\n",
    "        y_pred = y_pred.argmax(dim=1)\n",
    "\n",
    "\n",
    "    tp = (y_true * y_pred).sum().to(torch.float32)\n",
    "    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
    "    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
    "    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
    "\n",
    "    epsilon = 1e-7\n",
    "\n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "\n",
    "    f1 = 2* (precision*recall) / (precision + recall + epsilon)\n",
    "    f1.requires_grad = is_training\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJZARlB3bSlt"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZTJ299UbSlv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from tqdm import *\n",
    "import wandb\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "def track_mem():\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if (torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data))) and hasattr(obj, '__name__'):\n",
    "                print(obj.__name__, obj.size())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "class GAT(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, embed_features, atom_out_features, res_out_features,\n",
    "                 n_contact_channels, dropout, alpha, train_data, val_data,\n",
    "                 test_data):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        # Properties\n",
    "        self.embed_features = embed_features\n",
    "        self.atom_out_features = atom_out_features\n",
    "        self.res_out_features = res_out_features\n",
    "        self.n_contact_channels = n_contact_channels\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Datasets\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "        # Model components\n",
    "        self.embed = AtomEmbed(embed_features, scale_grad_by_freq=True)\n",
    "\n",
    "        self.embed_batch_norm = nn.BatchNorm1d(embed_features)\n",
    "\n",
    "        # Define number of graph conv layers and heads\n",
    "        n_atom_layers = 5\n",
    "        n_atom_heads = 3\n",
    "\n",
    "        n_res_layers = 2\n",
    "        n_res_heads = 3\n",
    "\n",
    "        # Set up atom attention\n",
    "        self.atom_attns = nn.ModuleList([])\n",
    "        for i in range(n_atom_layers):\n",
    "            if i == 0:\n",
    "                attn_layer = MultiHeadGraphAttn(n_atom_heads, embed_features,\n",
    "                                                atom_out_features,\n",
    "                                                dropout, alpha)\n",
    "            else:\n",
    "                attn_layer = MultiHeadGraphAttn(n_atom_heads,\n",
    "                                                n_atom_heads * atom_out_features,\n",
    "                                                atom_out_features,\n",
    "                                                dropout, alpha)\n",
    "\n",
    "            self.atom_attns.append(attn_layer)\n",
    "\n",
    "\n",
    "        # Set up condensation\n",
    "        self.condense_batch_norm = nn.BatchNorm1d(n_atom_heads * atom_out_features)\n",
    "\n",
    "        # Set up res attention\n",
    "        self.res_attns = nn.ModuleList([])\n",
    "        for i in range(n_res_layers):\n",
    "            if i == 0:\n",
    "                attn_layer = MultiHeadGraphAttn(n_res_heads,\n",
    "                                                n_atom_heads * atom_out_features,\n",
    "                                                res_out_features, dropout,\n",
    "                                                alpha)\n",
    "            else:\n",
    "                attn_layer = MultiHeadGraphAttn(n_res_heads,\n",
    "                                                n_res_heads * res_out_features,\n",
    "                                                res_out_features, dropout,\n",
    "                                                alpha)\n",
    "\n",
    "            self.res_attns.append(attn_layer)\n",
    "\n",
    "        self.h_bond_pred_linear1 = nn.Linear(n_res_heads * res_out_features + 2, 10)\n",
    "        self.h_bond_pred_linear2 = nn.Linear(10, 10)\n",
    "        self.h_bond_pred_linear_predict = nn.Linear(10, 1)\n",
    "        self.h_bond_contact_batch_norm = nn.BatchNorm1d(1)\n",
    "\n",
    "        self.h_bond_attn = MultiHeadGraphAttn(n_res_heads,\n",
    "                                             n_res_heads * res_out_features,\n",
    "                                             res_out_features, dropout,\n",
    "                                             alpha)\n",
    "\n",
    "        self.final_attn = MultiHeadGraphAttn(n_res_heads,\n",
    "                                             n_res_heads * res_out_features,\n",
    "                                             res_out_features, dropout,\n",
    "                                             alpha)\n",
    "\n",
    "        self.activation = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "        self.linear1 = nn.Linear(n_res_heads * res_out_features + 2, 10)\n",
    "        self.linear2 = nn.Linear(10, 10)\n",
    "        self.linear_predict = nn.Linear(10, 8)\n",
    "\n",
    "        self.pred_contact_batch_norm = nn.BatchNorm1d(n_contact_channels)\n",
    "\n",
    "        self.activations = {}\n",
    "\n",
    "\n",
    "        # Validation information\n",
    "        self.reset_epoch_metrics()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Put on gpus\n",
    "#         self.embed.cuda(0)\n",
    "#         self.embed_batch_norm.cuda(0)\n",
    "#         [layer.cuda(0) for layer in self.atom_attns]\n",
    "# #         self.condense.cuda(0)\n",
    "#         self.condense_batch_norm.cuda(0)\n",
    "#         [layer.cuda(0) for layer in self.res_attns]\n",
    "#         self.final_attn.cuda(0)\n",
    "#         self.linear1.cuda(1)\n",
    "#         self.linear2.cuda(1)\n",
    "#         self.linear_predict.cuda(1)\n",
    "# #         self.conn_attn.cuda(1)\n",
    "#         self.pred_contact_batch_norm.cuda(1)\n",
    "\n",
    "        # Regular forward pass\n",
    "        atom_embed = self.embed(x['atom_nodes'].squeeze())\n",
    "\n",
    "        atom_embed_update = self.embed_batch_norm(atom_embed)\n",
    "\n",
    "        atom_adj = x['atom_adj'].squeeze()\n",
    "\n",
    "        for layer in self.atom_attns:\n",
    "                atom_embed_update = checkpoint(layer, atom_embed_update, atom_adj)\n",
    "\n",
    "        res_embed = torch.matmul( x['mem_mat'].squeeze(), atom_embed_update)\n",
    "\n",
    "        res_embed_update = self.condense_batch_norm(res_embed)\n",
    "\n",
    "        adj = x['res_adj'].squeeze()\n",
    "\n",
    "        for layer in self.res_attns:\n",
    "            res_embed_update = checkpoint(layer, res_embed_update, adj)\n",
    "\n",
    "        # H-bond predictions\n",
    "        h_bond = torch.matmul(res_embed_update,\n",
    "                           res_embed_update.T)\n",
    "\n",
    "#         h_bond = torch.cat((h_bond_in,\n",
    "#                             x['res_dist'].permute(1, 2, 0),\n",
    "#                             x['chain_mem'].permute(1, 2, 0)),\n",
    "#                            dim=-1)\n",
    "\n",
    "\n",
    "#         h_bond_in = triu_condense(h_bond_in)\n",
    "#         h_bond = torch.relu(self.h_bond_pred_linear1(h_bond_in))\n",
    "#         h_bond = torch.relu(self.h_bond_pred_linear2(h_bond))\n",
    "#         h_bond = self.h_bond_pred_linear_predict(h_bond)\n",
    "#         h_bond = self.h_bond_contact_batch_norm(h_bond)\n",
    "\n",
    "        h_bond_pred = h_bond\n",
    "\n",
    "        h_bond = torch.round(h_bond.sigmoid())\n",
    "        h_bond = h_bond\n",
    "\n",
    "        h_bond_adj =  h_bond + adj\n",
    "\n",
    "        res_embed_update = self.h_bond_attn(res_embed_update, h_bond_adj)\n",
    "\n",
    "        res_embed_update = self.final_attn(res_embed_update, h_bond_adj)\n",
    "\n",
    "#         preds = torch.matmul(res_embed_update, res_embed_update.T)\n",
    "\n",
    "        preds = torch.bmm(res_embed_update.T.unsqueeze(2),\n",
    "                          res_embed_update.T.unsqueeze(1)).permute(1, 2, 0)\n",
    "\n",
    "        preds = torch.cat((preds,\n",
    "                           x['res_dist'].permute(1, 2, 0),\n",
    "                           x['chain_mem'].permute(1, 2, 0)),\n",
    "                          dim=-1)\n",
    "\n",
    "        preds = triu_condense(preds)\n",
    "        preds = torch.relu(self.linear1(preds))\n",
    "        preds = torch.relu(self.linear2(preds))\n",
    "        preds = self.linear_predict(preds)\n",
    "\n",
    "#         atom_attns = [[head.f_attn for head in layer.attn_heads]\n",
    "#                       for layer in self.atom_attns]\n",
    "#         res_attns = [[head.f_attn for head in layer.attn_heads]\n",
    "#                       for layer in self.res_attns]\n",
    "\n",
    "        preds = self.pred_contact_batch_norm(preds)\n",
    "\n",
    "#         self.activations = {\n",
    "#             'atom_embed': atom_embed.detach().cpu().numpy(),\n",
    "#             'atom_embed_update': atom_embed_update.detach().cpu().numpy(),\n",
    "# #             'atom_attn': atom_attns,\n",
    "#             'res_embed': res_embed.detach().cpu().numpy(),\n",
    "#             'res_embed_update': res_embed_update.detach().cpu().numpy(),\n",
    "# #             'res_attn': res_attns,\n",
    "# #             'pairwise': pairwise.detach().cpu().numpy(),\n",
    "# #             'combined': combined.detach().cpu().numpy(),\n",
    "#             'preds': preds.detach().cpu().numpy()\n",
    "#         }\n",
    "\n",
    "        return h_bond_pred, preds\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # REQUIRED\n",
    "        h_bond, y_hat = self.forward(batch)\n",
    "\n",
    "        h_bond = h_bond.squeeze(-1)\n",
    "        y_hat = y_hat.squeeze()\n",
    "\n",
    "        y = triu_condense(batch['res_contact'].squeeze())\n",
    "        weights = triu_condense(batch['res_mask'].squeeze())\n",
    "\n",
    "#         y_pos_masked = (y == 1).float() * weights\n",
    "#         y_neg_masked = (y == 1).float() * weights\n",
    "\n",
    "#         y_pos_frac = y_pos_masked.sum() / weights.sum()\n",
    "#         y_neg_frac = y_neg_masked.sum() / weights.sum()\n",
    "\n",
    "#         weights = (y_pos_masked * y_pos_frac) + (y_neg_masked * y_neg_frac)\n",
    "\n",
    "        h_bond_loss = F.binary_cross_entropy_with_logits(triu_condense(h_bond.unsqueeze(-1))[:, 0], y[:, 1], weight=weights[:, 1])\n",
    "\n",
    "        total_loss = F.binary_cross_entropy_with_logits(y_hat, y, weight=weights)\n",
    "\n",
    "        kl_div = F.kl_div(y_hat[:, 1].view(1, -1).log_softmax(-1), triu_condense(h_bond.unsqueeze(-1))[:, 0].view(1, -1).softmax(-1), reduction='batchmean')\n",
    "\n",
    "        loss = h_bond_loss + kl_div #+ total_loss + kl_div\n",
    "\n",
    "        self.train_losses.append(loss.item())\n",
    "\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        # OPTIONAL\n",
    "        h_bond, y_hat = self.forward(batch)\n",
    "        y_hat = y_hat.squeeze()\n",
    "\n",
    "        y = triu_condense(batch['res_contact'].squeeze())\n",
    "        weights = triu_condense(batch['res_mask'].squeeze())\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y, weight=weights)\n",
    "\n",
    "        self.val_losses.append(loss.item())\n",
    "\n",
    "        _, _, auroc = calc_metric_curve(y_hat.sigmoid().cpu(), y.cpu(), 'ROC', squareform=False)\n",
    "        _, _, auprc = calc_metric_curve(y_hat.sigmoid().cpu(), y.cpu(), 'PRC', squareform=False)\n",
    "\n",
    "        self.auroc['Hydrophobic'].append(auroc[0])\n",
    "        self.auroc['Hydrogen bond'].append(auroc[1])\n",
    "        self.auroc['Van der Waals'].append(auroc[2])\n",
    "        self.auroc['Water bridges'].append(auroc[3])\n",
    "        self.auroc['Salt bridges'].append(auroc[4])\n",
    "        self.auroc['Pi-stacking'].append(auroc[5])\n",
    "        self.auroc['Pi-cation'].append(auroc[6])\n",
    "        self.auroc['T-stacking'].append(auroc[7])\n",
    "\n",
    "        self.auprc['Hydrophobic'].append(auprc[0])\n",
    "        self.auprc['Hydrogen bond'].append(auprc[1])\n",
    "        self.auprc['Van der Waals'].append(auprc[2])\n",
    "        self.auprc['Water bridges'].append(auprc[3])\n",
    "        self.auprc['Salt bridges'].append(auprc[4])\n",
    "        self.auprc['Pi-stacking'].append(auprc[5])\n",
    "        self.auprc['Pi-cation'].append(auprc[6])\n",
    "        self.auprc['T-stacking'].append(auprc[7])\n",
    "\n",
    "        if batch_nb == 0:\n",
    "            self.pred_example = plot_channels(triu_expand(y_hat.detach().cpu()))\n",
    "            self.pred_h_bond = plt.imshow(h_bond.detach().cpu())\n",
    "\n",
    "    def on_post_performance_check(self):\n",
    "\n",
    "        epoch_metrics = {}\n",
    "        all_metrics = {}\n",
    "\n",
    "        for key in self.auroc:\n",
    "            values = np.asarray(self.auroc[key])\n",
    "            all_metrics[f'AUROC - {key}'] = values\n",
    "\n",
    "            values = values[~np.isnan(values)]\n",
    "            epoch_metrics[f'AUROC - {key}'] = wandb.Histogram(values)\n",
    "\n",
    "\n",
    "        for key in self.auprc:\n",
    "            values = np.asarray(self.auprc[key])\n",
    "            all_metrics[f'AUPRC - {key}'] = values\n",
    "\n",
    "            values = values[~np.isnan(values)]\n",
    "            epoch_metrics[f'AUPRC - {key}'] = wandb.Histogram(values)\n",
    "\n",
    "        epoch_metrics['Train Losses'] = wandb.Histogram(self.train_losses)\n",
    "\n",
    "        epoch_metrics['Val Losses'] = wandb.Histogram(self.val_losses)\n",
    "        all_metrics['Val Losses'] = np.asarray(self.val_losses)\n",
    "\n",
    "        all_metrics = pd.DataFrame(all_metrics)\n",
    "        cor_mat = all_metrics.corr()\n",
    "\n",
    "        corr_plot = metric_corr_plot(cor_mat)\n",
    "        epoch_metrics['Metric Corr Plot'] = wandb.Image(corr_plot)\n",
    "\n",
    "        epoch_metrics['train_loss'] = np.asarray(self.train_losses).mean()\n",
    "        epoch_metrics['val_loss'] = np.asarray(self.val_losses).mean()\n",
    "\n",
    "        epoch_metrics['Example Prediction'] = wandb.Image(self.pred_example)\n",
    "        epoch_metrics['Example H-bond Prediction'] = wandb.Image(self.pred_h_bond)\n",
    "\n",
    "        self.logger.experiment.log(epoch_metrics)\n",
    "\n",
    "        self.reset_epoch_metrics()\n",
    "\n",
    "    def reset_epoch_metrics(self):\n",
    "\n",
    "        # Validation information\n",
    "        self.auroc = {\n",
    "            'Hydrophobic': [],\n",
    "            'Hydrogen bond': [],\n",
    "            'Van der Waals': [],\n",
    "            'Water bridges': [],\n",
    "            'Salt bridges': [],\n",
    "            'Pi-stacking': [],\n",
    "            'Pi-cation': [],\n",
    "            'T-stacking': []\n",
    "        }\n",
    "\n",
    "        self.auprc = {\n",
    "            'Hydrophobic': [],\n",
    "            'Hydrogen bond': [],\n",
    "            'Van der Waals': [],\n",
    "            'Water bridges': [],\n",
    "            'Salt bridges': [],\n",
    "            'Pi-stacking': [],\n",
    "            'Pi-cation': [],\n",
    "            'T-stacking': []\n",
    "        }\n",
    "\n",
    "        self.val_losses = []\n",
    "        self.train_losses = []\n",
    "\n",
    "\n",
    "#     def validation_end(self, outputs):\n",
    "#         # OPTIONAL\n",
    "# #         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "# #         return {'avg_val_loss': avg_loss}\n",
    "#         pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "\n",
    "        parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "\n",
    "        return torch.optim.SGD(parameters, lr=0.01, momentum=0.9, weight_decay=1e-6)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        # REQUIRED\n",
    "        return DataLoader(self.train_data, shuffle=True, num_workers=30, pin_memory=True)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(self.val_data, shuffle=False, num_workers=20, pin_memory=True)\n",
    "\n",
    "#     def on_batch_end(self):\n",
    "#         wandb.log({key: plt.imshow(self.activations[key][:20]) for key in self.activations})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_channels(values):\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))\n",
    "\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    channel_names = [\n",
    "        'Hydrophobic',\n",
    "        'Hydrogen bond',\n",
    "        'Van der Waals',\n",
    "        'Water bridges',\n",
    "        'Salt bridges',\n",
    "        'Pi-stacking',\n",
    "        'Pi-cation',\n",
    "        'T-stacking'\n",
    "    ]\n",
    "\n",
    "    for channel in range(values.shape[-1]):\n",
    "        ax[channel].imshow(values[:, :, channel].squeeze(), vmin=0, vmax=1)\n",
    "        ax[channel].set(title=channel_names[channel], xlabel='Residue #', ylabel='Residue #')\n",
    "\n",
    "    plt.close()\n",
    "    return fig\n",
    "\n",
    "\n",
    "######################\n",
    "# ROC and PRC curves #\n",
    "######################\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "\n",
    "def calc_metric_curve(preds, target, curve_type, squareform=False):\n",
    "    \"\"\"\n",
    "    Calculate ROC or PRC curves and area for the predicted contact channels.\n",
    "\n",
    "    Args:\n",
    "    - preds (np.ndarray) - Numpy array of model predictions either of form\n",
    "        (n_res, n_res, n_chan) or (n_res * [n_res - 1] / 2, n_chan).\n",
    "    - target (np.ndarray) - Numpy array of target values either of form\n",
    "        (n_res, n_res, n_chan) or (n_res * [n_res - 1] / 2, n_chan),\n",
    "        must match form of preds.\n",
    "    - curve_type (str) - One of 'ROC' or 'PRC' to denote type of curve.\n",
    "    - squareform (bool) - True if tensors are of shape (n_res, n_res, n_chan),\n",
    "        False if they are of shape (n_res * [n_res - 1] / 2, n_chan)\n",
    "        (default = True).\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of x, y, and AUC values to be used for plotting the curves\n",
    "        using plot_curve metric.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get correct curve function\n",
    "    if curve_type.upper() == 'ROC':\n",
    "        curve_func = roc_curve\n",
    "    elif curve_type.upper() == 'PRC':\n",
    "        curve_func = precision_recall_curve\n",
    "\n",
    "    # Generate dicts to hold outputs from curve generation functions\n",
    "    x = dict()\n",
    "    y = dict()\n",
    "    auc_ = dict()\n",
    "\n",
    "    # Handle case of squareform matrix (only get non-redundant triu indices)\n",
    "    if squareform:\n",
    "        indices = np.triu_indices(target.shape[0])\n",
    "\n",
    "    # For each channel\n",
    "    for i in range(target.shape[-1]):\n",
    "\n",
    "        # Handle case of squareform\n",
    "        if squareform:\n",
    "            var1, var2, _ = curve_func(target[:, :, i][indices],\n",
    "                                       preds[:, :, i][indices])\n",
    "\n",
    "        # Handle case of pairwise\n",
    "        else:\n",
    "            var1, var2, _ = curve_func(target[:, i], preds[:, i])\n",
    "\n",
    "        # Assign outputs to correct dict for plotting\n",
    "        if curve_type.upper() == 'ROC':\n",
    "            x[i] = var1\n",
    "            y[i] = var2\n",
    "        elif curve_type.upper() == 'PRC':\n",
    "            x[i] = var2\n",
    "            y[i] = var1\n",
    "\n",
    "        # Calc AUC\n",
    "        auc_[i] = auc(x[i], y[i])\n",
    "\n",
    "    return (x, y, auc_)\n",
    "\n",
    "\n",
    "def plot_curve_metric(x, y, auc, curve_type, title=None, labels=None):\n",
    "    \"\"\"\n",
    "    Plot ROC or PRC curves per output channel.\n",
    "\n",
    "    Args:\n",
    "    - x (dict) - Dict of numpy arrays for values to plot on x axis.\n",
    "    - y (dict) - Dict of numpy arrays for values to plot on x axis.\n",
    "    - auc (dict) - Dict of numpy arrays for areas under each curve.\n",
    "    - curve_type (str) - One of 'ROC' or 'PRC' to denote type of curve.\n",
    "    - title\n",
    "    - labels\n",
    "\n",
    "    Returns:\n",
    "    - pyplot object of curves.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate figure\n",
    "    plt.figure()\n",
    "\n",
    "    # Linetype spec\n",
    "    lw = 2\n",
    "    curve_type = curve_type.upper()\n",
    "\n",
    "    # Get the number of channels being plotted\n",
    "    n_chan = len(x)\n",
    "\n",
    "    # Make labels numeric if not provided\n",
    "    if labels is None:\n",
    "        labels = list(range(n_chan))\n",
    "\n",
    "    # Check to make sure the labels are the right length\n",
    "    if len(labels) != n_chan:\n",
    "        raise ValueError('Number of labels ({}) does not match number of prediction channels ({}).'.format(len(labels), n_chan))\n",
    "\n",
    "    # Get a lit of colors for all the channels\n",
    "    color_list = plt.cm.Set1(np.linspace(0, 1, n_chan))\n",
    "\n",
    "    # Plot each line\n",
    "    for i, color in enumerate(color_list):\n",
    "        plt.plot(x[i], y[i], color=color,\n",
    "                 lw=lw, label='{} (area = {:0.2f})'.format(labels[i], auc[i]))\n",
    "\n",
    "    # Add labels and diagonal line for ROC\n",
    "    if curve_type == 'ROC':\n",
    "        xlab = 'FPR'\n",
    "        ylab = 'TPR'\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # Add labels for PRC\n",
    "    elif curve_type == 'PRC':\n",
    "        xlab = 'Recall'\n",
    "        ylab = 'Precision'\n",
    "        plt.legend(loc=\"lower left\")\n",
    "\n",
    "    # Extend limits, add labels and title\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title('{} for {}'.format(curve_type, title))\n",
    "    else:\n",
    "        plt.title('{}'.format(curve_type))\n",
    "\n",
    "    return plt.axes()\n",
    "\n",
    "def plot_curve(preds, target, curve_type, title=None, labels=None,\n",
    "               squareform=False):\n",
    "    \"\"\"\n",
    "    Wrapper to directly plot curves from model output and target.\n",
    "\n",
    "    Args:\n",
    "    - preds (np array-like) - Array or tensor of predicted values output by\n",
    "        model.\n",
    "    - target (np array-like) - Array or tensor of target values.\n",
    "    - curve_type (str) - One of 'ROC' or 'PRC'.\n",
    "    - title (str) - Title of plot (default = None).\n",
    "    - labels (list) - List of labels for each channel on the plot\n",
    "        (default = None).\n",
    "    - squareform (bool) - Whether the predictions and targets are in square form\n",
    "        (default = False).\n",
    "    \"\"\"\n",
    "    x, y, auc_ = calc_metric_curve(preds, target, curve_type, squareform)\n",
    "    return plot_curve_metric(x, y, auc_, curve_type, title, labels)\n",
    "\n",
    "\n",
    "################################\n",
    "# Correlations between metrics #\n",
    "################################\n",
    "\n",
    "def metric_corr_plot(corr):\n",
    "    mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(11, 9))\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, vmin = -1.0, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3QZxiiHbSl1"
   },
   "source": [
    "## Training\n",
    "\n",
    "### Instantiate dataloader and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6gcL3lpbbSl3"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "data_base = '/home/tshimko/tesselate/'\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "data_load = [\n",
    "                'pdb_id',\n",
    "#                 'model',\n",
    "                'atom_nodes',\n",
    "                'atom_adj',\n",
    "#                 'atom_contact',\n",
    "#                 'atom_mask',\n",
    "                'res_adj',\n",
    "                'res_dist',\n",
    "                'chain_mem',\n",
    "                'res_contact',\n",
    "#                 'conn_adj',\n",
    "                'res_mask',\n",
    "                'mem_mat',\n",
    "#                 'idx2atom_dict',\n",
    "#                 'idx2res_dict'\n",
    "            ]\n",
    "\n",
    "# train_data = TesselateDataset(data_base + 'id_lists/ligand_free_monomers/small_train.txt',\n",
    "#                               graph_dir=data_base + 'data/graphs',\n",
    "#                               contacts_dir=data_base + 'data/contacts',\n",
    "#                               return_data=data_load, in_memory=False)\n",
    "\n",
    "train_data = TesselateDataset(data_base + 'test4.txt',\n",
    "                              graph_dir=data_base + 'data/graphs',\n",
    "                              contacts_dir=data_base + 'data/contacts',\n",
    "                              return_data=data_load, in_memory=True)\n",
    "\n",
    "val_data = TesselateDataset(data_base + 'id_lists/ligand_free_monomers/small_val.txt',\n",
    "                            graph_dir=data_base + 'data/graphs',\n",
    "                            contacts_dir=data_base + 'data/contacts',\n",
    "                            return_data=data_load, in_memory=True)\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=os.getcwd() + '/checkpoints',\n",
    "    save_top_k=0,\n",
    "    verbose=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    prefix=''\n",
    ")\n",
    "\n",
    "model = GAT(embed_features=10, atom_out_features=10, res_out_features=10,\n",
    "             n_contact_channels=8, dropout=0, alpha=0.2, train_data=train_data, val_data=val_data,\n",
    "             test_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11611,
     "status": "ok",
     "timestamp": 1574002255203,
     "user": {
      "displayName": "Tyler Carter Shimko",
      "photoUrl": "",
      "userId": "18332645348589660395"
     },
     "user_tz": -60
    },
    "id": "od4NiBExbSl7",
    "outputId": "5fc65a3e-de61-40e7-afdf-eb5de7e1ef4b"
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.logging import WandbLogger\n",
    "\n",
    "logger = WandbLogger(project='tesselate')\n",
    "\n",
    "trainer = Trainer(max_nb_epochs=2000, checkpoint_callback=checkpoint_callback,\n",
    "                  early_stop_callback=False, accumulate_grad_batches=8,\n",
    "                  logger=logger, gpus=1) #, gradient_clip_val=0.5) # amp_level='O2', use_amp=True) <-- for half precision\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ycs4WmYKrqoE"
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acts = model.activations\n",
    "# model.cpu()\n",
    "for idx, i in enumerate(DataLoader(val_data, shuffle=False, num_workers=30, pin_memory=True)):\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = triu_expand(out).sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plot_channels(preds.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(triu_expand(i['res_contact'].squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions\n",
    "preds = triu_expand(torch.from_numpy(acts['preds'])).sigmoid()\n",
    "plot_channels(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the true values\n",
    "plot_channels(x['res_contact'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(x['res_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curve(torch.from_numpy(acts['preds']).sigmoid(), triu_condense(x['res_contact'].squeeze()), 'ROC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curve(torch.from_numpy(acts['preds']).sigmoid(), triu_condense(x['res_contact'].squeeze()), 'PRC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "p-gnn2D_four_channel_benchmark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
