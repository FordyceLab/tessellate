{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXUYGCpNbSla"
   },
   "source": [
    "# Benchmark position-aware graph neural network/2D CNN architecture\n",
    "\n",
    "This notebook contains all of the code to overfit a GAT to four contact channels of a single structure (6E6O).\n",
    "\n",
    "## Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRzY2E5pbYm6"
   },
   "source": [
    "### Dataloader code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jf5S3ZSabSlc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import periodictable as pt\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def read_files(acc, model, graph_dir, contacts_dir):\n",
    "    \"\"\"\n",
    "    Read graph and contacts files.\n",
    "\n",
    "    Args:\n",
    "    - acc (str) - String of the PDB ID (lowercese).\n",
    "    - model (int) - Model number of the desired bioassembly.\n",
    "    - graph_dir (str) - Directory containing the nodes, edges,\n",
    "        and mask files.\n",
    "    - contacts_dir (str) - Directory containing the .contacts\n",
    "        files from get_contacts.py.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary of DataFrames and lists corresponding to\n",
    "        graph nodes, edges, mask, and contacts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the file names for the graph files\n",
    "    node_file = os.path.join(graph_dir, '{}-{}_nodes.csv'.format(acc, model))\n",
    "    edge_file = os.path.join(graph_dir, '{}-{}_edges.csv'.format(acc, model))\n",
    "    mask_file = os.path.join(graph_dir, '{}-{}_mask.csv'.format(acc, model))\n",
    "\n",
    "    # Get the contacts file\n",
    "    contacts_file = os.path.join(contacts_dir, '{}-{}.contacts'.format(acc, model))\n",
    "\n",
    "    # Read the nodes and edges\n",
    "    nodes = pd.read_csv(node_file)\n",
    "    edges = pd.read_csv(edge_file)\n",
    "\n",
    "    # Check if the mask is empty\n",
    "    if os.path.getsize(mask_file) > 0:\n",
    "        with open(mask_file) as f:\n",
    "            mask = f.read().split('\\n')\n",
    "    else:\n",
    "        mask = []\n",
    "\n",
    "    # Read the contacts\n",
    "    contacts = pd.read_table(contacts_file, sep='\\t',\n",
    "                             header=None, names=['type', 'start', 'end'])\n",
    "\n",
    "    # Return the data\n",
    "    data = {\n",
    "        'nodes': nodes,\n",
    "        'edges': edges,\n",
    "        'mask': mask,\n",
    "        'contacts': contacts\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def process_res_data(data):\n",
    "    \"\"\"\n",
    "    Process residue-level data from atom-level data.\n",
    "\n",
    "    Args:\n",
    "    - data (dict) - Dictionary of graph data output from `read_files`.\n",
    "\n",
    "    Returns:\n",
    "    - Dictionary of atom and residue graph and contact data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract data form dict\n",
    "    nodes = data['nodes']\n",
    "    edges = data['edges']\n",
    "    mask = data['mask']\n",
    "    contacts = data['contacts']\n",
    "\n",
    "    # Get residue nodes\n",
    "    res_nodes = pd.DataFrame()\n",
    "    res_nodes['res'] = [':'.join(atom.split(':')[:3]) for atom in nodes['atom']]\n",
    "    res_nodes = res_nodes.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Get residue edges\n",
    "    res_edges = edges.copy()\n",
    "    res_edges['start'] = [':'.join(atom.split(':')[:3]) for atom in res_edges['start']]\n",
    "    res_edges['end'] = [':'.join(atom.split(':')[:3]) for atom in res_edges['end']]\n",
    "    res_edges = res_edges[res_edges['start'] != res_edges['end']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Get residue contacts\n",
    "    res_contacts = contacts.copy()\n",
    "    res_contacts['start'] = [':'.join(atom.split(':')[:3]) for atom in res_contacts['start']]\n",
    "    res_contacts['end'] = [':'.join(atom.split(':')[:3]) for atom in res_contacts['end']]\n",
    "    res_contacts = res_contacts[res_contacts['start'] != res_contacts['end']].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Get residue mask\n",
    "    res_mask = list(set([':'.join(atom.split(':')[:3]) for atom in mask]))\n",
    "\n",
    "    # Return data dict\n",
    "    data = {\n",
    "        'atom_nodes': nodes,\n",
    "        'atom_edges': edges,\n",
    "        'atom_contact': contacts,\n",
    "        'atom_mask': mask,\n",
    "        'res_nodes': res_nodes,\n",
    "        'res_edges': res_edges,\n",
    "        'res_contact': res_contacts,\n",
    "        'res_mask': res_mask\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_map_dicts(entity_list):\n",
    "    \"\"\"\n",
    "    Map identifiers to indices and vice versa.\n",
    "\n",
    "    Args:\n",
    "    - entity_list (list) - List of entities (atoms, residues, etc.)\n",
    "        to index.\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of the entity to index and index to entity dicts, respectively.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the entity:index dictionary\n",
    "    ent2idx_dict = {entity: idx for idx, entity in enumerate(entity_list)}\n",
    "\n",
    "    # Create the index:entity dictionary\n",
    "    idx2ent_dict = {idx: entity for entity, idx in ent2idx_dict.items()}\n",
    "\n",
    "    return (ent2idx_dict, idx2ent_dict)\n",
    "\n",
    "\n",
    "def create_adj_mat(data, dict_map, mat_type):\n",
    "    \"\"\"\n",
    "    Creates an adjacency matrix.\n",
    "\n",
    "    Args:\n",
    "    - data (DataFrame) - Dataframe with 'start' and 'end' column\n",
    "        for each interaction. For atom-level adjacency, 'order'\n",
    "        column is also required. For atom or residue conatcts,\n",
    "        'type' column is also required.\n",
    "\n",
    "    Returns:\n",
    "    - Coordinate format matrix (numpy). For atom adjacency, third column\n",
    "        corresponds to bond order. For contacts, third column\n",
    "        corresponds to channel.\n",
    "\n",
    "    Channel mappings (shorthand from get_contacts.py source):\n",
    "\n",
    "        0:\n",
    "            hp             hydrophobic interactions\n",
    "        1:\n",
    "            hb             hydrogen bonds\n",
    "            lhb            ligand hydrogen bonds\n",
    "            hbbb           backbone-backbone hydrogen bonds\n",
    "            hbsb           backbone-sidechain hydrogen bonds\n",
    "            hbss           sidechain-sidechain hydrogen bonds\n",
    "            hbls           ligand-sidechain residue hydrogen bonds\n",
    "            hblb           ligand-backbone residue hydrogen bonds\n",
    "        2:\n",
    "            vdw            van der Waals\n",
    "        3:\n",
    "            wb             water bridges\n",
    "            wb2            extended water bridges\n",
    "            lwb            ligand water bridges\n",
    "            lwb2           extended ligand water bridges\n",
    "        4:\n",
    "            sb             salt bridges\n",
    "        5:\n",
    "            ps             pi-stacking\n",
    "        6:\n",
    "            pc             pi-cation\n",
    "        7:\n",
    "            ts             t-stacking\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the coordinate list\n",
    "    coord_mat = []\n",
    "\n",
    "    # Map channel names to numeric channels\n",
    "    channel = {\n",
    "        # Hydrophobic interactions in first channel\n",
    "        'hp': 0,\n",
    "        'hplp': 0,\n",
    "        'hpll': 0,\n",
    "\n",
    "        # Hydrogen bonds in second channel\n",
    "        'hb': 1,\n",
    "        'lhb': 1,\n",
    "        'hbbb': 1,\n",
    "        'hbsb': 1,\n",
    "        'hbss': 1,\n",
    "        'hbls': 1,\n",
    "        'hblb': 1,\n",
    "\n",
    "        # VdW in third channel\n",
    "        'vdw': 2,\n",
    "\n",
    "        # Water bridges\n",
    "        'wb': 3,\n",
    "        'wb2': 3,\n",
    "        'lwb': 3,\n",
    "        'lwb2': 3,\n",
    "\n",
    "        # Salt bridges\n",
    "        'sb': 4,\n",
    "        'sbpl': 4,\n",
    "\n",
    "        # Other interactions\n",
    "        'ps': 5,\n",
    "        'pc': 6,\n",
    "        'ts': 7,\n",
    "    }\n",
    "\n",
    "    # Assemble the contacts\n",
    "    for idx, row in data.iterrows():\n",
    "\n",
    "        if row['start'] in dict_map and row['end'] in dict_map:\n",
    "\n",
    "            entry = [dict_map[row['start']], dict_map[row['end']]]\n",
    "\n",
    "            # Add order or type if necessary\n",
    "            if mat_type == 'atom_graph':\n",
    "                entry.append(row['order'])\n",
    "            elif mat_type == 'atom_contact':\n",
    "                entry.append(channel[row['type']])\n",
    "            elif mat_type == 'res_contact':\n",
    "                entry.append(channel[row['type']])\n",
    "\n",
    "            coord_mat.append(entry)\n",
    "\n",
    "    return np.array(coord_mat)\n",
    "\n",
    "\n",
    "def create_conn_adj_mat(adj):\n",
    "    \"\"\"\n",
    "    Create connection adjacency matrix\n",
    "    \"\"\"\n",
    "\n",
    "    conn_map = {(a, b): idx for idx, (a, b) in enumerate(zip(*np.triu_indices_from(adj)))}\n",
    "\n",
    "    one_hop_neighbors = {idx: np.argwhere(row > 0).squeeze().tolist() for idx, row in enumerate(adj)}\n",
    "\n",
    "    conns = []\n",
    "\n",
    "    for i, j in it.combinations_with_replacement(one_hop_neighbors, 2):\n",
    "        row = conn_map[(i, j)]\n",
    "        adj_conn_coords = set([(m, n) if m <= n else (n, m)\n",
    "                               for m, n in it.product(one_hop_neighbors[i],\n",
    "                                                      one_hop_neighbors[j])])\n",
    "        adj_conns = [conn_map[x] for x in adj_conn_coords]\n",
    "\n",
    "        conns.append(np.array(list(it.product([row], adj_conns))))\n",
    "\n",
    "    conn_adj = np.concatenate(conns, axis=0).T\n",
    "\n",
    "    return conn_adj\n",
    "\n",
    "\n",
    "def create_mem_mat(atom_dict, res_dict):\n",
    "    \"\"\"\n",
    "    Create a membership matrix mapping atoms to residues.\n",
    "\n",
    "    Args:\n",
    "    - atom_dict (dict) - Dictionary mapping atoms to indices.\n",
    "    - res_dict (dict) - Dictionary mapping residues to indices.\n",
    "\n",
    "    Returns:\n",
    "    - Coordinate format membership matrix (numpy) with first\n",
    "        row being residue number and the second column being\n",
    "        atom number.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the coordinate list\n",
    "    mem_coord = []\n",
    "\n",
    "    # Map atoms to residues\n",
    "    for atom, atom_idx in atom_dict.items():\n",
    "        res_idx = res_dict[':'.join(atom.split(':')[:3])]\n",
    "\n",
    "        mem_coord.append([res_idx, atom_idx])\n",
    "\n",
    "    mem_coord = np.array(mem_coord)\n",
    "\n",
    "    return mem_coord\n",
    "\n",
    "\n",
    "def create_idx_list(id_list, dict_map):\n",
    "    \"\"\"\n",
    "    Create list of indices.\n",
    "\n",
    "    Args:\n",
    "    - id_list (list) - List of masked atom or residue identifiers.\n",
    "    - dict_map (dict) - Dictionary mapping entities to indices.\n",
    "\n",
    "    Returns:\n",
    "    - A numpy array of the masked indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate the numpy index array\n",
    "    idx_array = np.array([dict_map[iden] for iden in id_list])\n",
    "\n",
    "    return idx_array\n",
    "\n",
    "\n",
    "class TesselateDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for structural data.\n",
    "\n",
    "    Args:\n",
    "    - accession_list (str) - File path from which to read PDB IDs for dataset.\n",
    "    - graph_dir (str) - Directory containing the nodes, edges, and mask files.\n",
    "    - contacts_dir (str) - Directory containing the .contacts files from\n",
    "        get_contacts.py.\n",
    "    - return_data (list) - List of datasets to return. Value must be 'all' or\n",
    "        a subset of the following list:\n",
    "            - pdb_id\n",
    "            - model\n",
    "            - atom_nodes\n",
    "            - atom_adj\n",
    "            - atom_contact\n",
    "            - atom_mask\n",
    "            - res_adj\n",
    "            - res_dist\n",
    "            - res_contact\n",
    "            - res_mask\n",
    "            - mem_mat\n",
    "            - idx2atom_dict\n",
    "            - idx2res_dict\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, accession_list, graph_dir, contacts_dir, add_covalent=False, return_data='all', in_memory=False):\n",
    "\n",
    "        if return_data == 'all':\n",
    "            self.return_data = [\n",
    "                'pdb_id',\n",
    "                'model',\n",
    "                'atom_nodes',\n",
    "                'atom_adj',\n",
    "                'atom_contact',\n",
    "                'atom_mask',\n",
    "                'res_adj',\n",
    "                'res_dist',\n",
    "                'conn_adj',\n",
    "                'res_contact',\n",
    "                'res_mask',\n",
    "                'mem_mat',\n",
    "                'idx2atom_dict',\n",
    "                'idx2res_dict'\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            self.return_data = return_data\n",
    "\n",
    "        # Store reference to accession list file\n",
    "        self.accession_list = accession_list\n",
    "\n",
    "        # Store references to the necessary directories\n",
    "        self.graph_dir = graph_dir\n",
    "        self.contacts_dir = contacts_dir\n",
    "\n",
    "        # Whether to add covalent bonds to prediction task and\n",
    "        # remove sequence non-deterministic covalent bonds from the adjacency matrix\n",
    "        self.add_covalent=add_covalent\n",
    "\n",
    "        # Read in and store a list of accession IDs\n",
    "        with open(accession_list, 'r') as handle:\n",
    "            self.accessions = np.array([acc.strip().lower().split() for acc in handle.readlines()])\n",
    "\n",
    "        self.data = {}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - Integer count of number of examples.\n",
    "        \"\"\"\n",
    "        return len(self.accessions)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an item with a particular index value.\n",
    "\n",
    "        Args:\n",
    "        - idx (int) - Index of desired sample.\n",
    "\n",
    "        Returns:\n",
    "        - Dictionary of dataset example. All tensors are sparse when possible.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            if idx in self.data:\n",
    "                return self.data[idx]\n",
    "\n",
    "            # initialize the return dictionary\n",
    "            return_dict = {}\n",
    "\n",
    "            acc_entry = self.accessions[idx]\n",
    "\n",
    "            # Get the PDB ID\n",
    "            acc = acc_entry[0]\n",
    "\n",
    "            # Get the model number if one exists\n",
    "            if len(acc_entry) == 1:\n",
    "                model = 1\n",
    "            else:\n",
    "                model = acc_entry[1]\n",
    "\n",
    "            # Read and process the files\n",
    "            data = read_files(acc, model, self.graph_dir, self.contacts_dir)\n",
    "            data = process_res_data(data)\n",
    "\n",
    "            # Generate the mapping dictionaries\n",
    "            atom2idx_dict, idx2atom_dict = get_map_dicts(data['atom_nodes']['atom'].unique())\n",
    "            res2idx_dict, idx2res_dict = get_map_dicts(data['res_nodes']['res'].unique())\n",
    "\n",
    "            # Get numbers of atoms and residues per sample\n",
    "            n_atoms = len(atom2idx_dict)\n",
    "            n_res = len(res2idx_dict)\n",
    "\n",
    "            # Handle all of the possible returned datasets\n",
    "            if 'pdb_id' in self.return_data:\n",
    "                return_dict['pdb_id'] = acc\n",
    "\n",
    "            if 'model' in self.return_data:\n",
    "                return_dict['model'] = model\n",
    "\n",
    "            if 'atom_nodes' in self.return_data:\n",
    "                ele_nums = [pt.elements.symbol(element).number for element in data['atom_nodes']['element']]\n",
    "                return_dict['atom_nodes'] = torch.LongTensor(ele_nums)\n",
    "                assert not torch.isnan(return_dict['atom_nodes']).any()\n",
    "\n",
    "            if 'atom_adj' in self.return_data:\n",
    "\n",
    "                adj = create_adj_mat(data['atom_edges'], atom2idx_dict, mat_type='atom_graph').T\n",
    "\n",
    "                x = torch.LongTensor(adj[0, :]).squeeze()\n",
    "                y = torch.LongTensor(adj[1, :]).squeeze()\n",
    "                val = torch.FloatTensor(adj[2, :]).squeeze()\n",
    "\n",
    "                atom_adj = torch.zeros([n_atoms, n_atoms]).index_put_((x, y), val, accumulate=False)\n",
    "\n",
    "                atom_adj = atom_adj.index_put_((y, x), val, accumulate=False)\n",
    "\n",
    "                atom_adj[range(n_atoms), range(n_atoms)] = 1\n",
    "\n",
    "                atom_adj = (atom_adj > 0).float()\n",
    "\n",
    "                return_dict['atom_adj'] = atom_adj\n",
    "\n",
    "                assert not torch.isnan(return_dict['atom_adj']).any()\n",
    "\n",
    "            if 'atom_contact' in self.return_data:\n",
    "                atom_contact = create_adj_mat(data['atom_contact'], atom2idx_dict, mat_type='atom_contact').T\n",
    "\n",
    "                x = torch.LongTensor(atom_contact[0, :]).squeeze()\n",
    "                y = torch.LongTensor(atom_contact[1, :]).squeeze()\n",
    "                z = torch.LongTensor(atom_contact[2, :]).squeeze()\n",
    "\n",
    "                atom_contact = torch.zeros([n_atoms, n_atoms, 8]).index_put_((x, y, z),\n",
    "                                                                        torch.ones(len(x)))\n",
    "                atom_contact = atom_contact.index_put_((y, x, z),\n",
    "                                                       torch.ones(len(x)))\n",
    "\n",
    "                return_dict['atom_contact'] = atom_contact\n",
    "\n",
    "                assert not torch.isnan(return_dict['atom_contact']).any()\n",
    "\n",
    "            if 'atom_mask' in self.return_data:\n",
    "                atom_mask = create_idx_list(data['atom_mask'], atom2idx_dict)\n",
    "\n",
    "                masked_pos = torch.from_numpy(atom_mask)\n",
    "\n",
    "                if self.add_covalent:\n",
    "                    channels = 9\n",
    "                else:\n",
    "                    channels = 8\n",
    "\n",
    "                mask = torch.ones([n_atoms, n_atoms, channels])\n",
    "\n",
    "                if len(masked_pos) > 0:\n",
    "                    mask[masked_pos, :, :] = 0\n",
    "                    mask[:, masked_pos, :] = 0\n",
    "\n",
    "                return_dict['atom_mask'] = mask\n",
    "\n",
    "                assert not torch.isnan(return_dict['atom_mask']).any()\n",
    "\n",
    "            if 'res_adj' in self.return_data:\n",
    "                adj = create_adj_mat(data['res_edges'], res2idx_dict, mat_type='res_graph').T\n",
    "\n",
    "                x = torch.LongTensor(adj[0, :]).squeeze()\n",
    "                y = torch.LongTensor(adj[1, :]).squeeze()\n",
    "\n",
    "                res_adj = torch.zeros([n_res, n_res]).index_put_((x, y), torch.ones(len(x)))\n",
    "\n",
    "                res_adj = res_adj.index_put_((y, x), torch.ones(len(x)))\n",
    "\n",
    "                res_adj[range(n_res), range(n_res)] = 1\n",
    "\n",
    "                return_dict['res_adj'] = res_adj\n",
    "\n",
    "                assert not torch.isnan(return_dict['res_adj']).any()\n",
    "\n",
    "                if 'res_dist' in self.return_data:\n",
    "                    G = nx.from_numpy_matrix(return_dict['res_adj'].numpy())\n",
    "                    res_dist = torch.from_numpy(nx.floyd_warshall_numpy(G)).float()\n",
    "\n",
    "                    res_dist[torch.isinf(res_dist)] = -1\n",
    "\n",
    "                    return_dict['res_dist'] = res_dist\n",
    "\n",
    "                    chain_mem = torch.zeros(res_dist.shape)\n",
    "                    chain_mem[~torch.isinf(return_dict['res_dist'])] = 1\n",
    "\n",
    "                    return_dict['chain_mem'] = chain_mem\n",
    "\n",
    "                    assert not torch.isnan(return_dict['res_dist']).any()\n",
    "                    assert not torch.isinf(return_dict['res_dist']).any()\n",
    "                    assert not torch.isnan(return_dict['chain_mem']).any()\n",
    "                    assert not torch.isinf(return_dict['chain_mem']).any()\n",
    "\n",
    "                if 'conn_adj' in self.return_data:\n",
    "\n",
    "                    conn_adj = create_conn_adj_mat(return_dict['res_adj'].numpy())\n",
    "\n",
    "                    return_dict['conn_adj'] = torch.from_numpy(conn_adj)\n",
    "\n",
    "            if 'res_contact' in self.return_data:\n",
    "                res_contact = create_adj_mat(data['res_contact'], res2idx_dict, mat_type='res_contact').T\n",
    "\n",
    "                x = torch.LongTensor(res_contact[0, :]).squeeze()\n",
    "                y = torch.LongTensor(res_contact[1, :]).squeeze()\n",
    "                z = torch.LongTensor(res_contact[2, :]).squeeze()\n",
    "\n",
    "                res_contact = torch.zeros([n_res, n_res, 8]).index_put_((x, y, z),\n",
    "                                                                        torch.ones(len(x)))\n",
    "\n",
    "                res_contact = res_contact.index_put_((y, x, z),\n",
    "                                                     torch.ones(len(x)))\n",
    "\n",
    "                return_dict['res_contact'] = res_contact\n",
    "\n",
    "                assert not torch.isnan(return_dict['res_contact']).any()\n",
    "\n",
    "            if 'res_mask' in self.return_data:\n",
    "                res_mask = create_idx_list(data['res_mask'], res2idx_dict)\n",
    "\n",
    "                masked_pos = torch.from_numpy(res_mask)\n",
    "\n",
    "                if self.add_covalent:\n",
    "                    channels = 9\n",
    "                else:\n",
    "                    channels = 8\n",
    "\n",
    "                mask = torch.ones([n_res, n_res, channels])\n",
    "\n",
    "                if len(masked_pos) > 0:\n",
    "                    mask[masked_pos, :, :] = 0\n",
    "                    mask[:, masked_pos, :] = 0\n",
    "\n",
    "                return_dict['res_mask'] = mask\n",
    "\n",
    "                assert not torch.isnan(return_dict['res_mask']).any()\n",
    "\n",
    "            if 'mem_mat' in self.return_data:\n",
    "                mem_mat = create_mem_mat(atom2idx_dict, res2idx_dict).T\n",
    "\n",
    "                x = torch.LongTensor(mem_mat[0, :]).squeeze()\n",
    "                y = torch.LongTensor(mem_mat[1, :]).squeeze()\n",
    "\n",
    "                mem_mat = torch.zeros([n_res, n_atoms]).index_put_((x, y),\n",
    "                                                                   torch.ones(len(x)))\n",
    "\n",
    "                return_dict['mem_mat'] = mem_mat\n",
    "\n",
    "                assert not torch.isnan(return_dict['mem_mat']).any()\n",
    "\n",
    "            if 'idx2atom_dict' in self.return_data:\n",
    "                return_dict['idx2atom_dict'] = idx2atom_dict\n",
    "\n",
    "            if 'idx2res_dict' in self.return_data:\n",
    "                return_dict['idx2res_dict'] = idx2res_dict\n",
    "\n",
    "            self.data[idx] = return_dict\n",
    "\n",
    "            # Return the processed data\n",
    "            return return_dict\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", acc, str(e))\n",
    "            return np.array([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yn1eZ_eMbSlj"
   },
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6k6mfHhlbSlk"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from torch_scatter import scatter_softmax\n",
    "\n",
    "\n",
    "####################\n",
    "# Embedding layers #\n",
    "####################\n",
    "\n",
    "class AtomEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Embed the atoms to fixed-length input vectors.\n",
    "\n",
    "    Args:\n",
    "    - num_features (int) - Size of the returned embedding vectors.\n",
    "    - scale_grad_by_freq (bool) - Scale gradients by the inverse of\n",
    "        frequency (default=True).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_features, scale_grad_by_freq=True):\n",
    "        super(AtomEmbed, self).__init__()\n",
    "        self.embedding = nn.Embedding(118,\n",
    "                                      n_features,\n",
    "                                      scale_grad_by_freq=scale_grad_by_freq)\n",
    "\n",
    "    def forward(self, atomic_numbers):\n",
    "        \"\"\"\n",
    "        Return the embeddings for each atom in the graph.\n",
    "\n",
    "        Args:\n",
    "        - atoms (torch.LongTensor) - Tensor (n_atoms) containing atomic numbers.\n",
    "\n",
    "        Returns:\n",
    "        - torch.FloatTensor of dimension (n_atoms, n_features) containing\n",
    "            the embedding vectors.\n",
    "        \"\"\"\n",
    "\n",
    "        # Get and return the embeddings for each atom\n",
    "        embedded_atoms = self.embedding(atomic_numbers)\n",
    "        return embedded_atoms\n",
    "\n",
    "\n",
    "####################\n",
    "# Attention layers #\n",
    "####################\n",
    "\n",
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b\n",
    "\n",
    "\n",
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)\n",
    "\n",
    "\n",
    "class GraphAttnLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttnLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "\n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
    "\n",
    "        N = input.size()[0]\n",
    "\n",
    "        if adj.shape[0] == adj.shape[1]:\n",
    "            edge = adj.nonzero().t()\n",
    "        else:\n",
    "            edge = adj\n",
    "\n",
    "#         mask = adj.byte()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        values = self.leakyrelu(self.a.mm(edge_h).squeeze())\n",
    "\n",
    "        edge_e = scatter_softmax(values, edge[0])\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n",
    "\n",
    "\n",
    "class GraphAttn(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttn, self).__init__()\n",
    "        self.attn = GraphAttnLayer(in_features, out_features, dropout, alpha, concat)\n",
    "        self.batch_norm = nn.BatchNorm1d(out_features)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "\n",
    "        out = self.attn(input, adj)\n",
    "        out_norm = self.batch_norm(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadGraphAttn(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head graph attention layer.\n",
    "\n",
    "    Args:\n",
    "    - n_head (int) - Number of heads for the attention layer.\n",
    "    - in_features (int) - Number of total input features.\n",
    "    - out_features (int) - Number of output features per head.\n",
    "    - dropout (bool) - P(keep) for dropout.\n",
    "    - alpha (float) - Alpha value for leaky ReLU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads, in_features, out_features, dropout, alpha):\n",
    "        super(MultiHeadGraphAttn, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Operations\n",
    "        heads = [GraphAttn(in_features, out_features, dropout, alpha)\n",
    "                 for i in range(n_heads)]\n",
    "        self.attn_heads = nn.ModuleList(heads)\n",
    "\n",
    "    def forward(self, nodes, adj, cat_dim=1):\n",
    "        \"\"\"\n",
    "        Perform forward pass through multi-head graph attention layer.\n",
    "\n",
    "        Args:\n",
    "        - nodes (torch.FloatTensor) - Node feature matrix\n",
    "            (n_nodes, in_features).\n",
    "        - adj (torch.FloatTensor) - Adjacency matrix (n_nodes, n_nodes).\n",
    "\n",
    "        Returns:\n",
    "        - torch.FloatTensor of dimension (n_nodes, n_nodes) of attentional\n",
    "            coefficients where a_ij is the attention value of for node j with\n",
    "            respect to node i.\n",
    "        \"\"\"\n",
    "\n",
    "        if cat_dim == 1:\n",
    "            vals = torch.cat([head(nodes, adj) for head in self.attn_heads],\n",
    "                         dim = 1)\n",
    "\n",
    "        elif cat_dim == -1:\n",
    "            vals = torch.stack([head(nodes, adj) for head in self.attn_heads])\n",
    "\n",
    "        return vals\n",
    "\n",
    "\n",
    "class FCContactPred(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully connected layer to perform contact prediction.\n",
    "\n",
    "    Args:\n",
    "    - node_features (int) - Number of input features.\n",
    "    - out_features (int) - Number of output prediction values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, node_features, out_preds, layers=3):\n",
    "        super(FCContactPred, self).__init__()\n",
    "\n",
    "        self.linear_first = nn.Linear(node_features, 25, bias=True)\n",
    "\n",
    "        self.int_layers = nn.ModuleList([nn.Linear(25, 25, bias=True)\n",
    "                                         for i in range(layers - 2)])\n",
    "\n",
    "        self.linear_final = nn.Linear(25, out_preds, bias=True)\n",
    "\n",
    "    def forward(self, combined_nodes):\n",
    "        \"\"\"\n",
    "        Predict pointwise multichannel contacts from summarized pairwise\n",
    "        residue features.\n",
    "\n",
    "        Args:\n",
    "        - nodes (torch.FloatTensor) - Tensor of (convolved) node features\n",
    "            (n_pairwise, n_features).\n",
    "\n",
    "        Returns:\n",
    "        - torch.FloatTensor (n_contacts, n_channels) containing the prediction\n",
    "            for every potential contact point and every contact channel.\n",
    "        \"\"\"\n",
    "        # Get the logits from the linear layer\n",
    "        prelogits = self.linear_first(combined_nodes)\n",
    "        prelogits = F.relu(prelogits)\n",
    "\n",
    "        for layer in self.int_layers:\n",
    "            prelogits = layer(prelogits)\n",
    "            prelogits = F.relu(prelogits)\n",
    "\n",
    "        logits = self.linear_final(prelogits)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "class AttnLinkPredict(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head graph attention for link prediction.\n",
    "\n",
    "    Args:\n",
    "    - n_layers (int) - Number of layers.\n",
    "    - n_head (int) - Number of heads for the attention layer.\n",
    "    - in_features (int) - Number of total input features.\n",
    "    - out_features (int) - Number of output features per head.\n",
    "    - dropout (bool) - P(keep) for dropout.\n",
    "    - alpha (float) - Alpha value for leaky ReLU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, n_heads, in_features, out_features, dropout, alpha):\n",
    "        super(AttnLinkPredict, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "        layers = [MultiHeadGraphAttn(n_heads, in_features,\n",
    "                                     out_features, dropout,\n",
    "                                     alpha)]\n",
    "\n",
    "        layers.extend([MultiHeadGraphAttn(n_heads, n_heads * out_features,\n",
    "                                     out_features, dropout,\n",
    "                                     alpha)\n",
    "                      for i in range(n_layers - 1)])\n",
    "\n",
    "        self.attn_layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.attn_batch_norm = nn.BatchNorm1d(n_heads * out_features)\n",
    "\n",
    "        self.output_linear = nn.Linear(n_heads * out_features, 8)\n",
    "\n",
    "    def forward(self, nodes, adjacency):\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for target1, target2 in it.combinations_with_replacement(range(len(nodes)), 2):\n",
    "            targets = adjacency[[target1, target2], :]\n",
    "\n",
    "            where = [target1, target2]\n",
    "            where.extend([ind for ind in np.where(targets.cpu() > 0)[1] if ind not in where])\n",
    "\n",
    "            out_adj_size = len(where) + 1\n",
    "            adj = torch.zeros(out_adj_size, out_adj_size)\n",
    "            adj[[0, 0, 1, 2], [1, 2, 0, 0]] = 1\n",
    "\n",
    "            neighbors = adjacency[where][:, where]\n",
    "            adj[1:, 1:] = neighbors\n",
    "\n",
    "            node_feats = nodes[where]\n",
    "\n",
    "            node_feats = torch.cat((torch.sum(node_feats[:2], dim=0, keepdim=True), node_feats), dim=0)\n",
    "\n",
    "            for layer in self.attn_layers:\n",
    "                node_feats = layer(node_feats, adj)\n",
    "\n",
    "            outputs.append(node_feats[0, :].unsqueeze(0))\n",
    "\n",
    "        linear_in = self.attn_batch_norm(torch.cat(outputs, dim=0))\n",
    "\n",
    "        return self.output_linear(linear_in)\n",
    "\n",
    "\n",
    "class LinkPredict(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head graph attention for link prediction.\n",
    "\n",
    "    Args:\n",
    "    - n_layers (int) - Number of layers.\n",
    "    - n_head (int) - Number of heads for the attention layer.\n",
    "    - in_features (int) - Number of total input features.\n",
    "    - out_features (int) - Number of output features per head.\n",
    "    - dropout (bool) - P(keep) for dropout.\n",
    "    - alpha (float) - Alpha value for leaky ReLU.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_layers, n_heads, in_features, out_features, dropout, alpha):\n",
    "        super(AttnLinkPredict, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "        layers = [MultiHeadGraphAttn(n_heads, in_features,\n",
    "                                     out_features, dropout,\n",
    "                                     alpha)]\n",
    "\n",
    "        layers.extend([MultiHeadGraphAttn(n_heads, n_heads * out_features,\n",
    "                                     out_features, dropout,\n",
    "                                     alpha)\n",
    "                      for i in range(n_layers - 1)])\n",
    "\n",
    "        self.attn_layers = nn.ModuleList(layers)\n",
    "\n",
    "        self.attn_batch_norm = nn.BatchNorm1d(n_heads * out_features)\n",
    "\n",
    "        self.output_linear = nn.Linear(n_heads * out_features, 8)\n",
    "\n",
    "    def forward(self, nodes, adjacency):\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for target1, target2 in it.combinations_with_replacement(range(len(nodes)), 2):\n",
    "            targets = adjacency[[target1, target2], :]\n",
    "\n",
    "            where = [target1, target2]\n",
    "            where.extend([ind for ind in np.where(targets.cpu() > 0)[1] if ind not in where])\n",
    "\n",
    "            out_adj_size = len(where) + 1\n",
    "            adj = torch.zeros(out_adj_size, out_adj_size)\n",
    "            adj[[0, 0, 1, 2], [1, 2, 0, 0]] = 1\n",
    "\n",
    "            neighbors = adjacency[where][:, where]\n",
    "            adj[1:, 1:] = neighbors\n",
    "\n",
    "            node_feats = nodes[where]\n",
    "\n",
    "            node_feats = torch.cat((torch.sum(node_feats[:2], dim=0, keepdim=True), node_feats), dim=0)\n",
    "\n",
    "            for layer in self.attn_layers:\n",
    "                node_feats = layer(node_feats, adj)\n",
    "\n",
    "            outputs.append(node_feats[0, :].unsqueeze(0))\n",
    "\n",
    "        linear_in = self.attn_batch_norm(torch.cat(outputs, dim=0))\n",
    "\n",
    "        return self.output_linear(linear_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HyQx2QqYbSlo"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NvIeIhAcbSlp"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "########################################\n",
    "# Pairwise matrix generation functions #\n",
    "########################################\n",
    "\n",
    "def pairwise_mat(nodes, method='mean'):\n",
    "    \"\"\"\n",
    "    Generate matrix for pairwise determination of interactions.\n",
    "\n",
    "    Args:\n",
    "    - nodes (torch.FloatTensor) - Tensor of node (n_nodes, n_features) features.\n",
    "    - method (str) - One of 'sum' or 'mean' for combination startegy for\n",
    "        pairwise combination matrix (default = 'mean').\n",
    "\n",
    "    Returns:\n",
    "    - torch.FloatTensor of shape (n_pairwise, n_nodes) than can be used used to\n",
    "        combine feature vectors. Values are 1 if method == \"sum\" and 0.5 if\n",
    "        method == \"mean\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the upper triangle indices\n",
    "    triu = np.vstack(np.triu_indices(nodes.shape[0]))\n",
    "\n",
    "    # Loop through all indices and add to list with\n",
    "    idxs = torch.from_numpy(triu).T\n",
    "\n",
    "    # Convert to tensor\n",
    "    combos = torch.zeros([idxs.shape[0], nodes.shape[0]]).scatter(1, idxs, 1)\n",
    "\n",
    "    # Set to 0.5 if method is 'mean'\n",
    "    if method == 'mean':\n",
    "        combos *= 0.5\n",
    "\n",
    "    return combos\n",
    "\n",
    "\n",
    "####################################\n",
    "# Pairwise concatenation functions #\n",
    "####################################\n",
    "\n",
    "def cat_pairwise(embeddings):\n",
    "\n",
    "    triu = np.vstack(np.triu_indices(embeddings.shape[0])).T\n",
    "\n",
    "    node1 = []\n",
    "    node2 = []\n",
    "\n",
    "    for i, j in triu:\n",
    "        node1.append(embeddings[i])\n",
    "        node2.append(embeddings[j])\n",
    "\n",
    "    node1 = torch.stack(node1, dim=0)\n",
    "    node2 = torch.flip(torch.stack(node2, dim=0), dims=(1,))\n",
    "\n",
    "    return torch.cat((node1, node2), dim=1)\n",
    "\n",
    "\n",
    "############################\n",
    "# Upper triangle functions #\n",
    "############################\n",
    "\n",
    "def triu_condense(input_tensor):\n",
    "    \"\"\"\n",
    "    Condense the upper triangle of a tensor into a 2d dense representation.\n",
    "\n",
    "    Args:\n",
    "    - input_tensor (torch.Tensor) - Tensor of shape (n, n, m).\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of shape (n(n+1)/2, m) where elements along the third dimension in\n",
    "        the original tensor are packed row-wise according to the upper\n",
    "        triangular indices.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get upper triangle index info\n",
    "    row_idx, col_idx = np.triu_indices(input_tensor.shape[0])\n",
    "    row_idx = torch.LongTensor(row_idx)\n",
    "    col_idx = torch.LongTensor(col_idx)\n",
    "\n",
    "    # Return the packed matrix\n",
    "    output = input_tensor[row_idx, col_idx, :]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def triu_expand(input_matrix):\n",
    "    \"\"\"\n",
    "    Expand a dense representation of the upper triangle of a tensor into\n",
    "    a 3D squareform representation.\n",
    "\n",
    "    Args:\n",
    "    - input_matrix (torch.Tensor) - Tensor of shape (n(n+1)/2, m).\n",
    "\n",
    "    Returns:\n",
    "    - Tensor of shape (n, n, m) where elements along the third dimension in the\n",
    "        original tensor are packed row-wise according to the upper triangular\n",
    "        indices.\n",
    "    \"\"\"\n",
    "    # Get the edge size n of the tensor\n",
    "    n_elements = input_matrix.shape[0]\n",
    "    n_chan = input_matrix.shape[1]\n",
    "    n_res = int((-1 + np.sqrt(1 + 4 * 2 * (n_elements))) / 2)\n",
    "\n",
    "    # Get upper triangle index info\n",
    "    row_idx, col_idx = np.triu_indices(n_res)\n",
    "    row_idx = torch.LongTensor(row_idx)\n",
    "    col_idx = torch.LongTensor(col_idx)\n",
    "\n",
    "    # Generate the output tensor\n",
    "    output = torch.zeros((n_res, n_res, n_chan))\n",
    "\n",
    "    # Input the triu values\n",
    "    for i in range(n_chan):\n",
    "        i_tens = torch.full((len(row_idx),), i, dtype=torch.long)\n",
    "        output.index_put_((row_idx, col_idx, i_tens), input_matrix[:, i])\n",
    "\n",
    "    # Input the tril values\n",
    "    for i in range(n_chan):\n",
    "        i_tens = torch.full((len(row_idx),), i, dtype=torch.long)\n",
    "        output.index_put_((col_idx, row_idx, i_tens), input_matrix[:, i])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mJZARlB3bSlt"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aZTJ299UbSlv"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch import autograd\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from tqdm import *\n",
    "import wandb\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "def track_mem():\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if (torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data))) and hasattr(obj, '__name__'):\n",
    "                print(obj.__name__, obj.size())\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "class GAT(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, embed_features, atom_out_features, res_out_features,\n",
    "                 n_contact_channels, dropout, alpha, train_data, val_data,\n",
    "                 test_data):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        # Properties\n",
    "        self.embed_features = embed_features\n",
    "        self.atom_out_features = atom_out_features\n",
    "        self.res_out_features = res_out_features\n",
    "        self.n_contact_channels = n_contact_channels\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # Datasets\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "        # Model components\n",
    "        self.embed = AtomEmbed(embed_features, scale_grad_by_freq=True)\n",
    "\n",
    "        self.embed_batch_norm = nn.BatchNorm1d(embed_features)\n",
    "\n",
    "        # Define number of graph conv layers and heads\n",
    "        n_atom_layers = 5\n",
    "        n_atom_heads = 3\n",
    "\n",
    "        n_res_layers = 2\n",
    "        n_res_heads = 3\n",
    "\n",
    "        # Set up atom attention\n",
    "        self.atom_attns = nn.ModuleList([])\n",
    "        for i in range(n_atom_layers):\n",
    "            if i == 0:\n",
    "                attn_layer = MultiHeadGraphAttn(n_atom_heads, embed_features,\n",
    "                                                atom_out_features,\n",
    "                                                dropout, alpha)\n",
    "            else:\n",
    "                attn_layer = MultiHeadGraphAttn(n_atom_heads,\n",
    "                                                n_atom_heads * atom_out_features,\n",
    "                                                atom_out_features,\n",
    "                                                dropout, alpha)\n",
    "\n",
    "            self.atom_attns.append(attn_layer)\n",
    "\n",
    "\n",
    "        # Set up condensation\n",
    "        self.condense_batch_norm = nn.BatchNorm1d(n_atom_heads * atom_out_features)\n",
    "\n",
    "        # Set up res attention\n",
    "        self.res_attns = nn.ModuleList([])\n",
    "        for i in range(n_res_layers):\n",
    "            if i == 0:\n",
    "                attn_layer = MultiHeadGraphAttn(n_res_heads,\n",
    "                                                n_atom_heads * atom_out_features,\n",
    "                                                res_out_features, dropout,\n",
    "                                                alpha)\n",
    "            else:\n",
    "                attn_layer = MultiHeadGraphAttn(n_res_heads,\n",
    "                                                n_res_heads * res_out_features,\n",
    "                                                res_out_features, dropout,\n",
    "                                                alpha)\n",
    "\n",
    "            self.res_attns.append(attn_layer)\n",
    "\n",
    "        self.final_attn = MultiHeadGraphAttn(n_res_heads,\n",
    "                                             n_res_heads * res_out_features,\n",
    "                                             res_out_features, dropout,\n",
    "                                             alpha)\n",
    "\n",
    "        self.conn_attn = MultiHeadGraphAttn(1, n_res_heads * res_out_features,\n",
    "                                            res_out_features, dropout,\n",
    "                                            alpha)\n",
    "\n",
    "        self.activation = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "        self.linear1 = nn.Linear(1 * res_out_features + 2, 10)\n",
    "        self.linear2 = nn.Linear(10, 10)\n",
    "        self.linear_predict = nn.Linear(10, 8)\n",
    "\n",
    "        self.pred_contact_batch_norm = nn.BatchNorm1d(n_contact_channels)\n",
    "\n",
    "        self.activations = {}\n",
    "\n",
    "\n",
    "        # Validation information\n",
    "        self.reset_epoch_metrics()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Put on gpus\n",
    "        self.embed.cuda(0)\n",
    "        self.embed_batch_norm.cuda(0)\n",
    "        [layer.cuda(0) for layer in self.atom_attns]\n",
    "#         self.condense.cuda(0)\n",
    "        self.condense_batch_norm.cuda(0)\n",
    "        [layer.cuda(0) for layer in self.res_attns]\n",
    "        self.final_attn.cuda(0)\n",
    "        self.linear1.cuda(1)\n",
    "        self.linear2.cuda(1)\n",
    "        self.linear_predict.cuda(1)\n",
    "#         self.conn_attn.cuda(1)\n",
    "        self.pred_contact_batch_norm.cuda(1)\n",
    "\n",
    "        # Regular forward pass\n",
    "        atom_embed = self.embed(x['atom_nodes'].squeeze().cuda(0))\n",
    "\n",
    "        atom_embed_update = self.embed_batch_norm(atom_embed)\n",
    "\n",
    "        atom_adj = x['atom_adj'].squeeze().cuda(0)\n",
    "\n",
    "        for layer in self.atom_attns:\n",
    "                atom_embed_update = checkpoint(layer, atom_embed_update, atom_adj)\n",
    "\n",
    "        res_embed = torch.matmul( x['mem_mat'].squeeze().cuda(0), atom_embed_update)\n",
    "\n",
    "        res_embed_update = self.condense_batch_norm(res_embed)\n",
    "\n",
    "        adj = x['res_adj'].squeeze().cuda(0)\n",
    "\n",
    "        for layer in self.res_attns:\n",
    "            res_embed_update = checkpoint(layer, res_embed_update, adj)\n",
    "\n",
    "        res_embed_update = self.final_attn(res_embed_update, adj)\n",
    "\n",
    "#         preds = torch.matmul(res_embed_update, res_embed_update.T)\n",
    "\n",
    "        preds = torch.bmm(res_embed_update.T.unsqueeze(2),\n",
    "                          res_embed_update.T.unsqueeze(1)).permute(1, 2, 0)\n",
    "\n",
    "        preds = triu_condense(preds).cpu()\n",
    "        preds = self.conn_attn(preds, x['conn_adj'].squeeze()).cuda(1)\n",
    "\n",
    "        preds = torch.cat((preds,\n",
    "                           triu_condense(x['res_dist'].permute(1, 2, 0).cuda(1)),\n",
    "                           triu_condense(x['chain_mem'].permute(1, 2, 0).cuda(1))),\n",
    "                          dim=-1)\n",
    "\n",
    "        preds = torch.relu(self.linear1(preds))\n",
    "        preds = torch.relu(self.linear2(preds))\n",
    "        preds = self.linear_predict(preds)\n",
    "\n",
    "#         atom_attns = [[head.f_attn for head in layer.attn_heads]\n",
    "#                       for layer in self.atom_attns]\n",
    "#         res_attns = [[head.f_attn for head in layer.attn_heads]\n",
    "#                       for layer in self.res_attns]\n",
    "\n",
    "        preds = self.pred_contact_batch_norm(preds)\n",
    "\n",
    "#         self.activations = {\n",
    "#             'atom_embed': atom_embed.detach().cpu().numpy(),\n",
    "#             'atom_embed_update': atom_embed_update.detach().cpu().numpy(),\n",
    "# #             'atom_attn': atom_attns,\n",
    "#             'res_embed': res_embed.detach().cpu().numpy(),\n",
    "#             'res_embed_update': res_embed_update.detach().cpu().numpy(),\n",
    "# #             'res_attn': res_attns,\n",
    "# #             'pairwise': pairwise.detach().cpu().numpy(),\n",
    "# #             'combined': combined.detach().cpu().numpy(),\n",
    "#             'preds': preds.detach().cpu().numpy()\n",
    "#         }\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # REQUIRED\n",
    "        y_hat = self.forward(batch).squeeze()\n",
    "        y = triu_condense(batch['res_contact'].squeeze()).cuda(1)\n",
    "        weights = triu_condense(batch['res_mask'].squeeze()).cuda(1)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y, weight=weights)\n",
    "\n",
    "        self.train_losses.append(loss.item())\n",
    "\n",
    "        return {'loss': loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        # OPTIONAL\n",
    "        y_hat = self.forward(batch).squeeze()\n",
    "        y = triu_condense(batch['res_contact'].squeeze()).cuda(1)\n",
    "        weights = triu_condense(batch['res_mask'].squeeze()).cuda(1)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y, weight=weights)\n",
    "\n",
    "        self.val_losses.append(loss.item())\n",
    "\n",
    "        _, _, auroc = calc_metric_curve(y_hat.sigmoid().cpu(), y.cpu(), 'ROC', squareform=False)\n",
    "        _, _, auprc = calc_metric_curve(y_hat.sigmoid().cpu(), y.cpu(), 'PRC', squareform=False)\n",
    "\n",
    "        self.auroc['Hydrophobic'].append(auroc[0])\n",
    "        self.auroc['Hydrogen bond'].append(auroc[1])\n",
    "        self.auroc['Van der Waals'].append(auroc[2])\n",
    "        self.auroc['Water bridges'].append(auroc[3])\n",
    "        self.auroc['Salt bridges'].append(auroc[4])\n",
    "        self.auroc['Pi-stacking'].append(auroc[5])\n",
    "        self.auroc['Pi-cation'].append(auroc[6])\n",
    "        self.auroc['T-stacking'].append(auroc[7])\n",
    "\n",
    "        self.auprc['Hydrophobic'].append(auprc[0])\n",
    "        self.auprc['Hydrogen bond'].append(auprc[1])\n",
    "        self.auprc['Van der Waals'].append(auprc[2])\n",
    "        self.auprc['Water bridges'].append(auprc[3])\n",
    "        self.auprc['Salt bridges'].append(auprc[4])\n",
    "        self.auprc['Pi-stacking'].append(auprc[5])\n",
    "        self.auprc['Pi-cation'].append(auprc[6])\n",
    "        self.auprc['T-stacking'].append(auprc[7])\n",
    "\n",
    "        if batch_nb == 0:\n",
    "            self.pred_example = plot_channels(triu_expand(y_hat.detach().cpu()))\n",
    "\n",
    "    def on_post_performance_check(self):\n",
    "\n",
    "        epoch_metrics = {}\n",
    "        all_metrics = {}\n",
    "\n",
    "        for key in self.auroc:\n",
    "            values = np.asarray(self.auroc[key])\n",
    "            all_metrics[f'AUROC - {key}'] = values\n",
    "\n",
    "            values = values[~np.isnan(values)]\n",
    "            epoch_metrics[f'AUROC - {key}'] = wandb.Histogram(values)\n",
    "\n",
    "\n",
    "        for key in self.auprc:\n",
    "            values = np.asarray(self.auprc[key])\n",
    "            all_metrics[f'AUPRC - {key}'] = values\n",
    "\n",
    "            values = values[~np.isnan(values)]\n",
    "            epoch_metrics[f'AUPRC - {key}'] = wandb.Histogram(values)\n",
    "\n",
    "        epoch_metrics['Train Losses'] = wandb.Histogram(self.train_losses)\n",
    "\n",
    "        epoch_metrics['Val Losses'] = wandb.Histogram(self.val_losses)\n",
    "        all_metrics['Val Losses'] = np.asarray(self.val_losses)\n",
    "\n",
    "        all_metrics = pd.DataFrame(all_metrics)\n",
    "        cor_mat = all_metrics.corr()\n",
    "\n",
    "        corr_plot = metric_corr_plot(cor_mat)\n",
    "        epoch_metrics['Metric Corr Plot'] = wandb.Image(corr_plot)\n",
    "\n",
    "        epoch_metrics['train_loss'] = np.asarray(self.train_losses).mean()\n",
    "        epoch_metrics['val_loss'] = np.asarray(self.val_losses).mean()\n",
    "\n",
    "        epoch_metrics['Example Prediction'] = wandb.Image(self.pred_example)\n",
    "\n",
    "        self.logger.experiment.log(epoch_metrics)\n",
    "\n",
    "        self.reset_epoch_metrics()\n",
    "\n",
    "    def reset_epoch_metrics(self):\n",
    "\n",
    "        # Validation information\n",
    "        self.auroc = {\n",
    "            'Hydrophobic': [],\n",
    "            'Hydrogen bond': [],\n",
    "            'Van der Waals': [],\n",
    "            'Water bridges': [],\n",
    "            'Salt bridges': [],\n",
    "            'Pi-stacking': [],\n",
    "            'Pi-cation': [],\n",
    "            'T-stacking': []\n",
    "        }\n",
    "\n",
    "        self.auprc = {\n",
    "            'Hydrophobic': [],\n",
    "            'Hydrogen bond': [],\n",
    "            'Van der Waals': [],\n",
    "            'Water bridges': [],\n",
    "            'Salt bridges': [],\n",
    "            'Pi-stacking': [],\n",
    "            'Pi-cation': [],\n",
    "            'T-stacking': []\n",
    "        }\n",
    "\n",
    "        self.val_losses = []\n",
    "        self.train_losses = []\n",
    "\n",
    "\n",
    "#     def validation_end(self, outputs):\n",
    "#         # OPTIONAL\n",
    "# #         avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "# #         return {'avg_val_loss': avg_loss}\n",
    "#         pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        # can return multiple optimizers and learning_rate schedulers\n",
    "\n",
    "        parameters = filter(lambda p: p.requires_grad, self.parameters())\n",
    "\n",
    "        return torch.optim.SGD(parameters, lr=0.1, momentum=0.9, weight_decay=1e-6)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        # REQUIRED\n",
    "        return DataLoader(self.train_data, shuffle=True, num_workers=30, pin_memory=True)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(self.val_data, shuffle=False, num_workers=20, pin_memory=True)\n",
    "\n",
    "#     def on_batch_end(self):\n",
    "#         wandb.log({key: plt.imshow(self.activations[key][:20]) for key in self.activations})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def plot_channels(values):\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))\n",
    "\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    channel_names = [\n",
    "        'Hydrophobic',\n",
    "        'Hydrogen bond',\n",
    "        'Van der Waals',\n",
    "        'Water bridges',\n",
    "        'Salt bridges',\n",
    "        'Pi-stacking',\n",
    "        'Pi-cation',\n",
    "        'T-stacking'\n",
    "    ]\n",
    "\n",
    "    for channel in range(values.shape[-1]):\n",
    "        ax[channel].imshow(values[:, :, channel].squeeze(), vmin=0, vmax=1)\n",
    "        ax[channel].set(title=channel_names[channel], xlabel='Residue #', ylabel='Residue #')\n",
    "\n",
    "    plt.close()\n",
    "    return fig\n",
    "\n",
    "\n",
    "######################\n",
    "# ROC and PRC curves #\n",
    "######################\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "\n",
    "def calc_metric_curve(preds, target, curve_type, squareform=False):\n",
    "    \"\"\"\n",
    "    Calculate ROC or PRC curves and area for the predicted contact channels.\n",
    "\n",
    "    Args:\n",
    "    - preds (np.ndarray) - Numpy array of model predictions either of form\n",
    "        (n_res, n_res, n_chan) or (n_res * [n_res - 1] / 2, n_chan).\n",
    "    - target (np.ndarray) - Numpy array of target values either of form\n",
    "        (n_res, n_res, n_chan) or (n_res * [n_res - 1] / 2, n_chan),\n",
    "        must match form of preds.\n",
    "    - curve_type (str) - One of 'ROC' or 'PRC' to denote type of curve.\n",
    "    - squareform (bool) - True if tensors are of shape (n_res, n_res, n_chan),\n",
    "        False if they are of shape (n_res * [n_res - 1] / 2, n_chan)\n",
    "        (default = True).\n",
    "\n",
    "    Returns:\n",
    "    - Tuple of x, y, and AUC values to be used for plotting the curves\n",
    "        using plot_curve metric.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get correct curve function\n",
    "    if curve_type.upper() == 'ROC':\n",
    "        curve_func = roc_curve\n",
    "    elif curve_type.upper() == 'PRC':\n",
    "        curve_func = precision_recall_curve\n",
    "\n",
    "    # Generate dicts to hold outputs from curve generation functions\n",
    "    x = dict()\n",
    "    y = dict()\n",
    "    auc_ = dict()\n",
    "\n",
    "    # Handle case of squareform matrix (only get non-redundant triu indices)\n",
    "    if squareform:\n",
    "        indices = np.triu_indices(target.shape[0])\n",
    "\n",
    "    # For each channel\n",
    "    for i in range(target.shape[-1]):\n",
    "\n",
    "        # Handle case of squareform\n",
    "        if squareform:\n",
    "            var1, var2, _ = curve_func(target[:, :, i][indices],\n",
    "                                       preds[:, :, i][indices])\n",
    "\n",
    "        # Handle case of pairwise\n",
    "        else:\n",
    "            var1, var2, _ = curve_func(target[:, i], preds[:, i])\n",
    "\n",
    "        # Assign outputs to correct dict for plotting\n",
    "        if curve_type.upper() == 'ROC':\n",
    "            x[i] = var1\n",
    "            y[i] = var2\n",
    "        elif curve_type.upper() == 'PRC':\n",
    "            x[i] = var2\n",
    "            y[i] = var1\n",
    "\n",
    "        # Calc AUC\n",
    "        auc_[i] = auc(x[i], y[i])\n",
    "\n",
    "    return (x, y, auc_)\n",
    "\n",
    "\n",
    "def plot_curve_metric(x, y, auc, curve_type, title=None, labels=None):\n",
    "    \"\"\"\n",
    "    Plot ROC or PRC curves per output channel.\n",
    "\n",
    "    Args:\n",
    "    - x (dict) - Dict of numpy arrays for values to plot on x axis.\n",
    "    - y (dict) - Dict of numpy arrays for values to plot on x axis.\n",
    "    - auc (dict) - Dict of numpy arrays for areas under each curve.\n",
    "    - curve_type (str) - One of 'ROC' or 'PRC' to denote type of curve.\n",
    "    - title\n",
    "    - labels\n",
    "\n",
    "    Returns:\n",
    "    - pyplot object of curves.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate figure\n",
    "    plt.figure()\n",
    "\n",
    "    # Linetype spec\n",
    "    lw = 2\n",
    "    curve_type = curve_type.upper()\n",
    "\n",
    "    # Get the number of channels being plotted\n",
    "    n_chan = len(x)\n",
    "\n",
    "    # Make labels numeric if not provided\n",
    "    if labels is None:\n",
    "        labels = list(range(n_chan))\n",
    "\n",
    "    # Check to make sure the labels are the right length\n",
    "    if len(labels) != n_chan:\n",
    "        raise ValueError('Number of labels ({}) does not match number of prediction channels ({}).'.format(len(labels), n_chan))\n",
    "\n",
    "    # Get a lit of colors for all the channels\n",
    "    color_list = plt.cm.Set1(np.linspace(0, 1, n_chan))\n",
    "\n",
    "    # Plot each line\n",
    "    for i, color in enumerate(color_list):\n",
    "        plt.plot(x[i], y[i], color=color,\n",
    "                 lw=lw, label='{} (area = {:0.2f})'.format(labels[i], auc[i]))\n",
    "\n",
    "    # Add labels and diagonal line for ROC\n",
    "    if curve_type == 'ROC':\n",
    "        xlab = 'FPR'\n",
    "        ylab = 'TPR'\n",
    "        plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # Add labels for PRC\n",
    "    elif curve_type == 'PRC':\n",
    "        xlab = 'Recall'\n",
    "        ylab = 'Precision'\n",
    "        plt.legend(loc=\"lower left\")\n",
    "\n",
    "    # Extend limits, add labels and title\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel(xlab)\n",
    "    plt.ylabel(ylab)\n",
    "\n",
    "    if title is not None:\n",
    "        plt.title('{} for {}'.format(curve_type, title))\n",
    "    else:\n",
    "        plt.title('{}'.format(curve_type))\n",
    "\n",
    "    return plt.axes()\n",
    "\n",
    "def plot_curve(preds, target, curve_type, title=None, labels=None,\n",
    "               squareform=False):\n",
    "    \"\"\"\n",
    "    Wrapper to directly plot curves from model output and target.\n",
    "\n",
    "    Args:\n",
    "    - preds (np array-like) - Array or tensor of predicted values output by\n",
    "        model.\n",
    "    - target (np array-like) - Array or tensor of target values.\n",
    "    - curve_type (str) - One of 'ROC' or 'PRC'.\n",
    "    - title (str) - Title of plot (default = None).\n",
    "    - labels (list) - List of labels for each channel on the plot\n",
    "        (default = None).\n",
    "    - squareform (bool) - Whether the predictions and targets are in square form\n",
    "        (default = False).\n",
    "    \"\"\"\n",
    "    x, y, auc_ = calc_metric_curve(preds, target, curve_type, squareform)\n",
    "    return plot_curve_metric(x, y, auc_, curve_type, title, labels)\n",
    "\n",
    "\n",
    "################################\n",
    "# Correlations between metrics #\n",
    "################################\n",
    "\n",
    "def metric_corr_plot(corr):\n",
    "    mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(11, 9))\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1.0, vmin = -1.0, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3QZxiiHbSl1"
   },
   "source": [
    "## Training\n",
    "\n",
    "### Instantiate dataloader and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6gcL3lpbbSl3"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "data_base = '/home/tshimko/tesselate/'\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "data_load = [\n",
    "                'pdb_id',\n",
    "#                 'model',\n",
    "                'atom_nodes',\n",
    "                'atom_adj',\n",
    "#                 'atom_contact',\n",
    "#                 'atom_mask',\n",
    "                'res_adj',\n",
    "                'res_dist',\n",
    "                'chain_mem',\n",
    "                'res_contact',\n",
    "                'conn_adj',\n",
    "                'res_mask',\n",
    "                'mem_mat',\n",
    "#                 'idx2atom_dict',\n",
    "#                 'idx2res_dict'\n",
    "            ]\n",
    "\n",
    "# train_data = TesselateDataset(data_base + 'id_lists/ligand_free_monomers/small_train.txt',\n",
    "#                               graph_dir=data_base + 'data/graphs',\n",
    "#                               contacts_dir=data_base + 'data/contacts',\n",
    "#                               return_data=data_load, in_memory=False)\n",
    "\n",
    "# train_data = TesselateDataset(data_base + 'test4.txt',\n",
    "#                               graph_dir=data_base + 'data/graphs',\n",
    "#                               contacts_dir=data_base + 'data/contacts',\n",
    "#                               return_data=data_load, in_memory=True)\n",
    "\n",
    "train_data = TesselateDataset(data_base + 'test3.txt',\n",
    "                              graph_dir=data_base + 'data/graphs',\n",
    "                              contacts_dir=data_base + 'data/contacts',\n",
    "                              return_data=data_load, in_memory=True)\n",
    "\n",
    "val_data = TesselateDataset(data_base + 'test3.txt',\n",
    "                              graph_dir=data_base + 'data/graphs',\n",
    "                              contacts_dir=data_base + 'data/contacts',\n",
    "                              return_data=data_load, in_memory=True)\n",
    "\n",
    "# val_data = TesselateDataset(data_base + 'id_lists/ligand_free_monomers/small_val.txt',\n",
    "#                             graph_dir=data_base + 'data/graphs',\n",
    "#                             contacts_dir=data_base + 'data/contacts',\n",
    "#                             return_data=data_load, in_memory=True)\n",
    "\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=os.getcwd() + '/checkpoints',\n",
    "    save_top_k=0,\n",
    "    verbose=False,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    prefix=''\n",
    ")\n",
    "\n",
    "model = GAT(embed_features=10, atom_out_features=10, res_out_features=10,\n",
    "             n_contact_channels=8, dropout=0, alpha=0.2, train_data=train_data, val_data=val_data,\n",
    "             test_data=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11611,
     "status": "ok",
     "timestamp": 1574002255203,
     "user": {
      "displayName": "Tyler Carter Shimko",
      "photoUrl": "",
      "userId": "18332645348589660395"
     },
     "user_tz": -60
    },
    "id": "od4NiBExbSl7",
    "outputId": "5fc65a3e-de61-40e7-afdf-eb5de7e1ef4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/5 [00:00<?, ?batch/s]/home/tshimko/.virtualenvs/tesselate/lib/python3.6/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/tshimko/.virtualenvs/tesselate/lib/python3.6/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/tshimko/.virtualenvs/tesselate/lib/python3.6/site-packages/sklearn/metrics/_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n",
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/2 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|     | 1/2 [00:02<00:02,  2.20s/batch]"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/tshimko126/tesselate\" target=\"_blank\">https://app.wandb.ai/tshimko126/tesselate</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/tshimko126/tesselate/runs/t3utmwlv\" target=\"_blank\">https://app.wandb.ai/tshimko126/tesselate/runs/t3utmwlv</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Wandb version 0.8.28 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  50%|     | 1/2 [00:03<00:02,  2.20s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 1: 100%|| 2/2 [00:05<00:00,  2.50s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tshimko/.virtualenvs/tesselate/lib/python3.6/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/home/tshimko/.virtualenvs/tesselate/lib/python3.6/site-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "/home/tshimko/.virtualenvs/tesselate/lib/python3.6/site-packages/sklearn/metrics/_ranking.py:677: RuntimeWarning: invalid value encountered in true_divide\n",
      "  recall = tps / tps[-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|| 2/2 [00:06<00:00,  2.50s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 2:  50%|     | 1/2 [00:02<00:02,  2.39s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 2: 100%|| 2/2 [00:04<00:00,  2.19s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 3:  50%|     | 1/2 [00:02<00:02,  2.18s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 3: 100%|| 2/2 [00:05<00:00,  2.04s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 4:  50%|     | 1/2 [00:02<00:02,  2.15s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 4: 100%|| 2/2 [00:05<00:00,  1.96s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 5:  50%|     | 1/2 [00:02<00:02,  2.05s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 5: 100%|| 2/2 [00:04<00:00,  1.89s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 6:  50%|     | 1/2 [00:02<00:01,  1.98s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 6: 100%|| 2/2 [00:04<00:00,  1.90s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 7:  50%|     | 1/2 [00:02<00:01,  1.96s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 7: 100%|| 2/2 [00:04<00:00,  1.82s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 8:  50%|     | 1/2 [00:02<00:01,  1.96s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 8: 100%|| 2/2 [00:05<00:00,  1.86s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 9:  50%|     | 1/2 [00:02<00:01,  1.99s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 9: 100%|| 2/2 [00:04<00:00,  1.86s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 10:  50%|     | 1/2 [00:02<00:01,  1.96s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 10: 100%|| 2/2 [00:05<00:00,  1.90s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 11:  50%|     | 1/2 [00:02<00:02,  2.00s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 11: 100%|| 2/2 [00:05<00:00,  1.89s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 12:  50%|     | 1/2 [00:02<00:01,  1.99s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 12: 100%|| 2/2 [00:04<00:00,  1.86s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 13:  50%|     | 1/2 [00:02<00:02,  2.01s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 13: 100%|| 2/2 [00:05<00:00,  1.91s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 14:  50%|     | 1/2 [00:02<00:01,  2.00s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 14: 100%|| 2/2 [00:05<00:00,  1.94s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 15:  50%|     | 1/2 [00:02<00:02,  2.08s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 15: 100%|| 2/2 [00:05<00:00,  1.94s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 16:  50%|     | 1/2 [00:02<00:02,  2.10s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 16: 100%|| 2/2 [00:05<00:00,  1.94s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 17:  50%|     | 1/2 [00:02<00:02,  2.04s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 17: 100%|| 2/2 [00:05<00:00,  1.95s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 18:  50%|     | 1/2 [00:02<00:02,  2.06s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 18: 100%|| 2/2 [00:05<00:00,  1.95s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 19:  50%|     | 1/2 [00:02<00:02,  2.05s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 19: 100%|| 2/2 [00:05<00:00,  1.91s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 20:  50%|     | 1/2 [00:02<00:02,  2.06s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 20: 100%|| 2/2 [00:05<00:00,  1.95s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 21:  50%|     | 1/2 [00:02<00:02,  2.07s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 21: 100%|| 2/2 [00:05<00:00,  2.07s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 22:  50%|     | 1/2 [00:02<00:02,  2.16s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 22: 100%|| 2/2 [00:05<00:00,  2.04s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 23:  50%|     | 1/2 [00:02<00:02,  2.13s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 23: 100%|| 2/2 [00:05<00:00,  1.99s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 24:  50%|     | 1/2 [00:02<00:02,  2.14s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 24: 100%|| 2/2 [00:05<00:00,  1.98s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 25:  50%|     | 1/2 [00:02<00:02,  2.13s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 25: 100%|| 2/2 [00:05<00:00,  1.98s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 26:  50%|     | 1/2 [00:02<00:02,  2.12s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 26: 100%|| 2/2 [00:05<00:00,  1.97s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 27:  50%|     | 1/2 [00:02<00:02,  2.08s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 27: 100%|| 2/2 [00:05<00:00,  1.99s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 28:  50%|     | 1/2 [00:02<00:02,  2.10s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 28: 100%|| 2/2 [00:05<00:00,  2.01s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 29:  50%|     | 1/2 [00:02<00:02,  2.13s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 29: 100%|| 2/2 [00:05<00:00,  1.99s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 30:  50%|     | 1/2 [00:02<00:02,  2.11s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 30: 100%|| 2/2 [00:05<00:00,  2.00s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 31:  50%|     | 1/2 [00:02<00:02,  2.14s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 31: 100%|| 2/2 [00:05<00:00,  1.98s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 32:  50%|     | 1/2 [00:02<00:02,  2.11s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 32: 100%|| 2/2 [00:05<00:00,  2.02s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 33:  50%|     | 1/2 [00:02<00:02,  2.12s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 33: 100%|| 2/2 [00:05<00:00,  2.05s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 34:  50%|     | 1/2 [00:02<00:02,  2.18s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 34: 100%|| 2/2 [00:05<00:00,  2.09s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 35:  50%|     | 1/2 [00:02<00:02,  2.19s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 35: 100%|| 2/2 [00:05<00:00,  2.04s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 36:  50%|     | 1/2 [00:02<00:02,  2.16s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 36: 100%|| 2/2 [00:05<00:00,  2.02s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 37:  50%|     | 1/2 [00:02<00:02,  2.17s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 37: 100%|| 2/2 [00:05<00:00,  2.02s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 38:  50%|     | 1/2 [00:02<00:02,  2.16s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 38: 100%|| 2/2 [00:05<00:00,  2.02s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 39:  50%|     | 1/2 [00:02<00:02,  2.14s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 39: 100%|| 2/2 [00:05<00:00,  2.04s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 40:  50%|     | 1/2 [00:02<00:02,  2.18s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 40: 100%|| 2/2 [00:05<00:00,  2.06s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 41:  50%|     | 1/2 [00:02<00:02,  2.20s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 41: 100%|| 2/2 [00:05<00:00,  2.06s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 42:  50%|     | 1/2 [00:02<00:02,  2.26s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 42: 100%|| 2/2 [00:05<00:00,  2.09s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 43:  50%|     | 1/2 [00:02<00:02,  2.20s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 43: 100%|| 2/2 [00:05<00:00,  2.05s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 44:  50%|     | 1/2 [00:02<00:02,  2.18s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 44: 100%|| 2/2 [00:05<00:00,  2.02s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 45:  50%|     | 1/2 [00:02<00:02,  2.13s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 45: 100%|| 2/2 [00:06<00:00,  2.04s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 46:  50%|     | 1/2 [00:02<00:02,  2.18s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 46: 100%|| 2/2 [00:05<00:00,  2.04s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 47:  50%|     | 1/2 [00:02<00:02,  2.24s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 47: 100%|| 2/2 [00:05<00:00,  2.10s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 48:  50%|     | 1/2 [00:02<00:02,  2.24s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 48: 100%|| 2/2 [00:05<00:00,  2.16s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 49:  50%|     | 1/2 [00:02<00:02,  2.24s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 49: 100%|| 2/2 [00:05<00:00,  2.10s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 50:  50%|     | 1/2 [00:02<00:02,  2.23s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 50: 100%|| 2/2 [00:05<00:00,  2.10s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 51:  50%|     | 1/2 [00:02<00:02,  2.24s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 51: 100%|| 2/2 [00:05<00:00,  2.10s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 52:  50%|     | 1/2 [00:02<00:02,  2.23s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 52: 100%|| 2/2 [00:05<00:00,  2.09s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 53:  50%|     | 1/2 [00:02<00:02,  2.25s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 53: 100%|| 2/2 [00:05<00:00,  2.11s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 54:  50%|     | 1/2 [00:02<00:02,  2.24s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 54: 100%|| 2/2 [00:05<00:00,  2.09s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 55:  50%|     | 1/2 [00:02<00:02,  2.22s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 55: 100%|| 2/2 [00:05<00:00,  2.15s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 56:  50%|     | 1/2 [00:02<00:02,  2.27s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 56: 100%|| 2/2 [00:05<00:00,  2.16s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 57:  50%|     | 1/2 [00:02<00:02,  2.32s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 57: 100%|| 2/2 [00:06<00:00,  2.55s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 58:  50%|     | 1/2 [00:02<00:02,  2.56s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 58: 100%|| 2/2 [00:05<00:00,  2.31s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 59:  50%|     | 1/2 [00:02<00:02,  2.42s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 59: 100%|| 2/2 [00:05<00:00,  2.23s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 60:  50%|     | 1/2 [00:02<00:02,  2.33s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 60: 100%|| 2/2 [00:05<00:00,  2.20s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 61:  50%|     | 1/2 [00:02<00:02,  2.31s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 61: 100%|| 2/2 [00:05<00:00,  2.23s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 62:  50%|     | 1/2 [00:02<00:02,  2.33s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 62: 100%|| 2/2 [00:05<00:00,  2.20s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 63:  50%|     | 1/2 [00:02<00:02,  2.38s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 63: 100%|| 2/2 [00:05<00:00,  2.21s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 64:  50%|     | 1/2 [00:02<00:02,  2.33s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 64: 100%|| 2/2 [00:05<00:00,  2.25s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 65:  50%|     | 1/2 [00:02<00:02,  2.34s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 65: 100%|| 2/2 [00:05<00:00,  2.18s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 66:  50%|     | 1/2 [00:02<00:02,  2.30s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 66: 100%|| 2/2 [00:05<00:00,  2.19s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 67:  50%|     | 1/2 [00:02<00:02,  2.32s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 67: 100%|| 2/2 [00:05<00:00,  2.15s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 68:  50%|     | 1/2 [00:02<00:02,  2.29s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 68: 100%|| 2/2 [00:05<00:00,  2.16s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 69:  50%|     | 1/2 [00:02<00:02,  2.33s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 69: 100%|| 2/2 [00:05<00:00,  2.17s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 70:  50%|     | 1/2 [00:02<00:02,  2.32s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 70: 100%|| 2/2 [00:05<00:00,  2.17s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 71:  50%|     | 1/2 [00:02<00:02,  2.35s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 71: 100%|| 2/2 [00:06<00:00,  2.57s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 72:  50%|     | 1/2 [00:02<00:02,  2.61s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 72: 100%|| 2/2 [00:05<00:00,  2.37s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 73:  50%|     | 1/2 [00:02<00:02,  2.49s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 73: 100%|| 2/2 [00:05<00:00,  2.31s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 74:  50%|     | 1/2 [00:02<00:02,  2.45s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 74: 100%|| 2/2 [00:05<00:00,  2.26s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 75:  50%|     | 1/2 [00:02<00:02,  2.41s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 75: 100%|| 2/2 [00:05<00:00,  2.25s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 76:  50%|     | 1/2 [00:02<00:02,  2.39s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 76: 100%|| 2/2 [00:05<00:00,  2.24s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 77:  50%|     | 1/2 [00:02<00:02,  2.43s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 77: 100%|| 2/2 [00:05<00:00,  2.28s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 78:  50%|     | 1/2 [00:02<00:02,  2.41s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 78: 100%|| 2/2 [00:05<00:00,  2.23s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 79:  50%|     | 1/2 [00:02<00:02,  2.41s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 79: 100%|| 2/2 [00:05<00:00,  2.25s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 80:  50%|     | 1/2 [00:02<00:02,  2.40s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 80: 100%|| 2/2 [00:05<00:00,  2.24s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 81:  50%|     | 1/2 [00:02<00:02,  2.40s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 81: 100%|| 2/2 [00:05<00:00,  2.25s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 82:  50%|     | 1/2 [00:02<00:02,  2.39s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 82: 100%|| 2/2 [00:05<00:00,  2.22s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 83:  50%|     | 1/2 [00:02<00:02,  2.38s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 83: 100%|| 2/2 [00:05<00:00,  2.21s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 84:  50%|     | 1/2 [00:02<00:02,  2.41s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 84: 100%|| 2/2 [00:05<00:00,  2.25s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 85:  50%|     | 1/2 [00:02<00:02,  2.39s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 85: 100%|| 2/2 [00:05<00:00,  2.26s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 86:  50%|     | 1/2 [00:02<00:02,  2.41s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 86: 100%|| 2/2 [00:05<00:00,  2.25s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 87:  50%|     | 1/2 [00:02<00:02,  2.44s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 87: 100%|| 2/2 [00:06<00:00,  2.33s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 88:  50%|     | 1/2 [00:02<00:02,  2.47s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 88: 100%|| 2/2 [00:07<00:00,  2.33s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 89:  50%|     | 1/2 [00:02<00:02,  2.43s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 89: 100%|| 2/2 [00:05<00:00,  2.26s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 90:  50%|     | 1/2 [00:02<00:02,  2.41s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 90: 100%|| 2/2 [00:05<00:00,  2.26s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 91:  50%|     | 1/2 [00:02<00:02,  2.42s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 91: 100%|| 2/2 [00:05<00:00,  2.26s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 92:  50%|     | 1/2 [00:02<00:02,  2.43s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 92: 100%|| 2/2 [00:06<00:00,  2.29s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 93:  50%|     | 1/2 [00:02<00:02,  2.48s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 93: 100%|| 2/2 [00:06<00:00,  2.34s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 94:  50%|     | 1/2 [00:02<00:02,  2.49s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 94: 100%|| 2/2 [00:05<00:00,  2.31s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 95:  50%|     | 1/2 [00:02<00:02,  2.45s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 95: 100%|| 2/2 [00:05<00:00,  2.31s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 96:  50%|     | 1/2 [00:02<00:02,  2.44s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 96: 100%|| 2/2 [00:05<00:00,  2.29s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 97:  50%|     | 1/2 [00:02<00:02,  2.46s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 97: 100%|| 2/2 [00:05<00:00,  2.30s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 98:  50%|     | 1/2 [00:02<00:02,  2.47s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 98: 100%|| 2/2 [00:05<00:00,  2.31s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 99:  50%|     | 1/2 [00:02<00:02,  2.46s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 99: 100%|| 2/2 [00:05<00:00,  2.33s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 100:  50%|     | 1/2 [00:02<00:02,  2.50s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 100: 100%|| 2/2 [00:05<00:00,  2.33s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 101:  50%|     | 1/2 [00:02<00:02,  2.47s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 101: 100%|| 2/2 [00:05<00:00,  2.32s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 102:  50%|     | 1/2 [00:03<00:02,  2.53s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 102: 100%|| 2/2 [00:06<00:00,  2.37s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 103:  50%|     | 1/2 [00:02<00:02,  2.51s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 103: 100%|| 2/2 [00:06<00:00,  2.36s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 104:  50%|     | 1/2 [00:02<00:02,  2.50s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 104: 100%|| 2/2 [00:06<00:00,  2.36s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 105:  50%|     | 1/2 [00:02<00:02,  2.51s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 105: 100%|| 2/2 [00:05<00:00,  2.36s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 106:  50%|     | 1/2 [00:02<00:02,  2.49s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 106: 100%|| 2/2 [00:05<00:00,  2.34s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 107:  50%|     | 1/2 [00:02<00:02,  2.53s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 107: 100%|| 2/2 [00:06<00:00,  2.43s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 108:  50%|     | 1/2 [00:02<00:02,  2.56s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 108: 100%|| 2/2 [00:06<00:00,  2.44s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 109:  50%|     | 1/2 [00:02<00:02,  2.58s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 109: 100%|| 2/2 [00:07<00:00,  2.94s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 110:  50%|     | 1/2 [00:02<00:02,  2.91s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 110: 100%|| 2/2 [00:06<00:00,  2.65s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 111:  50%|     | 1/2 [00:02<00:02,  2.71s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 111: 100%|| 2/2 [00:06<00:00,  2.52s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 112:  50%|     | 1/2 [00:02<00:02,  2.61s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 112: 100%|| 2/2 [00:05<00:00,  2.41s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 113:  50%|     | 1/2 [00:02<00:02,  2.56s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 113: 100%|| 2/2 [00:06<00:00,  2.39s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 114:  50%|     | 1/2 [00:02<00:02,  2.54s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 114: 100%|| 2/2 [00:06<00:00,  2.39s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 115:  50%|     | 1/2 [00:02<00:02,  2.52s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 115: 100%|| 2/2 [00:06<00:00,  2.41s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 116:  50%|     | 1/2 [00:03<00:02,  2.63s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 116: 100%|| 2/2 [00:06<00:00,  2.50s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 117:  50%|     | 1/2 [00:02<00:02,  2.63s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 117: 100%|| 2/2 [00:06<00:00,  2.50s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 118:  50%|     | 1/2 [00:02<00:02,  2.64s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 118: 100%|| 2/2 [00:06<00:00,  2.48s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 119:  50%|     | 1/2 [00:02<00:02,  2.61s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 119: 100%|| 2/2 [00:06<00:00,  2.42s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 120:  50%|     | 1/2 [00:02<00:02,  2.57s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 120: 100%|| 2/2 [00:06<00:00,  2.42s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 121:  50%|     | 1/2 [00:02<00:02,  2.57s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 121: 100%|| 2/2 [00:06<00:00,  2.39s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 122:  50%|     | 1/2 [00:03<00:02,  2.61s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 122: 100%|| 2/2 [00:06<00:00,  2.48s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 123:  50%|     | 1/2 [00:02<00:02,  2.60s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 123: 100%|| 2/2 [00:06<00:00,  2.46s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 124:  50%|     | 1/2 [00:02<00:02,  2.60s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 124: 100%|| 2/2 [00:06<00:00,  2.47s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 125:  50%|     | 1/2 [00:02<00:02,  2.61s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 125: 100%|| 2/2 [00:06<00:00,  2.51s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 126:  50%|     | 1/2 [00:03<00:02,  2.68s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 126: 100%|| 2/2 [00:06<00:00,  2.48s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 127:  50%|     | 1/2 [00:03<00:02,  2.66s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 127: 100%|| 2/2 [00:06<00:00,  2.50s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 128:  50%|     | 1/2 [00:03<00:02,  2.66s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 128: 100%|| 2/2 [00:06<00:00,  2.48s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 129:  50%|     | 1/2 [00:03<00:02,  2.64s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 129: 100%|| 2/2 [00:06<00:00,  2.50s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 130:  50%|     | 1/2 [00:03<00:02,  2.67s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 130: 100%|| 2/2 [00:06<00:00,  2.53s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 131:  50%|     | 1/2 [00:02<00:02,  2.64s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 131: 100%|| 2/2 [00:06<00:00,  2.53s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 132:  50%|     | 1/2 [00:03<00:02,  2.69s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 132: 100%|| 2/2 [00:06<00:00,  2.52s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 133:  50%|     | 1/2 [00:03<00:02,  2.68s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 133: 100%|| 2/2 [00:06<00:00,  2.50s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 134:  50%|     | 1/2 [00:02<00:02,  2.64s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 134: 100%|| 2/2 [00:08<00:00,  3.15s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 135:  50%|     | 1/2 [00:02<00:03,  3.10s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 135: 100%|| 2/2 [00:06<00:00,  2.80s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 136:  50%|     | 1/2 [00:03<00:02,  2.88s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 136: 100%|| 2/2 [00:06<00:00,  2.65s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 137:  50%|     | 1/2 [00:02<00:02,  2.75s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 137: 100%|| 2/2 [00:06<00:00,  2.57s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 138:  50%|     | 1/2 [00:03<00:02,  2.70s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 138: 100%|| 2/2 [00:06<00:00,  2.56s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 139:  50%|     | 1/2 [00:03<00:02,  2.75s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 139: 100%|| 2/2 [00:06<00:00,  2.56s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 140:  50%|     | 1/2 [00:03<00:02,  2.73s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 140: 100%|| 2/2 [00:06<00:00,  2.53s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 141:  50%|     | 1/2 [00:03<00:02,  2.68s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 141: 100%|| 2/2 [00:06<00:00,  2.50s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 142:  50%|     | 1/2 [00:03<00:02,  2.69s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 142: 100%|| 2/2 [00:06<00:00,  2.50s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 143:  50%|     | 1/2 [00:03<00:02,  2.67s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 143: 100%|| 2/2 [00:06<00:00,  2.54s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 144:  50%|     | 1/2 [00:03<00:02,  2.69s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 144: 100%|| 2/2 [00:06<00:00,  2.58s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 145:  50%|     | 1/2 [00:03<00:02,  2.73s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 145: 100%|| 2/2 [00:06<00:00,  2.58s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 146:  50%|     | 1/2 [00:03<00:02,  2.74s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 146: 100%|| 2/2 [00:06<00:00,  2.58s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 147:  50%|     | 1/2 [00:03<00:02,  2.72s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 147: 100%|| 2/2 [00:06<00:00,  2.54s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 148:  50%|     | 1/2 [00:03<00:02,  2.72s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 148: 100%|| 2/2 [00:06<00:00,  2.59s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 149:  50%|     | 1/2 [00:03<00:02,  2.77s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 149: 100%|| 2/2 [00:06<00:00,  2.64s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 150:  50%|     | 1/2 [00:03<00:02,  2.80s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 150: 100%|| 2/2 [00:06<00:00,  2.60s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 151:  50%|     | 1/2 [00:03<00:02,  2.77s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 151: 100%|| 2/2 [00:06<00:00,  2.59s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 152:  50%|     | 1/2 [00:03<00:02,  2.83s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 152: 100%|| 2/2 [00:06<00:00,  2.66s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 153:  50%|     | 1/2 [00:03<00:02,  2.82s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 153: 100%|| 2/2 [00:06<00:00,  2.64s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 154:  50%|     | 1/2 [00:03<00:02,  2.79s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 154: 100%|| 2/2 [00:06<00:00,  2.61s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 155:  50%|     | 1/2 [00:03<00:02,  2.76s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 155: 100%|| 2/2 [00:06<00:00,  2.60s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 156:  50%|     | 1/2 [00:03<00:02,  2.83s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 156: 100%|| 2/2 [00:06<00:00,  2.65s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 157:  50%|     | 1/2 [00:03<00:02,  2.79s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 157: 100%|| 2/2 [00:06<00:00,  2.68s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 158:  50%|     | 1/2 [00:03<00:02,  2.86s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 158: 100%|| 2/2 [00:06<00:00,  2.66s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 159:  50%|     | 1/2 [00:03<00:02,  2.83s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 159: 100%|| 2/2 [00:06<00:00,  2.66s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 160:  50%|     | 1/2 [00:03<00:02,  2.86s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 160: 100%|| 2/2 [00:06<00:00,  2.65s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 161:  50%|     | 1/2 [00:03<00:02,  2.82s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 161: 100%|| 2/2 [00:06<00:00,  2.64s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 162:  50%|     | 1/2 [00:03<00:02,  2.80s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 162: 100%|| 2/2 [00:06<00:00,  2.62s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 163:  50%|     | 1/2 [00:03<00:02,  2.78s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 163: 100%|| 2/2 [00:06<00:00,  2.62s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 164:  50%|     | 1/2 [00:03<00:02,  2.89s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 164: 100%|| 2/2 [00:09<00:00,  2.67s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 165:  50%|     | 1/2 [00:03<00:02,  2.82s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 165: 100%|| 2/2 [00:06<00:00,  2.68s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 166:  50%|     | 1/2 [00:03<00:02,  2.90s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 166: 100%|| 2/2 [00:06<00:00,  2.71s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 167:  50%|     | 1/2 [00:03<00:02,  2.86s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 167: 100%|| 2/2 [00:06<00:00,  2.68s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 168:  50%|     | 1/2 [00:03<00:02,  2.82s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 168: 100%|| 2/2 [00:06<00:00,  2.65s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 169:  50%|     | 1/2 [00:03<00:02,  2.92s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 169: 100%|| 2/2 [00:06<00:00,  2.72s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 170:  50%|     | 1/2 [00:03<00:02,  2.86s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 170: 100%|| 2/2 [00:06<00:00,  2.71s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 171:  50%|     | 1/2 [00:03<00:02,  2.88s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 171: 100%|| 2/2 [00:06<00:00,  2.72s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 172:  50%|     | 1/2 [00:03<00:02,  2.88s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 172: 100%|| 2/2 [00:06<00:00,  2.70s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 173:  50%|     | 1/2 [00:03<00:02,  2.95s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 173: 100%|| 2/2 [00:07<00:00,  2.76s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 174:  50%|     | 1/2 [00:03<00:02,  2.91s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 174: 100%|| 2/2 [00:06<00:00,  2.75s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 175:  50%|     | 1/2 [00:03<00:02,  2.92s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 175: 100%|| 2/2 [00:06<00:00,  2.75s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 176:  50%|     | 1/2 [00:03<00:02,  2.97s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 176: 100%|| 2/2 [00:07<00:00,  2.77s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 177:  50%|     | 1/2 [00:03<00:02,  2.89s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 177: 100%|| 2/2 [00:06<00:00,  2.71s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 178:  50%|     | 1/2 [00:03<00:02,  2.91s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 178: 100%|| 2/2 [00:07<00:00,  2.77s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 179:  50%|     | 1/2 [00:03<00:02,  2.93s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 179: 100%|| 2/2 [00:06<00:00,  2.75s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 180:  50%|     | 1/2 [00:03<00:02,  2.92s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 180: 100%|| 2/2 [00:06<00:00,  2.75s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 181:  50%|     | 1/2 [00:03<00:02,  2.91s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 181: 100%|| 2/2 [00:06<00:00,  2.73s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 182:  50%|     | 1/2 [00:03<00:02,  2.92s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 182: 100%|| 2/2 [00:06<00:00,  2.72s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 183:  50%|     | 1/2 [00:03<00:02,  2.94s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 183: 100%|| 2/2 [00:06<00:00,  2.74s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 184:  50%|     | 1/2 [00:03<00:02,  2.89s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 184: 100%|| 2/2 [00:07<00:00,  2.80s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 185:  50%|     | 1/2 [00:03<00:02,  2.97s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 185: 100%|| 2/2 [00:06<00:00,  2.77s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 186:  50%|     | 1/2 [00:03<00:02,  2.94s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 186: 100%|| 2/2 [00:06<00:00,  2.76s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 187:  50%|     | 1/2 [00:03<00:02,  2.93s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 187: 100%|| 2/2 [00:06<00:00,  2.72s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 188:  50%|     | 1/2 [00:03<00:02,  2.93s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 188: 100%|| 2/2 [00:07<00:00,  2.78s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 189:  50%|     | 1/2 [00:03<00:02,  2.96s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 189: 100%|| 2/2 [00:07<00:00,  2.80s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 190:  50%|     | 1/2 [00:03<00:03,  3.05s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 190: 100%|| 2/2 [00:07<00:00,  2.81s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 191:  50%|     | 1/2 [00:03<00:02,  3.00s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 191: 100%|| 2/2 [00:07<00:00,  2.81s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 192:  50%|     | 1/2 [00:03<00:03,  3.03s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 192: 100%|| 2/2 [00:07<00:00,  2.87s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 193:  50%|     | 1/2 [00:03<00:03,  3.03s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 193: 100%|| 2/2 [00:06<00:00,  2.84s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 194:  50%|     | 1/2 [00:03<00:03,  3.03s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 194: 100%|| 2/2 [00:07<00:00,  2.84s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 195:  50%|     | 1/2 [00:03<00:03,  3.01s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 195: 100%|| 2/2 [00:07<00:00,  2.82s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 196:  50%|     | 1/2 [00:03<00:03,  3.01s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 196: 100%|| 2/2 [00:07<00:00,  2.85s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 197:  50%|     | 1/2 [00:03<00:03,  3.02s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 197: 100%|| 2/2 [00:07<00:00,  2.83s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 198:  50%|     | 1/2 [00:03<00:02,  2.99s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 198: 100%|| 2/2 [00:07<00:00,  2.83s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 199:  50%|     | 1/2 [00:03<00:02,  2.99s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 199: 100%|| 2/2 [00:06<00:00,  2.78s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 200:  50%|     | 1/2 [00:03<00:03,  3.04s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 200: 100%|| 2/2 [00:07<00:00,  2.85s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 201:  50%|     | 1/2 [00:03<00:03,  3.04s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 201: 100%|| 2/2 [00:10<00:00,  3.91s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 202:  50%|     | 1/2 [00:03<00:03,  3.76s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 202: 100%|| 2/2 [00:07<00:00,  3.36s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 203:  50%|     | 1/2 [00:03<00:03,  3.44s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 203: 100%|| 2/2 [00:07<00:00,  3.13s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 204:  50%|     | 1/2 [00:03<00:03,  3.22s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 204: 100%|| 2/2 [00:07<00:00,  3.00s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Epoch 205:  50%|     | 1/2 [00:03<00:03,  3.12s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]\n",
      "Validating:   0%|          | 0/1 [00:00<?, ?batch/s]\u001b[A\n",
      "Epoch 205: 100%|| 2/2 [00:05<00:00,  2.91s/batch, batch_idx=0, loss=0.000, v_num=t3utmwlv]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-210c3991ed01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m                   \u001b[0mearly_stop_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulate_grad_batches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                   logger=logger) #gpus=1) #, gradient_clip_val=0.5) # amp_level='O2', use_amp=True) <-- for half precision\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_schedulers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;31m# return 1 when finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# update LR schedulers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;31m# fast_dev_run always forces val checking after train batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_dev_run\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mshould_check_val\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;31m# when logs should be saved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/pytorch_lightning/trainer/evaluation_loop.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(self, test)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0;31m# hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_post_performance_check\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# add model specific metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a27d7a6e88cc>\u001b[0m in \u001b[0;36mon_post_performance_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mepoch_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mepoch_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Example Prediction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_example\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/wandb/data_types.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_or_path, mode, caption, grouping)\u001b[0m\n\u001b[1;32m    751\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_matplotlib_typename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_full_typename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_matplotlib_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPILImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPILImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   2178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2180\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframeon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, **kwargs)\u001b[0m\n\u001b[1;32m   2089\u001b[0m                     \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morientation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2090\u001b[0m                     \u001b[0mbbox_inches_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bbox_inches_restore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2091\u001b[0;31m                     **kwargs)\n\u001b[0m\u001b[1;32m   2092\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2093\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_inches\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mrestore_bbox\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m             \u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_renderer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mRendererAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m             \u001b[0;31m# A GUI class may be need to update a window using this draw, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# don't forget to call the superclass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer)\u001b[0m\n\u001b[1;32m   1707\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1708\u001b[0m             mimage._draw_list_compositing_images(\n\u001b[0;32m-> 1709\u001b[0;31m                 renderer, self, artists, self.suppressComposite)\n\u001b[0m\u001b[1;32m   1710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1711\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'figure'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, inframe)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_rasterizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2647\u001b[0;31m         \u001b[0mmimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_list_compositing_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'axes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/image.py\u001b[0m in \u001b[0;36m_draw_list_compositing_images\u001b[0;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnot_composite\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0martists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m             \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrenderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# Composite any adjacent images together\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mdraw_wrapper\u001b[0;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mrenderer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0martist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrenderer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_agg_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mdraw\u001b[0;34m(self, renderer, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1203\u001b[0m         \u001b[0mticks_to_draw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_ticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m         ticklabelBoxes, ticklabelBoxes2 = self._get_tick_bboxes(ticks_to_draw,\n\u001b[0;32m-> 1205\u001b[0;31m                                                                 renderer)\n\u001b[0m\u001b[1;32m   1206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mticks_to_draw\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m_get_tick_bboxes\u001b[0;34m(self, ticks, renderer)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0;32m-> 1150\u001b[0;31m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[0m\u001b[1;32m   1151\u001b[0m                 [tick.label2.get_window_extent(renderer)\n\u001b[1;32m   1152\u001b[0m                  for tick in ticks if tick.label2.get_visible()])\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;34m\"\"\"Return lists of bboxes for ticks' label1's and label2's.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m         return ([tick.label1.get_window_extent(renderer)\n\u001b[0;32m-> 1150\u001b[0;31m                  for tick in ticks if tick.label1.get_visible()],\n\u001b[0m\u001b[1;32m   1151\u001b[0m                 [tick.label2.get_window_extent(renderer)\n\u001b[1;32m   1152\u001b[0m                  for tick in ticks if tick.label2.get_visible()])\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/text.py\u001b[0m in \u001b[0;36mget_window_extent\u001b[0;34m(self, renderer, dpi)\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_renderer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unitless_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    893\u001b[0m         \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdpi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/tesselate/lib/python3.6/site-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mget_transform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIdentityTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m         elif (not isinstance(self._transform, Transform)\n\u001b[0m\u001b[1;32m    378\u001b[0m               and hasattr(self._transform, '_as_mpl_transform')):\n\u001b[1;32m    379\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_mpl_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.logging import WandbLogger\n",
    "\n",
    "logger = WandbLogger(project='tesselate')\n",
    "\n",
    "trainer = Trainer(max_nb_epochs=2000, checkpoint_callback=checkpoint_callback,\n",
    "                  early_stop_callback=False, accumulate_grad_batches=8,\n",
    "                  logger=logger) #gpus=1) #, gradient_clip_val=0.5) # amp_level='O2', use_amp=True) <-- for half precision\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ycs4WmYKrqoE"
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acts = model.activations\n",
    "# model.cpu()\n",
    "for idx, i in enumerate(DataLoader(val_data, shuffle=False, num_workers=30, pin_memory=True)):\n",
    "    print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = triu_expand(out).sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plot_channels(preds.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(triu_expand(i['res_contact'].squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the predictions\n",
    "preds = triu_expand(torch.from_numpy(acts['preds'])).sigmoid()\n",
    "plot_channels(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the true values\n",
    "plot_channels(x['res_contact'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(x['res_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curve(torch.from_numpy(acts['preds']).sigmoid(), triu_condense(x['res_contact'].squeeze()), 'ROC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curve(torch.from_numpy(acts['preds']).sigmoid(), triu_condense(x['res_contact'].squeeze()), 'PRC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "p-gnn2D_four_channel_benchmark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
